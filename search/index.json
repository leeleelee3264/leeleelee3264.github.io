[{"content":"\nNginx에서 서브도메인을 사용할 수 있도록 설정하는 방법을 알아본다.\nIndex\nIntro Wildcard 적용한 Let\u0026rsquo;s encrypt 인증서 발급 서브도메인 CNAME 등록 Nginx에 서브도메인 연결 Intro leelee.me 라는 도메인을 사서 쓰고 있었는데 jenkins를 설치하면서 jenkins를 위한 별도의 url를 만들고 싶었다. 이전까지는 leelee.me/jenkins 이런 식으로 path로 분기를 했는데 이번에는 jenkins.leelee.me 라는 jenkins 전용 서브도메인을 하나 만들어보기로 했다.\n서브도메인 설정을 처음 해봐서, 다음에 서브도메인 설정을 할 때 참고할 수 있도록 블로그로 문서화를 해둔다. 해당 문서를 따라하면 아래와 같은 결과물이 나온다.\n결과물\nNginx에 서브도메인 연결 서브도메인 https 적용 Wildcard 적용한 Let\u0026rsquo;s encrypt 인증서 발급 다음에 서브도메인을 설정할 때 이 단계는 하지 않아도 된다.\n처음에 Nginx를 세팅할 때 https 연결을 하기 위해 Let's encrypt에서 무료로 인증서를 발급받았었다. 그때는 leelee.me만을 위한 인증서였기 때문에 leelee.me만 https 연결이 가능하게 되어있다. 앞으로 생성할 모든 서브도메인들도 https 연결을 할 수 있도록 Wildcard를 적용한 Let\u0026rsquo;s encrypt 인증서를 발급 받는 작업을 했다.\n인증서 발급 받기\n1 certbot certonly --manual -d *.leelee.me -d leelee.me --preferred-challenges dns Wildcard에서는 leelee.me가 커버가 안되기 때문에 위의 커맨드에서 leelee.me는 따로 명시해뒀다. 커맨드를 입력하면 도메인 소유권을 인증하기 위해 특정 값을 TXT로 설정해달라는 안내문이 나온다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 Saving debug log to /var/log/letsencrypt/letsencrypt.log Requesting a certificate for jenkins.leelee.me and leelee.me - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Please deploy a DNS TXT record under the name: _acme-challenge.leelee.me. with the following value: y8vZt4fm0cGviPNmAUAsHz9zST6vjBbBbQTgkvZMoewewaGs Before continuing, verify the TXT record has been deployed. Depending on the DNS provider, this may take some time, from a few seconds to multiple minutes. You can check if it has finished deploying with aid of online tools, such as the Google Admin Toolbox: https://toolbox.googleapps.com/apps/dig/#TXT/_acme-challenge.jenkins.leelee.me. Look for one or more bolded line(s) below the line \u0026#39;;ANSWER\u0026#39;. It should show the value(s) you\u0026#39;ve just added. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Press Enter to Continue 도메인을 가비아라는 호스팅 서비스에서 사용하고 있기 때문에 DNS 설정도 가비아에서 하면 된다. 처음에 레코드 수정을 잘못해서 시간이 좀 소요되었는데, 호스트 부분에 _acme-challenge.leelee.me 가 아니라 _acme-challenge 만 입력하면 된다.\n[Picture 1] TXT 등록 TXT 등록 확인\nTXT가 가비아의 DNS에 잘 등록이 되었나 아래의 커맨드로 확인한다.\n1 dig _acme-challenge.leelee.me TXT ANSWER SECTION에 TXT가 등록이 된 것을 확인할 수 있다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 ; \u0026lt;\u0026lt;\u0026gt;\u0026gt; DiG 9.16.1-Ubuntu \u0026lt;\u0026lt;\u0026gt;\u0026gt; _acme-challenge.leelee.me TXT ;; global options: +cmd ;; Got answer: ;; -\u0026gt;\u0026gt;HEADER\u0026lt;\u0026lt;- opcode: QUERY, status: NOERROR, id: 56685 ;; flags: qr rd ra; QUERY: 1, ANSWER: 2, AUTHORITY: 0, ADDITIONAL: 1 ;; OPT PSEUDOSECTION: ; EDNS: version: 0, flags:; udp: 65494 ;; QUESTION SECTION: ;_acme-challenge.leelee.me.\tIN\tTXT ;; ANSWER SECTION: _acme-challenge.leelee.me. 600\tIN\tTXT\t\u0026#34;Yr39uLJh-4QeLs4W2HV5F2JNxL8RqF8qrEbS6DWtpgo\u0026#34; _acme-challenge.leelee.me. 600\tIN\tTXT\t\u0026#34;SGzb5ZwSW1kI_bCjylmfvCkGNlML8Q0uUFJCtcoOxgQ\u0026#34; ;; Query time: 75 msec ;; SERVER: 127.0.0.53#53(127.0.0.53) ;; WHEN: Sun Mar 26 12:00:36 KST 2023 ;; MSG SIZE rcvd: 166 Wildcard 인증서 Nginx 적용\n새로 발급받은 wildcard 인증서 leelee.me-0001/fullchain.pem 를 Nginx 설정에 적용한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # HTTPS server block to serve all HTTPS traffic server { listen 443 ssl http2; listen [::]:443 ssl http2; server_name www.leelee.me, leelee.me; # SSL/TLS configuration using Let\u0026#39;s Encrypt certificate ssl_certificate /etc/letsencrypt/live/leelee.me-0001/fullchain.pem; ssl_certificate_key /etc/letsencrypt/live/leelee.me-0001/privkey.pem; include /etc/letsencrypt/options-ssl-nginx.conf; ssl_dhparam /etc/letsencrypt/ssl-dhparams.pem; # Root location block to serve static content location / { root /var/www/html; index index.html index.htm index.nginx-debian.html; try_files $uri $uri/ =404; } } Nginx 설정을 바꿨다면 잊지 말고 꼭 Nginx를 reload 해주자.\n1 systemctl reload nginx Nginx를 reload 하고 브라우저로 leelee.me에 접속해 wildcard 인증서가 적용되었나 확인한다.\n[Picture 3] Wildcard 인증서 적용 확인 서브도메인 CNAME 등록 가비아에서 TXT를 등록해준 것 처럼 서브도메인의 CNAME도 등록해준다. 처음에는 *를 사용해서 wildcard로 CNAME을 등록했는데 이렇게 하면 내가 의도하지 않은 모든 leelee.me의 서브도메인을 사용해서 서버로 들어올 수 있어, jenkins만을 위한 CNAME을 등록해줬다.\n[Picture 4] CNAME 등록 Nginx에 서브도메인 연결 Nginx에서 jenkins.leelee.me 로 들어오는 요청을 jenkins 서버가 떠 있는 localhost의 8081 포트로 보내주도록 설정했다. 기존에는 /etc/nginx/sites-available 에 있는 leelee.me 설정 파일 하나만 존재했는데 서브도메인별로 설정 파일이 있는 게 관리하기 편할 것 같아서 jenkins.leelee.me 설정 파일을 만들었다.\n설정 파일 구성도\n1 2 3 4 sites-available/ ├── default ├── jenkins.leelee.me └── leelee.me leelee.me\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 # HTTP server block to redirect all HTTP traffic to HTTPS server { listen 80; listen [::]:80; server_name www.leelee.me, leelee.me; return 301 https://$host$request_uri; } # HTTPS server block to serve all HTTPS traffic server { listen 443 ssl http2; listen [::]:443 ssl http2; server_name www.leelee.me, leelee.me; # SSL/TLS configuration using Let\u0026#39;s Encrypt certificate ssl_certificate /etc/letsencrypt/live/leelee.me-0001/fullchain.pem; ssl_certificate_key /etc/letsencrypt/live/leelee.me-0001/privkey.pem; include /etc/letsencrypt/options-ssl-nginx.conf; ssl_dhparam /etc/letsencrypt/ssl-dhparams.pem; # Root location block to serve static content location / { root /var/www/html; index index.html index.htm index.nginx-debian.html; try_files $uri $uri/ =404; } } 원래 server_name *leelee.me 라고 했다가 leelee.me로 들어가도 jenkins가 나왔다. 뭔가 jenkins의 conf를 덮어쓰는 거 같아서 아스테리크를 쓰지 않고 도메인을 server_name www.leelee.me leelee.me 로 명시했다.\n앞으로 서브도메인을 추가할 때마다 site-avaliable에 서브도메인.leelee.me 설정 파일을 만들고, server_name 서브도메인.leelee.me 를 명시하는 작업을 해야 한다.\njenkins.leelee.me\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # Jenkins server block to proxy requests to Jenkins running on port 8081 server { listen 443 ssl http2; listen [::]:443 ssl http2; server_name jenkins.leelee.me; # SSL/TLS configuration using Let\u0026#39;s Encrypt certificate ssl_certificate /etc/letsencrypt/live/leelee.me-0001/fullchain.pem; ssl_certificate_key /etc/letsencrypt/live/leelee.me-0001/privkey.pem; include /etc/letsencrypt/options-ssl-nginx.conf; ssl_dhparam /etc/letsencrypt/ssl-dhparams.pem; location / { proxy_pass http://localhost:8081/; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; } } 80 포트 리다이렉트 설정은 leelee.me에 이미 해두어서 jenkins.leelee.me를 https 연결 하게 해주는 설정만 추가해준다. leelee.me-0001 디렉터리에 들어있는 인증서가 wildcard용 인증서이기 때문에 jenkins.leelee.me 에서도 동일한 인증서를 사용해준다.\nNginx에 적용하고, 적용이 잘 되었는지 브라우저에서 jenkins.leelee.me로 접속해 확인한다.\n[Picture 6] 브라우저 접속 확인 ","date":"2023-03-25","permalink":"https://leeleelee3264.github.io/post/2023-03-25-subdomain-with-nginx/","tags":["infra"],"title":"[Infra] Nginx에 서브도메인 연결하기 with Https"},{"content":"\n김원일, 서종호의 저서 [따라하며 배우는 AWS 네트워크 입문]을 요약한다. 이 포스트에서는 1장 AWS 인프라, 2장 VPC 기초를 다룬다.\nIndex\n1장 AWS 인프라 2장 VPC 기초 1장 AWS 인프라 01. AWS 소개 1.1 클라우드 란? 인터넷을 통해 원하는 만큼의 IT 리소스를 손쉽게 사용할 수 있는 서비스를 뜻한다. AWS, Azure, GCP 등 이다.\n클라우드 서비스 종류 IaaS 사업자: 서버, 네트워크, 스토리지 등 자원을 제공 사용자: 가상 서버에 필요한 프로그램을 설치하여 사용 및 운영 예시: E2C, VPC, EBS PasS 사업자: IaaS + Runtime + Middleware 제공 → 앱을 개발하기 위한 build, test, deploy 플랫폼과 환경을 제공 사용자: Application 개발 예시: Elastic Beanstalk SaaS 사업자: 서비스 사용에 필요한 모든 것을 제공 사용자: 서비스를 사용 예시: MSK, EFS [Picture 1] 클라우드 서비스 종류 1.2 AWS 클라우드 소개 [Picture 2] 인프라 논리적 규모 데이터 센터\n서버, 네트워크, 스토리지, 로드 밸런서, 라우터 등 일반적인 IT 인프라 디바이스를 모아둔 곳을 말한다. 가용 영역\n한 개 이상의 데이터 센터들의 모음을 말한다. 각 데이터 센터는 분산 되어 있으며 전용망으로 연결되어있다. 리전\n지리적인 영역 내에서 격리되어 있는(물리적으로 분리) 여러 개의 가용 영역을 말한다. 리전은 최소 2개의 가용 영역으로 구성되어있다. 재난과 재해로 인한 서비스 장애를 막기 위해 가용 영역을 분산하여 구성하는 것을 권장하고 있다. 엣지\n외부 인터넷과 AWS 글로벌 네트워크망을 연결하는 곳을 말한다. CloudFront, Direct Connect, Route 53, AWS Shield, AWS Global Accelerator, API Gateway가 동작한다. [Picture 3] Edge 엣지 pop이 무엇인가?\n전 세계적으로 분산된 AWS infra에서 가장자리 지역에서 실행되는 캐싱서버이다. AWS 성능 최적화하는데 중요한 역할을 한다. 클라이언트와 가장 가까운 위치에 있어, CDN을 통한 전송 로드 시간을 최소화한다. 가장자리 지역이 무엇인가?\nRegion과 Availability Zone 이외의 지역으로, 데이터센터와 물리적으로 떨어져있다. 가장자리 지역은 세계 곳곳에 위치하며, 사용자와의 지리적 거리가 멀어질 수록 네트워크 대기 시간이 증가하는 문제를 해결하기 위해 도입되었다. 1.3 AWS 제품 Name Desc EC2 컴퓨팅 리소스를 제공하는 가상 머신 서비스로, 인스턴스라고 부른다. EBS 가용 영역 내의 EC2 인스턴스에 연결해 사용할 수 있는 블록 스토리지이다. S3 객체 기반의 파일을 저장할 수 있는 스토리지이다. AWS CloudFormation yaml 등의 파일을 사용해 AWS 리소스를 자동으로 배포하는 서비스이다. 02. AWS Network 소개 [Picture 4] AWS Network 서비스 AWS VPC Virtual Private Cloud AWS 클라우드 내 논리적으로 독립된 섹션을 제공한다. AWS VPN Virtual Private Network 가상의 사설 네트워크를 구성하여 프라이빗 통신을 제공한다. AWS에서는 Site-to-Site VPN과 Client VPN을 제공한다. ELB Elastic Load Balancing AWS에서 제공하는 로드 밸런싱 기술, E2C의 health checking를 더불어 트래픽을 분산하여 전달한다. 종류 ALB: Application (HTTP, HTTPS) 분산 처리한다. -\u0026gt; Application 계층에서 사용 NLB: TCP, UDP (IP, Port) 분산 처리한다. -\u0026gt; Transport 계층에서 사용 CLB: Classic Load Balancer, VPC의 전신인 EC2-Classic 에서 사용했다. AWS PrivateLink 내부 네트워크를 통해 AWS 서비스를 비공개 연결하는 기능을 제공한다. Route 53 관리형 DNS 서비스이다. 제공하는 서비스 도메인 구매 대행 DNS 역할 라우팅 정책 설정하여 트래픽 흐름 제어 AWS Transit 게이트웨어 VPC나 온프레미스 네트워크를 단일 지점으로 연결할 수 있는 라우팅 서비스이다. 다른 네트워크에 연결할 필요없이 Transit Gateway에만 연결하면 되어 관리가 쉽다. 다수의 VPC, VPN 등이 있을 때 복잡하게 개별 연결할 필요 없이 Transit 게이트웨어를 연결해 중앙집중형으로 관리한다. [Picture 5] Transit Gateway AWS Direct Connect Direct Connect = 전용선 물리적이거나 논리적일 수 있다. 사용 이유 안정적 높은 보안성 Low Latency AWS CloudFront AWS의 CDN (Contents Delivery Network) 서비스이다. 물리적 한계를 극복하기 위해 사용자와 가까운 곳에 캐시 서버를 두고 Contents를 분배한다. 엣지 pop을 두고 콘텐츠를 캐싱해 서비스를 제공한다. AWS Global Accelerator 트래픽 경로를 최적화 해준다. 싱가포르 서버를 한국이 접속하려면 많은 네트워크 망을 거치며 Latency와 Packet Loss가 증가하는데 이를 방지해준다. 네트워크 보안 네트워크 접근 제어로 IP, Port, Protocal 기반으로 접근제어를 한다.\nACL 서브넷에 적용한다. 허용/거부 규칙을 만들 수 있다. 보안 그룹 (SG) 인스턴스에 적용한다. 허용 규칙만 만들 수 있다. 2장 VPC 기초 01. VPC VPC는 독립된 가상의 클라우드 네트워크다. 리전별로 기본 VPC가 1개씩 제공되고, 사용자 커스텀 VPC는 리전 별 5개씩 만들 수 있다. 사용자는 VPC에 생성하고 제어할 수 있는 것들은 아래와 같다.\nIP 대역 인터페이스 서브넷 라우팅 테이블 인터넷 게이트웨이 보안그룹/ACL [Picture 6] VPC 02. 기본 네트워크 개념 이해 2.1 OSI 7 레이어 모델 [Picture 7] OSI 7 레이어 3 Layer인 Network 계층에서 라우팅을 하며, 최적 경로를 찾는다는 점을 집중해서 봐야 한다.\n2.2 IP와 서브넷 마스크 Public IP, Private IP [Picture 8] Public \u0026 Private IP Private IP는 NAT를 사용해 Public IP로 변환해야만 외부 인터넷과 통신할 수 있다. Docker: 각 컨테이너에 Class B에 해당하는 Private IP를 할당한다. K8s: 각 Pod에 Private IP를 할당하는데 CNI (Container Network Interface)에 따라 사용하는 대역이 다르다. 서브넷 IP = 네트워크 ID + 호스트 ID\n서브넷은 네트워크를 작은 네트워크로 나누는 기술이다.\n예시\n네트워크 ID가 192.168.0.0/16 서브넷이 존재한다. 하위에 192.168.0.0/24, 192.168.1.0/24 서브넷으로 분리한다. 위의 2개의 서브넷은 192.168.0.0/16 이라는 같은 네트워크 ID를 가진다. 동일한 서브넷에 속해있다면 같은 네트워크 ID를 가지기 때문에 호스트 ID를 통해 구분한다. 이렇게 서브넷을 구분하기 위해 네트워크 ID와 호스트 ID를 분리하는 것을 서브넷 마스크라고 한다.\n서브넷 마스크 Class 대역 서브넷 마스크 A 0.0.0.0 ~ 127.255.255.255 255.0.0.0 B 128.0.0.0 ~ 191.255.255.255 255.255.0.0 C 192.0.0.0 ~ 223.255.255.255 255.255.255.0 예시 210.77.8.3\nClass C 범위에 속하므로 사용하는 서브넷 마스크는 255.255.255.0이다.\nC 서브넷 마스크 2진수 표현은 1111 1111. 1111 1111. 1111 1111. 0000 0000 이다.\n1이 사용된 지점은 네트워크 ID 이며, 0이 사용된 지점이 호스트 ID이다. 0의 개수로 하나의 네트워크가 최대 사용할 수 있는 호스트 개수를 유추할 수 있다. 0의 갯수 8개 -\u0026gt; (2^8) -2 = 254 -\u0026gt; 4 옥텟만 호스트로 사용할 수 있다. 두 개를 빼는 이유는 맨 앞 은 네트워크 주소, 맨 뒤는 브로드캐스트 주소가 되기 때문이다. 네트워크 주소: 210.77.8.0 브로드캐스트 수조: 210.77.8.255 사용 가능 범위: 210.77.8.1 ~ 210.77.8.254 CIDR Class 기반 IP 체계를 사용하면 IP를 A,B,C 세 블록으로 나눠야 하는데, CIDR를 사용하면 블록을 더 작게 나눠 할당할 수 있다.\n[Picture 9] CIDR [Picture 10] Subnet Mask CIDR는 /8, /16 이런식으로 표기하는데 / 뒤에 나오는 숫자는 네트워크 ID의 길이이다. 즉 /8 이라면 1111 1111. 0000 0000. 0000 0000. 0000 0000 으로, 사용가능한 호스트 ID가 32-8 = 24로 (2^24) -2가 된다.\nIP를 보는 입장에서는 /24 이렇게 되어있으면 2진수로 풀었을 때 얖의 24자리 까지 같다면 같은 서브넷에 있다고 생각하면 된다. 예를 들어 192.168.1.1 와 192.168.1.254는 같은 서브넷에 있다는 뜻이다. (24자리인 1,2,3 옥텟의 숫자가 같다)\n예시 210.77.8.3/25\n/25의 서브넷 마스크는 255.255.255.128 이다.\n2진수로 풀었을 때 앞에서 25자리까지 같아야 한다. 사용할 수 있는 호스트의 갯수는 32 - 25 = 7 -\u0026gt; (2^7) - 2 = 126개가 된다. 네트워크 주소: 210.77.8.0 브로드캐스트 주소: 210.77.8.127 사용 가능 점위: 210.77.8.1 ~ 210.77.8.126 CIDR의 이점 Class 기반의 IP 주소 체계를 Classful 이라고 부르고, Class의 한계를 극복하고자 나온 것이 Classless한 주소 체계인 CIDR (Classess Inter-Domain Routing) 이다. CIDR은 IP 주소의 더욱 효율적인 사용과 주소 할당 및 라우팅의 더 큰 유연성, 그리고 네트워크 관리의 간소화를 제공하기 때문에 일반적으로 기존의 IP 주소 클래스보다 더욱 좋은 방법으로 간주되고 있다.\n2.3 TCP와 UDP 그리고 포트 번호 TCP 사용 서비스\nHTTP SSH FTP UDP 사용 서비스\nDNS DHCP 2.3.2 포트 번호 IANA 라는 단체에서 TCP와 UDP의 포트 번호 범위를 정하고 있으며, 범위에 따라 크게 3가지로 구분한다.\n포트 범위 잘 알려진 포트 0 ~ 1023 등록된 포트 1024 ~ 49151 동적 포트 49152 ~ 65535 2.4 DHCP 동적으로 IPv4 주소를 일정 기간 임대하는 프로토콜로, 임대 시간이 만료되면 반환하거나 갱신을 해야 한다.\n[Picture 11] DHCP 2.6 라우팅 최적의 경로를 잡아 통신하는 것이 라우팅이다. 라우터: 라우팅을 수행하는 장비 라우팅 테이블: 라우터가 테이블을 통해 경로를 파악하고 데이터 전달할 수 있게 함 03. VPC 리소스 소개 3.1 서브넷 VPC도 서브넷을 통해 네트워크를 분리할 수 있는데, 서브넷 IP 대역은 VPC IP 대역에 속해있어야 한다.\n[Picture 12] VPC 서브넷 예역된 서브넷 IP\n예시: 10.0.2.0/24\n네트워크 주소: 10.0.2.0 AWS VPC 가상 라우터 주소: 10.0.2.1 AWS DNS 서버 주소: 10.0.2.2 AWS 예약: 10.0.2.3 브로드캐스트 주소: 10.0.2.225 3.1.2 Public 서브넷과 Private 서브넷 Public 서브넷: 외부 인터넷 구간과 통신 가능 -\u0026gt; Public IP 존재 Private 서브넷: 사설 네트워크로, 내부만 통신 가능 -\u0026gt; NAT 게이트웨이를 사용하면 외부와 통신 가능 [Picture 12] Public \u0026 Private 서브넷 3.2 가상 라우터와 라우팅 테이블 가상 라우터?\nVPC의 각 서브넷에 대한 라우팅 관리한다. VPC를 생성하면 자동으로 생성된다. Custom 라우터를 추가할 수 있다. 서브넷별로 라우팅 테이블을 매핑할 수 있다. [Picture 13] 서브넷 당 라우팅 테이블 매핑 3.3 인터넷 게이트웨이 VPC에서 인터넷 구간으로 나가는 관문 VPC 당 1개만 연결 가능 대상: 퍼블릭 IP를 사용하는 퍼블릭 서브넷 내의 자원 양방향 연결 지원 [Picture 14] 인터넷 게이트웨이 3.4 NAT 게이트웨이 Network Address Translation 프라이빗 IP를 퍼블릭 IP 로 변환 프라이빗 IP는 인터넷 구간으로 넘어올 수 없기 때문 단방향 지원 외부에서 프라이빗 네트워크 접근 불가 [Picture 15] NAT 게이트웨이 [Picture 16] IG \u0026 NAT 게이트웨이 통신 흐름 3.5 보안 그룹과 네트워크 ACL 인스턴스 보안: 보안 그룹 서브넷 보안: ACL [Picture 17] 보안그룹과 ACL ","date":"2023-03-21","permalink":"https://leeleelee3264.github.io/post/2023-03-21-introduction-to-aws-networking-first-session-part1/","tags":["Book"],"title":"[Book] [따라하며 배우는 AWS 네트워크 입문] 노트정리 1회차 (1/2)"},{"content":"\nKasa에서 1년 동안 백엔드 엔지니어로서 지낸 시간을 회고합니다.\nIndex\nIntro: 네? 회사가 매각된다구요? 그런데 부분 매각이라구요? Work Dev Culture Intro: 네? 회사가 매각된다구요? 그런데 부분 매각이라구요? 2021년 12월, 싱가포르 사업을 위한 신생 팀에 입사하다! 저는 2021년 12월 백엔드 엔지니어로 카사에 입사했습니다. 그때는 회사가 막 싱가포르로 사업을 확장하기 시작했을 때였습니다. 제가 이 회사에 입사한 이유는 두 가지입니다. 첫 번째는 글로벌 사업을 경험해 볼 수 있는 기회가 있다는 것이었습니다. 항상 글로벌 사업을 경험해 볼 수 있는 포지션을 선호해왔습니다.\n두 번째 이유는 부동산 조각 투자라는 프롭테크 도메인을 경험해 볼 수 있다는 것이었습니다. 일반 부동산이 아닌 부동산을 조각내서 투자하는 사업이 굉장히 흥미로웠습니다. 이에 입사한 팀은 싱가포르 버전의 프로덕트를 만들어 내는 것이 주요 역할입니다.\n부동산 조각 투자 도메인? 하나의 건물을 선택해 공모를 진행하면, 사용자들이 청약을 하고 회사는 청약금을 모아 DABS라고 불리는 디지털 수익 증권을 한국투자신탁을 통해 발행하여 사용자들에게 분배한다. 그 후, 사용자들은 DABS를 주식을 거래하는 것처럼 다른 사용자들과 거래할 수 있다\n또한 협력사인 하나은행과 연결하여 계좌를 개설하고 예치금을 입금하거나 출금할 수 있어, 뱅킹 도메인의 경험을 쌓을 수 있다. 이와 함께 DABS 거래가 가능하기 때문에 Trading-Matching 도메인의 경험도 할 수 있다.\n2023년 01월, 카사 매각 소식을 듣다! 2023년 1월, 카사의 부분 매각 소식을 접했습니다. 매각 대상은 코리아 프로덕트 팀이었으며, 싱가포르 프로덕트 팀은 남았습니다. 회사는 이전에는 약 60명이 근무했지만, 매각 후 10명으로 축소되어 많은 업무를 담당해야 했습니다. 그 결과, DevOps 서포트 업무를 맡게 되었습니다.\n새로운 업무에 도전하게 된 것은 기쁨과 걱정이 함께하는 일이었지만, 이전 1년간 수행해온 일들을 회고해봐야 한다는 생각이 들어 긴 Intro를 작성한 끝에 회고를 시작하게 되었습니다.\nWork JWT를 이용한 Token Authentication 개발 JWT를 토큰으로 사용하는 Token Authentication을 구현했습니다. 이 구조에서는 access token으로 요청하며, 만료될 경우 refresh token을 사용하여 새로운 access token을 발급합니다. 이를 위해 Django rest framework의 simplejwt 라이브러리를 사용하여 토큰 생성, revoke 및 refresh를 구현했습니다.\n처음에는 토큰의 유효 기간이나 폐기등의 토큰 정책을 고려하지 않았습니다. 하지만 싱가포르 금융감독기관의 인증인 MAS Regulation을 준수하기 위해랙 서버 보안 강화해야 했고, 이에 토큰 정책도 고려하게 되었습니다.\n[Picture 1] Token Authentication 흐름 토큰 보안 강화 처음에 access token의 만료시간은 10분, refresh token의 만료시간은 12시간으로 설정했습니다. 그러나 refresh token의 만료시간이 너무 길어서, access token을 재발급 받을 때마다 refresh token도 새롭게 발급하는 ROTATE_REFRESH_TOKENS 정책을 적용하여 refresh token의 수명을 10분으로 줄였습니다.\n조사 결과, 링크드인의 access token의 만료시간은 1일, refresh token의 만료시간은 6일이었고, 구글 클라우드의 access token은 30분, refresh token은 200일이었습니다. refresh token의 수명을 줄이면, 사용자가 활동이 없을 때 쉽게 로그아웃 되는 단점이 있지만, 앱이 아닌 웹 서비스이기 때문에 비교적 짧은 토큰 수명을 가지도록 설정했습니다.\n또한 기존의 refresh token을 내부 DB에서 블랙리스트로 등록하고, access token은 10분이 지나기 전에 새로 발급받으면 기존의 access token을 사용할 수 있기 때문에 Redis 캐시에 폐기된 access token을 관리하고 주기적으로 데이터를 지우는 방식으로 토큰 보안을 강화했습니다.\nAPI 접근제한을 위한 Permission 개발 TokenAuthentication을 사용하여 인증을 마치면 해당 사용자를 식별할 수 있습니다. 우리의 BE API에는 사용자 및 운영자 전용 API가 모두 포함되어 있습니다. 또한, 사용자는 3단계 로그인을 수행하고, 운영자는 공동 운영자 및 회원 관리 운영자 등으로 나누어져 있으므로 Role-based access control (RBAC) 정책을 채택했습니다.\nDjango Rest Framework의 Permission 시스템을 확장하여 요구 사항에 맞는 사용자 정의 Permission을 만들었습니다. 이러한 Permission은 각 API View에 permission_classes를 설정하여 API 접근을 관리합니다.\n이전에는 운영자 및 사용자용 API 서버를 완전히 분리하여 접근 권한을 고민한적이 없었으나, 이번 기회를 통해 Authentication 뿐 아니라 Authorization 구조도 구축할 수 있었습니다.\nOffering 모듈 요구사항 분석 부동산 조각 투자의 핵심인 건물, 공모/청약, DABS 할당/발행을 다루는 Offering 모듈의 요구사항을 분석하였습니다. 기획자들은 싱가폴에 상주하고 있는 싱가폴인들이라서 문서 작성과 회의 등은 모두 영어로 이루어졌습니다. 그 뒤에 팀 내에서 공유하기 위해서는 다시 한국어로 번역하여 작성하였습니다.\n처음에는 이 도메인이 생소하여 이해하는 것이 어려웠지만, 분석을 하면서 회사의 도메인에 대해 깊게 이해할 수 있었습니다.\n이 작업을 통해 개발자는 코딩도 중요하지만 요구사항을 정확히 파악하고 분석하는 것이 매우 중요하며, 기획자나 다른 실무진들과 원활하게 소통하는 기술을 배웠습니다. 또한, 분석 과정에서는 모르는 부분을 정확하게 파악하기 위해 적극적으로 질문해야 한다는 것을 배웠습니다. 분석 과정에서 총 160개의 질문을 작성하면서, 정확하게 질문하는 방법에 대해 많이 고민해 보았습니다.\n[Picture 2] 질문 예시 Offering 모듈 API 명세 작성 API 명세를 작성하여 요구사항을 충족시키기 위해 필요한 API 호출 방법을 정리했습니다. 이 명세서는 백엔드 팀에서 실제 개발에 사용될 예정이기 때문에, 누락된 요구사항이나 요구사항과 불일치하는 부분이 없는지 검토해야 했습니다.\n해당 모듈은 회사 도메인에서 가장 중요한 부분이었기 때문에, 다양한 요구사항을 고려하여 API 명세를 작성하는 것은 쉬운 일이 아니었습니다. 하지만 저는 셀장님, 팀원분들, 그리고 프론트엔드 담당자분들과 함께 여러 번의 논의와 검토를 거쳐, 총 113개의 API 명세서를 완성할 수 있었습니다.\nAPI 명세 Case\nIssuer (건물주) 관리 Issuer 온보딩 Issuer 승인 Deal 관리 Deal 등록 Deal 공개 Deal 정보 수정 공모/청약 청약 신청/취소 공모 종료 DABS 확인 [Picture 3] API 명세 예시 싱가포르 정부 Mydata API인 Myinfo 서비스 연결 카사 싱가포르는 자산이 10억 이상일 때만 회원가입이 가능하며, 이를 증명하기 위해 통장 잔액, 세금 내역서 등의 증명서류를 까다롭게 요구할 수 밖에 없어 온보딩 과정에서 병목 현상이 발생하였습니다.\n따라서 싱가포르 국민을 포함한 Finance 분야의 모든 사용자 데이터를 제공하는 Myinfo 서비스를 도입하게 되었습니다. Myinfo는 2017년부터 정부가 주도하여 디지털화된 신원확인 체계를 수립하려는 NDI (National Digital Identity) 프로젝트의 일부입니다. 도입 후, 증명서류 없이도 데이터를 불러올 수 있어 온보딩 이탈률을 50% 가까이 줄일 수 있었습니다.\n[Picture 4] Myinfo Requirements 정부의 사업이다보니 API Key 와 Secret을 발급받으면 API를 사용할 수 있는 기존의 API 서비스와는 다르게 필수 사항이 많았습니다. 3, 4, 5 항목은 크게 문제가 없었지만 Transaction Log와 X.509 Public Key 부분은 작업이 필요했습니다.\nTransaction Log\n기존에는 AWS Opensearch로 로그를 기록하고 대시보드에서 확인하고 있었는데, Myinfo에서 가져오는 데이터에는 개인정보인 NRIC(싱가포르 주민번호)와 이름 등이 포함되어 있어서 ISMS 규정에 맞춰 암호화가 필요했습니다. 이를 위해 AWS KMS 서비스를 사용하여 개인 식별 정보를 암호화하고, 암호화 키를 주기적으로 변경하도록 스케쥴러를 설정했습니다. 이렇게 함으로써 ISMS 심사의 중요 기준 중 하나인 개인정보 보호를 강화할 수 있었습니다.\nX.509 Public Key\nMyinfo 서버에 데이터를 요청하고, 받기 위해서는 모든 요청과 데이터를 암호화하는 PKI 구조를 취해야 했습니다. Kasa가 Myinfo에 요청을 보낼 때, 인증되었음을 확인하기 위해 Kasa의 Private key로 서명해야 했습니다. 이를 위해 Kasa는 Myinfo에서 요구하는 X.509 인증서, 즉 Public key를 제출했습니다.\n그리고 Myinfo에서 보내온 응답을 확인할 때는 Myinfo에서 발급해준 Myinfo Public key로 verify를 수행하고, 이후 Kasa의 Private key로 decrypt를 해야 했습니다.\n[Picture 5] Myinfo에서 허가하는 Root CA Myinfo에서 인증하는 Root CA가 별도로 존재하기 때문에, digicert와 netrust에 개별적으로 문의하며 netrust에서 인증서 발급을 받게 되었습니다.\n인증서에 대한 지식이 없었고, 비용 문제와 싱가포르 정부의 기업 인증 등 많은 어려움에 직면했습니다. 하지만 경영팀, 인프라 팀, 정보보안 팀의 그레이존 역할을 수행하면서, 모든 업무를 담당하면서 많은 것을 배울 수 있었고, 이 프로젝트는 Kasa에서 가장 보람있게 수행한 프로젝트 중 하나였습니다.\n프로젝트를 하면서 공부했던 포스팅은 아래에서 확인이 가능합니다.\n[[Infra] 네트워크 필수교양, 인증서 (1/2) - 쌩기초] [[Infra] 네트워크 필수교양, 인증서 (2/2) - 심화학습] [[Project] Django로 Myinfo oauth2 클라이언트 만들기] 공모 청산 모듈 개발 공모가 끝나고 청약을 신청한 사용자들의 공모금을 Custody에 이체하고, 청약 금액만큼의 DABS를 할당/발행하는 모듈을 개발했습니다. 여기서 Custody란 디지털 자산 수탁자로, 디지털화된 자산을 보관하는 서비스입니다.\n공모 종료와 DABS 할당량 계산 공모가 종료되면 DABS 할당량을 계산해야합니다. 이를 위해서는 청약 금액과 DABS 수를 고려하여 안분비례 방식으로 배분해야합니다. 안분비례에서는 소수점 몇 자리를 유지할 것인지가 중요합니다. 부동산 자산이 대상이므로, 15자리까지 유지하고 Decimal로 관리했습니다.\n안분비례를 적용하려면 모든 청약을 확인해야합니다. 첫 번째 계산은 전체 안분 비율을 계산하고, 청약한 DABS의 비율을 곱하여 배분합니다. 이렇게하면 남은 DABS가 발생하고, 정수부는 제외하고, 소수부가 가장 큰 DABS를 추가로 할당합니다.\n이러한 형태는 많은 양의 데이터를 가져와 안분비례 방식으로 처리해야하는 문제를 발생시킬 수 있습니다. 예를 들어, 청약 데이터가 10만 건 이상인 경우, 모든 데이터를 메모리에 올리면 Out of Memory 오류가 발생할 수 있습니다. 따라서 Python의 yield 기능을 사용하여 100건씩 끊어서 청약 데이터를 처리하고, 결과를 누적하는 방식으로 연산을 수행했습니다.\n공모금 정산과 DABS 할당 DABS 할당 후, 공모금은 Custody의 건물 판매자 계좌로 이체됩니다. 서버는 Custody와 블록체인과 많은 소통이 필요하며, 디지털 자산의 변경 내역은 블록체인에 기록됩니다.\n미할당 공모금에 대한 환불 후, 공모금은 각 투자자의 예치금 계좌에서 출금되어야 합니다. 이때 Custody와 내부 블록체인을 호출하여 이체가 이뤄지며, 이체가 모두 완료될 때까지 DABS는 발행되지 않습니다. 이러한 긴 흐름에서 예외 상황과 Rollback 방지 등을 고민하였으며, 트랜잭션 단위와 재시도 방안 등을 구현했습니다.\n이 과정에서 서버가 멀티파드 환경이기 때문에 Race Condition을 방지하기 위해 Django ORM의 select_for_update 방식을 사용했습니다.\nDABS 발행 DABS 발행은 공모의 마지막 단계입니다. 이는 투자자들의 DABS 계좌에 실제로 입금해주는 것이며, 이를 위해 Custody와 Blockchain과 소통이 필요합니다. DABS 분배 요청을 Custody에 보내고, 이를 승인받아야만 DB에 기록하고 블록체인에 기록할 수 있습니다.\n처음에는 내부에서 기록 후 Custody에 마지막 승인 요청을 보내는 방식을 사용했지만, 이는 법적인 문제가 있을 수 있어 코드 수정이 필요했습니다. 초기에 코드를 함수로 추상화하고 분리해두어서, 함수를 호출하는 외부는 변경 없이 함수 내부의 수정만으로 문제를 해결할 수 있었습니다.\n이를 통해 초기 구현 단계부터 읽기 쉬운 코드와 추상화를 적용하면 유지보수가 쉬워지는 것을 몸소 경험할 수 있었습니다. 1시간 이내로 수정이 되어, 프러덕트 팀의 예상과는 다르게 런칭 일정에 차질이 생기지 않았습니다.\n대사 모듈 개발 회사의 서비스에서는 예치금 계좌와 DABS 계좌를 회사의 DB 뿐만 아니라 블록체인과 Custody에서도 관리합니다. 이에 따라, 이 세 가지 계좌의 데이터가 서로 일치하는지 확인해야 하며, 일치하지 않는 경우 모든 로그를 조사하여 즉각적인 조치가 필요합니다. 이를 위해 매일 오후 10시에 DB, 블록체인, Custody 간의 대사(reconciliation)를 조정하는 모듈을 개발하였습니다.\n대사의 대상\nDB 예치금 계좌 \u0026lt;-\u0026gt; 블록체인 예치금 계좌 DB DABS 계좌 \u0026lt;-\u0026gt; 블록체인 DABS 계좌 DB 예치금 계좌 \u0026lt;-\u0026gt; Custody 예치금 계좌 DB DABS 계좌 \u0026lt;-\u0026gt; Custody DABS 계좌 리소스는 다르지만 과정은 동일하기 때문에 추상화를 통해 통일된 구조를 가져갈 수 있도록 고민했습니다. 또한 비즈니스 로직과 서비스 정책을 세밀히 이해하는 과정이 필요했습니다. 예를 들어, 장이 마감된 후 거래가 되지 않은 DABS가 원래 계좌로 돌아가는 것과, 예치금 입금이 보류 중인 상태에서 Custody에서는 해당 금액을 인정하지 않는 것 등이 있습니다.\n따라서 이러한 이해 과정에서 점차 수정 작업을 수행했으며, 정책 이해 수준이 개발 볼륨에 미치는 영향을 다시 한 번 생각했습니다. 더 꼼꼼한 정책 파악 태도를 내재해야 한다는 필요성을 느꼈습니다.\nNotification 모듈 개발 투자자에게 공모 오픈 소식, DABS 할당, Trading 내역 등 자산 관련 다양한 Notification을 제공하는 Notification 모듈을 개발했습니다. 이 모듈은 Notification 발생 시 투자자의 이메일로 알림을 보내며, 링크를 통해 홈페이지에 접속하여 Pdf 형태로 Notification을 다운로드할 수 있습니다.\n이를 위해 서버에 있는 템플릿에 DB에서 가져온 데이터를 채워 PDF를 생성합니다. 이 과정에서 HTML과 CSS를 사용하여 템플릿을 구현했습니다. 또한 API에서 DB의 변경에 따라 Notification을 발송해야 하는 경우도 있어, Django의 signal 기능을 사용했습니다.\nDev Culture 사내 스터디 리드 전 직장에서 스터디를 했던 게 너무 재미있어서 이번에도 사내 스터디를 조직했습니다. 토이 프로젝트나 회사 기술 블로그를 만들고 싶었지만 일정이 여의치 않아 책 스터디로 진행했습니다. 특히 HTTP 완벽 가이드는 쌩신입일때 추천받아 읽은 책인데 3년 정도 후에 다시 읽어보니 이해할 수 있는 폭이 넓어져있어서 뿌듯했습니다.\n진행한 책\nHTTP 완벽 가이드 하이퍼레저 패브릭으로 배우는 블록체인 Kotlin in Action 진행 방법\n준비 주차별로 정해진 분량 독서 (3색 독서법) 참가자 모두가 주차별로 할당된 주제에 대해 발표 준비 스터디 발표 토론/회고 (좋았던 내용, 고민, 어려움, 더 찾아본 부분 공유) 참고 2022 우아한 스터디 Django에서 동시성을 다루는 방법 세미나 진행 2022년 11월에 Django에서 동시성을 다루는 방법 이라는 제목으로 세미나를 진행했습니다. Django로 백엔드 서버를 다루면서 마주했던 동시성 관련 이슈를 트러블 슈팅하면서 배운 점을 정리해 사내의 개발자들과 함께 공유하는 시간을 가졌습니다.\n세미나에서 다룬 것\nAPI 요청을 Serialize 하는 방법 Request를 하나의 큐에 모아 Serialize 하게 처리 DB의 Transaction을 Serialize 하게 처리 Django Transaction Atomicity Django ORM: select_for_update Django ORM: Atomic compare-and-swaps Redis Lock Django Transaction Test \u0026amp; Durability 세미나 영상과 발표 자료, 회고는 [[Seminar] Django에서 동시성을 다루는 방법으로 인생 첫 세미나 진행하기] 에서 더 자세하게 다루고 있습니다.\nNHN Forward 2022 참석 NHN Forward 2022는 처음으로 참석한 오프라인 개발자 컨퍼런스입니다.. 듣고 싶은 세션이 너무 많아 하루 종일 빡빡한 스케쥴로 파르나스 여기저기를 옮겨다녔습니다. 참여한 기업 부스가 많아서 참석 경품을 정말 한 바가지 받아서 컨퍼런스에 참여하지 못한 동료들에게 나눠주는 등 즐거운 시간이었습니다.\n앞으로는 컨퍼런스에 참석으로 그치는 것이 아닌 현장에서 열심히 듣고, 집에 와서 모르는 내용을 찾아보고 정리해야 할 필요를 느꼈습니다. 그리고 시간이 넉넉하더라도 꼭 일찍 등록을 하고, 세미나실에도 미리 들어가서 자리를 확보 해야합니다.\n[Picture 6] NHN Forward 초대창 Naver Deview 2023 참석 2023년 02월에 Naver 개발자 컨퍼런스인 Deview 2023에 참석했습니다.\n급하게 회사로 복귀해야 해서 세션은 하나 밖에 듣지 못했지만 부스가 알찼습니다. 특히 네이버 클로바와 네이버 제트의 개발팀에게 직접 사내 개발 문화와 분위기, 필수로 요구되는 스킬 등을 질문하고 그 분들의 답변을 들을 수 있어서 좋았습니다.\n특히 네이버 제트에 계신 분이 해주신 답변이 인상 깊었습니다. 단순히 기술을 도입하는 것이 아니라 왜 이 기술을 도입해야 하며, 이 기술을 통해 해결할 수 있는 것이 무엇인지를 고민하며 사내의 기술 스텍을 쌓아간다는 말씀이 좋았습니다. 내가 도입하고 싶어하는 기술이 정말 이 Use Case에 적합해서인지, 아니면 지금 인기가 많은 기술이라 그런지 가끔씩 의구심이 들었는데 정곡을 찔린 느낌이었습니다.\n[Picture 7] Deview 입장 팔찌 ","date":"2023-03-01","permalink":"https://leeleelee3264.github.io/post/2023-03-01-review-kasa/","tags":["General"],"title":"[General] Kasa에서 백엔드 엔지니어로서의 1년 회고하기"},{"content":"\n예제로 배우는 스프링 입문 강의를 들으며 작성한 노트를 정리한다.\nIndex\nIntro: 나와 Spring의 관계, 이대로 괜찮은가? 스프링 IoC 스프링 AOP 스프링 PSA Intro: 나와 Spring의 관계, 이대로 괜찮은가? Spring을 쓰기 시작하면서 개발자 경력을 시작했다. 그렇게 2년 동안 Spring을 쓰다가 지금 회사로 이직을 하면서 Django를 쓰게 되었다. 1년 정도 Django를 쓰다 보니 언제부터인가 Spring이 낯설게 느껴졌다. 나와 Spring의 관계, 이대로 괜찮지 않았다.\nSpring이 더 멀어지기 전에 다시 Spring 공부를 시작할 필요를 느꼈다. 어떻게 공부를 할까 고민하던 중, 예전에 인프런에서 백기선님의 Spring 강의를 듣던 게 생각이 나서 가볍게 들어보기로 했다. 예제로 배우는 스프링 입문 강의는 스프링 트라이앵클이라고 불리는 IoC, AOP, PSA의 기본 개념을 다루고 있다.\n스프링 트라이앵글 = IoC, AOP, PSA\n스프링 IoC Inversion Of Control (제어권의 역전) 내가 사용할 의존성을 내가 직접 만들지 않고, 다른 쪽에서 만들어 줄 것이라 생각하여 제어권을 넘겨준다. 내가 쓸 의존성(객체)를 new로 만들지 않는다. 이때 다른 쪽이란 Spring이며, Spring이 DI로 의존성을 주입해준다. 내가 사용할 의존성의 인터페이스만 맞으면 어떤 것을 주입해줘도 상관 없다. 테스트도 편리해진다. 내가 사용할 의존성을 내가 직접 만들 경우\n1 2 3 class OwnerController { private OwnerRepository repository = new OwnerRepository(); } 내가 사용할 의존성을 다른 쪽에서 만들어 주는 경우\n1 2 3 4 5 6 7 class OwnerController { private OwnerRepository repo; public OwnerController(OwnerRepository repo) { this.repo = repo; } } OwnerController는 OwnerRepository 없이는 아예 생성 조차 될 수 없다. OwnerRepository가 꼭 주어진다는 약속 아래에 코드가 만들어졌기 때문에 OwnerRepository에 구현되어있는 메서드를 써도 안전하다. OwnerController를 생성할 때 OwnerRepository를 주입해주는 예시\n1 2 3 4 5 6 7 @Test public void testDoSomething() { OwnerRepository ownerRepository = new OwnerRepository(); OwnerController ownerController = new OwnerController(ownerRepository); ownerController.doSomething(); } Bean 스프링이 관리하는 객체들을 뜻한다. 빈으로 등록하면 어떤 일이 일어나는가? OwnerController를 만든다고 하면 Spring이 Bean으로 만들어진 OwnerRepository를 가져와 주입을 해준다. Spring이 IoC 컨테이너에서 Bean을 관리하고 있기 때문에, 생성자나 어노테이션을 보고 필요한 의존성을 주입해주면서 의존성 관리를 해준다. 일반 객체 생성\n1 SimpleClass simpleClass = new SimpleClass(); 위에서는 일반 객체를 생성했다. 이렇게 객체를 생성하면 Spring은 이 객체가 무엇인지 알 수 없다. Spring 컨테이너 안에 빈을 등록하는 방법 @Bean 어노테이션 1 2 3 4 5 6 @Configuration public class SampleConfig { @Bean public SampleBean sampleBean() { return new SampleBean(); } Configuration 어노테이션을 사용한다. Configuration 안에 Bean 어노테이션을 사용해 빈을 만든다. new로 생성되어 리턴되는 것이 빈이 되어, IoC 컨테이너 안으로 들어간다. @Component 어노테이션 Controller, Service, Configuration 어노테이션 모두 Component 어노테이션을 가지고 있다. ComponentScan 어노테이션으로 Spring 런타임에 컴포넌트스캔을 해서 자동으로 detect 되어 IoC 컨테이너 안으로 들어간다. ComponentScan은 SpringBootApplication 어노테이션에 들어가 있다. 때떄로 SpringBootApplication 파일보다 위에 객체를 만들면 못 찾는 이유가 이것 때문이다. JPA에서 사용하는 Repository는 @Repository를 사용해서 빈으로 등록되는 것이 아니라, Repository 인터페이스를 상속받아야 빈으로 등록이 된다.\nSpringApplication\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 @Target({ElementType.TYPE}) @Retention(RetentionPolicy.RUNTIME) @Documented @Inherited @SpringBootConfiguration @EnableAutoConfiguration @ComponentScan( excludeFilters = {@Filter( type = FilterType.CUSTOM, classes = {TypeExcludeFilter.class} ), @Filter( type = FilterType.CUSTOM, classes = {AutoConfigurationExcludeFilter.class} )} ) public @interface SpringBootApplication { ... } Bean과 Component의 차이 Spring에서 관리하는 객체를 모두 Bean이라고 하기 때문에 사실 용어 자체로 볼 떄는 Component도 Bean에 포함이 된다. 개발자가 컨트롤 할 수 없는 외부 라이브러리를 빈으로 등록하고 싶을 때 @Bean을 사용한다. 빈은 메소드에 사용한다. 컴포넌트는 클래스에 사용한다. IoC 컨테이너 IoC 컨테이너는 ApplicationContext 또는 BeanFactory를 사용해서 접근할 수 있다. BeanFactory가 IoC 컨테이너 자체라고 볼 수 있으며, ApplicationContext는 BeanFactory를 상속했다. IoC 컨테이너는 빈을 만들고, 해당 빈을 필요로 하는 곳에 의존성을 주입해준다. Spring에서 의존성 주입은 빈끼리만 가능하다. 의존성 주입을 IoC에서 해주고 있기 때문에 빈이 아니라면 IoC에 들어가지 않아, 관리를 할 수 없다. SingleTon Bean은 SingleTon으로 만들어진다. 그래서 ApplicationContext나 BeanFactory로 Bean을 가져와 인스턴스의 값을 봐보면 매번 같다. SingleTon으로 단 한 번 만 만들어졌기 때문이다. 멀티 쓰레드에서 SingleTon을 만드는 것은 번거로운데 IoC 컨테이너를 사용하면 편하게 싱글톤을 만들 수 있고, 이게 IoC 컨테이너를 사용하는 이유 중 하나이다. DI 의존성 주입 방법 필드 주입\n1 2 3 4 5 6 @Controller class OwnerController { @Autowired private OwnerRepository owners; ... 생성자 주입\n1 2 3 4 5 6 7 8 9 10 @Controller class OwnerController { private final OwnerRepository owners; // @Autowired public OwnerController(OwnerRepository owners) { this.owners = owners; } ... 필수적으로 필요한 파라메터가 없을 때 인스턴스 자체를 만들 수 없어서 강제성을 줄 수 있다. 그런데 가끔 순환참조의 문제가 일어날 수 있다. 이럴때는 다른 주입법을 써야 하거나 순환참조가 일어나지 않도록 구조를 짜야 한다. 원래는 생성자에서도 @Autowired 를 써야 했는데 Spring 4.3 부터 없어졌다. Setter 주입\n1 2 3 4 5 6 7 8 9 @Controller class OwnerController { private OwnerRepository owners; @Autowired public void setOwners(OwnerRepository owners) { this.owners = owners; } Bean이 아닌 객체를 의존성 주입하려 하면? Bean으로 등록하지 않고 @Autowired로 의존성 주입을 해달라고 하면 Spring 입장에서는 모르기 때문에 의존성을 주입해 줄 수 없고, 결과적으로 어플리케이션 자체가 안 뜨게 된다.\n스프링 AOP Aspect Oriented Programming의 약자로, 관점(관심사)를 기준으로 흩어진 코드를 한 곳으로 모으는 기능이다. Spring의 대표적인 AOP로는 @Transactional이 있다.\nAS-IS\n[Picture 1] 흩어져있는 코드 같은 동작을 여러 곳에서 하고 있는데, 그 때마다 구현을 했다. AAAA가 변경이 일어난다면, 쓰는 곳에 다 찾아가 고쳐줘야 한다. 원래의 기능만 남겨두고 공통적이고 부가적인 기능을 한 곳에 모을 수 없을까? TO-BE\n[Picture 2] 모여있는 코드 AOP의 핵심은 실제 코드에는 없는데, 런타임에는 마치 있는 것처럼 동작을 한다는 것이다. 이렇게 없던 기능을 쏙 넣어주는 것에는 여러 방법이 있다.\n기능을 넣어주는 방법 컴파일할 때 넣어주기 실제 함수에는 AAAA가 없는데, 컴파일을 하면서 마치 함수에 AAAA가 있는 것처럼 넣어준다. AspectJ가 이런 일을 해주고 있다. 1 2 3 A.java --\u0026gt; (컴파일) --\u0026gt; A.class | AAAA 넣어주기 바이트코드를 조작해서 넣어주기 A.java에서 컴파일을 해 A.class를 만들면 A.class를 사용할 때 ClassLoader로 클래스를 읽어 메모리로 올린다. 메모리에 올라가는 A.class의 바이트코드를 조작해서 마치 함수에 AAAA가 있는 것처럼 넣어준다. 이또한 AspectJ가 해준다. 프록시 패턴으로 구현하기 Spring AOP가 사용하는 방법이다. 디자인 패턴 중 하나인 프록시 패턴을 사용해서 AOP를 구현했다. [프록시 패턴]의 핵심은 기존의 코드를 건드리지 않고 새로운 기능을 추가했다는 점이다. 이렇게 된다면 클라이언트 코드도 변화가 없거나, 최소화 된다. [Picture 3] 프록시 패턴 예시 프록시 패턴 만들어보기 시나리오\nCash에서 시간을 축정하는 기능을 추가하고 싶어서 CashPerf 프록시 클래스를 만들었다. CashPerf를 사용한다고 해도 클라이언트인 Store에는 아무런 변경이 없다. Store를 생성할 떄 의존성으로 Cash 대신 CashPerf를 넣어준다. 이 둘의 인터페이스인 Payment를 사용하기 때문에 이렇게 변경해도 문제가 없다. 인터페이스\n[Picture 4] payment 인터페이스 비즈니스 로직 클래스\n[Picture 5] cash 클래스 프록시 클래스\n[Picture 6] cash perf 클래스 stopwatch로 시간을 측정하는 부분을 추가했다. 프록스 클래스인 cash perf 클래스에서 비즈니스 로직 클래스인 cash를 호출한다. 클라언트\n[Picture 7] Store 클래스 테스트\n[Picture 8] Store Test 클래스 Spring AOP 실습 목표\nSpring AOP를 사용해서 API의 응답시간을 측정한다. LogExecutionTime 어노테이션 구현\n1 2 3 4 5 6 7 8 9 import java.lang.annotation.ElementType; import java.lang.annotation.Retention; import java.lang.annotation.RetentionPolicy; import java.lang.annotation.Target; @Target(ElementType.METHOD) //이 어노테이션을 어디에 쓸 것인지 @Retention(RetentionPolicy.RUNTIME) // 이 어노테이션 정보를 언제까지 유지할 것인지 public @interface LogExecutionTime { } 이렇게 어노테이션만 있으면 아무런 일도 일어나지 않고, 그냥 주석과 다름이 없다. 이 어노테이션을 읽어서 처리하는 부분이 있어야 한다. -\u0026gt; Aspect가 필요하다. 사용처\n1 2 3 4 5 6 7 @GetMapping(\u0026#34;/owners/new\u0026#34;) @LogExecutionTime public String initCreationForm(Map\u0026lt;String, Object\u0026gt; model) { Owner owner = new Owner(); model.put(\u0026#34;owner\u0026#34;, owner); return VIEWS_OWNER_CREATE_OR_UPDATE_FORM; } Aspect 구현\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 import org.aspectj.lang.ProceedingJoinPoint; import org.aspectj.lang.annotation.Around; import org.aspectj.lang.annotation.Aspect; import org.slf4j.Logger; import org.slf4j.LoggerFactory; import org.springframework.stereotype.Component; import org.springframework.util.StopWatch; @Component @Aspect public class LogAspect { Logger logger = LoggerFactory.getLogger(LogAspect.class); @Around(\u0026#34;@annotation(LogExecutionTime)\u0026#34;) public Object logExecutionTime(ProceedingJoinPoint joinPoint) throws Throwable { StopWatch stopWatch = new StopWatch(); stopWatch.start(); Object proceed = joinPoint.proceed(); stopWatch.stop(); logger.info(stopWatch.prettyPrint()); return proceed; } } @LogExecutionTime를 읽어서 처리하는 부분이다. [Line 15] Around: 어디 사이에 Aspect를 실행하면 되는지 알려준다. 여기서는 LogExecutionTime 어노테이션 사이로 설정한다. [Line 16] jointPoint: @LogExecutionTime를 부착한 API 메서드를 뜻한다. [Line 17]jointPoint를 실행하기 전에 먼저 실행한다. [Line 20] jointPoint인 API 메서드를 실행한다. [Line 22] jointPoint를 실행한 후 실행한다. [Line 25] jointPoint 실행 결과를 반환한다. 프록시 패턴예제와 Spring AOP 실습 이해 프록시 패턴 예제에서는 부가기능을 담은 프록시 클래스인 CashPerf를 만들어줬는데 Spring에서는 @LogExecutionTime를 보고 Spring AOP가 OwnerController 프록시 클래스를 자동으로 만들어준다. 그리고 이 프록시 클래스 버전의 OwnerController를 직접 주입해주기 까지 한다.\n비즈니스 로직이 담긴 클래스\nOwnerController Cash 프록시 클래스\nSpring AOP가 만들어준 OwnerController 프록시 클래스 CashPerf AOP는 공부할 부분이 굉장히 많다. 추후에는 After, Before 와 Aspect로 Exception 처리로직 만들기, 어노테이션 말고도 어디에 또 Around를 걸 수 있는지 등을 공부할 수 있다.\n스프링 PSA Portable Service Abstraction의 약자이다. PSA의 목표는 사용하는 기술스택이 달라도 우리의 코드는 달라지지 않아야 한다는 것이다.\nService Abstraction Spring으로 서블릿 어플리케이션을 만들고 있는데 실제로 Controller를 만들어보면 서블릿에 관련된 것을 특별히 코딩하지 않는다. URL 매핑도 @GetMapping, @PostMapping 을 사용해서 끝내버린다. 서블릿을 직접 조작한다면 아래와 같은 코드를 짜야 했을 것이다.\n[Picture 9] 서블릿을 직접 사용한다면 이렇게 간단하게 만들어도 그 밑에서는 서블릿 기반으로 동작한다. 이게 가능한 이유는 밑단의 서블릿 서비스를 추상화 했기 때문이다. Spring 에서는 서블릿이 포함된 MVC외에도 다양한 Service Abstraction을 제공한다.\nPortable Spring web mvc는 내장된 tomcat으로 돌아가고 있다. 이 상태에서 Spring web flux 를 도입하면 내장된 서버가 netty로 바뀐다. 완벽한 호환이 되는 것은 아니지만, 이렇게 내장 서버를 변경한다고 해도 별도의 변경 없이 실행이 된다.\nSpring에서 내장서버를 이미 추상화를 시켰기 때문에 변경이 되어도 지장없이 실행되는 것이다. 말 그대로 내장서버가 Portable(휴대용)이 된 것이다. Spring의 내장서버로는 tomcat, netty, jetty, undertow 가 있다.\nPSA 예시 1: @Controller 요청을 매핑할 수 있는 Controller 역할을 한다. @GetMapping 등으로 요청을 매핑한다. 매핑: 명시한 URL이 요청으로 들어왔을 때 GetMapping을 부착한 해당 메서드에서 처리한다는 뜻이다. path를 많이 명시해두지만 header, value, consumes, produces 등 요청과 관련된 것들로도 매핑이 가능하다. @GetMapping\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 @Target({ElementType.METHOD}) @Retention(RetentionPolicy.RUNTIME) @Documented @RequestMapping( method = {RequestMethod.GET} ) public @interface GetMapping { @AliasFor( annotation = RequestMapping.class ) String name() default \u0026#34;\u0026#34;; @AliasFor( annotation = RequestMapping.class ) String[] value() default {}; @AliasFor( annotation = RequestMapping.class ) String[] path() default {}; @AliasFor( annotation = RequestMapping.class ) String[] params() default {}; @AliasFor( annotation = RequestMapping.class ) String[] headers() default {}; @AliasFor( annotation = RequestMapping.class ) String[] consumes() default {}; @AliasFor( annotation = RequestMapping.class ) String[] produces() default {}; } PSA 예시 2: @Transactional Spring에서 DB의 All or Nothing인 트랜잭션 개념을 지키기 위해서는 @Transactional 만 부착하면 된다. 해당 어노테이션을 쓰지 않는다면 JDBC 트랜잭션을 사용해 매번 로우 레벨로 코딩을 해야 한다.\n[Picture 10] JDBC Transaction setAutoCommit을 false로 둬서 DB에 자동으로 반영되는 것을 방지한다. 모든 로직이 성공한다면 commit 해준다. 예외 발생시 catch 하며, 이때까지의 DB 액션은 rollback한다. @Transactional은 트랜잭션의 경계를 지정하는 기능이 들어있는 PlatformTransactionManager 인터페이스에 구현이 되어있고, PlatformTransactionManager는 내가 어떤 DB 기술 스텍을 사용하는지에 따라 구현체가 달라진다.\n[Picture 11] PlatformTransactionManager DB 가술 스텍에 따른 TransactionManager\nJDBC, IBatis SQLMap -\u0026gt; DataSourceTransactionManager JPA -\u0026gt; JpaTransactionManager Hibernate -\u0026gt; HibernateTransactionManger 레퍼런스 [트랜잭션 추상화 클래스의 종류와 사용법] [JPA, iBatis, MyBatis, Hibernate ORM] PSA 예시 3: Spring Cache Spring에서 사용하는 Cache도 PSA로 되어있다. 구현체로는 javax.cache, ehcahe 등이 있다.\n","date":"2023-02-06","permalink":"https://leeleelee3264.github.io/post/2023-02-06-inflearn-spring-with-example-note/","tags":["Backend"],"title":"[Class] [예제로 배우는 스프링 입문] 노트 정리"},{"content":"\nKasa에서 2022-11-30에 잔행한 Handling Concurrent request in Django를 주제로 진행한 세미나를 정리한다.\nIndex\nIntro: 네? 저보고 세미나를 진행하라구요? 세미나 - Handling Concurrent Request in Django 세미나 회고 참고 자료 Intro: 네? 저보고 세미나를 진행하라구요? 얼마 전부터 회사에서 주니어들이 돌아가면서 세미나를 진행하고 있었다. 내 차례가 올 것이라 의심조차 하지 않고 즐거운 마음으로 세미나를 참석하기를 여러 번, 결국 CTO님께 다음 세미나 발표자는 나라는 슬랙을 받았다. 그렇게 나는 20명 정도 되는 사내 엔지니어들 앞에서 세미나를 진행하게 되었다.\n주제 선정 주제를 선정할 때는 내가 실무를 하면서 마주했던 경험일 것과 백엔드 개발자가 General하게 마주할 수 있는 경험일 것을 중심으로 고려했다. 이렇게 추려보니 총 5개 정도의 후보가 나왔다.\n후보\nAPI에 동시 요청이 왔을 경우 처리 방법 Kasa Singapore에서 청약이 종료되고, 안분비례를 이용하여 DABS를 분배/발행/상장 하는 과정 PKI와 Digital Certificate Oauth Client 만들기 with Singpass 데이터 마이그레이션 모두 다 좋은 주제였지만 가장 최근에 만난 문제가 API에 동시 요청이 왔을 경우 처리 방법 이고, 다른 발표 주제들은 조금 더 구체적인 만큼 백엔드에 General하지 못한 거 같아 1번을 주제로 디벨롭 하기로 했다. 주제를 어나운스 하기 위해 멋진 타이틀을 만들었어야 했는데 결국 고심 끝에 Handling Concurrent Request in Django로 결정했다. in Django 인 이유는 회사에서 쓰고 있는 백엔드 프레임워크가 Django이기 때문이다.\n[Picture 1] 슬랙 어나운스 발표 흐름 주제 선정 후, 어떻게 자연스러우면서 일관성이 있는 발표를 할 수 있을까 고민을 하다가 아래의 흐름과 같이 발표를 진행하기로 했다.\n[Picture 2] 발표 순서 관련 자료를 수집하면서 주제 선정과 발표 흐름을 정하는 게 시간이 오래 걸렸는데 앞의 두 개가 정해지니 준비에 속도가 붙어 발표 자료를 만드는 것과 대본 작성은 비교적 빠른 시간 안에 끝낼 수 있었다. 앞으로는 실제로 어떤 내용으로, 어떻게 발표를 했는지 다뤄보겠다.\n세미나 - Handling Concurrent Request in Django 발표 영상 세미나에 참석한 동료분이 감사하게도 발표 영상을 찍어주셨다. 발표 중에는 앞을 보려고 엄청 많이 노력을 했다고 생각했는데, 막상 영상을 보니 생각보다 앞을 보는 시간이 짧았다. 아마도 긴장을 해서 대본을 까먹을지도 모른다는 마음에 나도 모르게 PPT 화면을 더 많이 본 거 같다.\n테크 회사에서 외부의 큰 세미나를 주최할 때 발표자들의 세션 영상이 올라오는 걸 봤었는데, 큰 세미나는 아니지만 나도 발표자로 참석한 세션 영상이 생겨 낯설기도 하면서 뿌듯하다.\nHandling Concurrent Request in Django 발표 떄 한 말들을 상세하게 블로그에 정리할까 싶었지만, 이미 PPT에 많은 말을 써두었기 때문에 블로그에는 PPT를 올리고, 간략하게 몇 가지 코멘트를 추가하기만 하겠다.\n목차 [Picture 3] 목차 서론 [Picture 4] Concurrent Request란? [Picture 5] Race Condition [Picture 6] Race Condition 예시 [Picture 7] 내가 만난 이슈 API 요청을 Serialize 하는 방법 [Picture 8] 이번 장에서 다룰 것들 [Picture 9] 하나의 큐에 모아서 처리 [Picture 10] 지금 API 환경 [Picture 11] Transaction이란? [Picture 12] DB lock approach Django Transaction Atomicity [Picture 13] Django ORM [Picture 14] DB lock Deal Finalize 오류 해결 방법 with select_for_update [Picture 15] 기존 코드 [Picture 16] select_for_update 1차 시도 [Picture 17] select_for_update 2차 시도 [Picture 18] select_for_update 3차 시도: 성공 [Picture 19] select_for_update 주의점: dead lock [Picture 20] select_for_update 주의점: lazy loading Deal Finalize 오류 해결 방법 with Compare-and-swaps [Picture 21] Compare-and-swaps 청약 최대 갯수 검증 오동작 오류 해결 방법 with select_for_update [Picture 22] 이력 관리 구조 [Picture 23] DB lock 없이 Exception Rasing으로 해결 [Picture 24] 이력 관리 구조에서 select_for_update 쓰기 위한 선 작업 [Picture 25] select_for_update 청약 최대 갯수 검증 오동작 오류 해결 방법 with redis lock [Picture 26] redis lock Django Transaction Test \u0026amp; Durability [Picture 27] Django Transaction Test Django Durability Atomicity를 다루다가 Durability 를 언급하여 주제가 조금 튀면 어쩌지 고민을 했지만, 이번 발표를 준비하면서 Durability 도 비즈니스 로직에서 충분히 쓸 여지가 많아보여 추가했다.\n[Picture 28] ACID [Picture 29] Transaction 단위를 잘 묶었을 경우 [Picture 30] Transaction 단위를 잘못 묶었을 경우 [Picture 31] Django Durability를 사용해서 해결 [Picture 32] 3.2 미만에서는 어떻게 하나...? [Picture 33] Transaction 단위 잘못 묶는 것을 방지하는 방법 결론 [Picture 34] 장고의 고수 [Picture 35] 결론 Q\u0026amp;A 결국 Handling Concurrent Request in Django는 3가지 방법이 있음을 발표 중에 다뤘다.\nMessage Queue를 사용하기 Exception Rasing 해서 Rollback 하기 DB lock 사용하기 질문이 들어왔던 부분은 1번이었다. 발표에서는 지금의 멀티 파드 구조에서 하나의 큐를 사용하면 멀티 파드의 이점을 살릴 수 없어서 선택하지 않았다고 말씀드렸으나, 사실은 아직 카프카 등의 메세지 큐에 대해 자세히 알지 못한 부분도 있었기에 바로 질문이 들어왔던 거 같다. 앞으로 메세지 큐에 대해서 공부를 해야 함을 느꼈다.\n또 Exception Rasing Rollback과 DB lock 사용에서 어떤 것을 선택하는 것이 더 이점인지에 대해 질문이 들어왔다. 실제로 문제를 해결한 방안은 Exception Rasing Rollback 방식이라고 답변을 드렸었는데 정확히 언제 Exception Rasing Rollback을 사용하고, 언제 DB lock을 사용하는 게 이점인지 뚜렷하게 알았다면 더 좋은 답변을 드렸을 거 같다.\n세미나 회고 처음에 CTO님이 세미나를 준비하라고 하셨을 때는 많이 당황스러웠다. 부끄럽지만 퇴근 하고 발표 자료를 준비할 때는 정말 하기 싫어서 미룬 적도 많았다. 하지만 하기 싫은 마음을 추스려 발표를 준비하고, 발표를 마치고 나니 오히려 배운 점이 많았던 것 같다.\n잘한 점 발표 순서를 선정할 때 흐름이 자연스러울 수 있도록 단계적으로 배치를 했다. 중간에 집중력이 흐트러 지는 것을 방지하기 위해 앞으로 다룰 내용을 정리하는 페이지들을 넣었다. 주제를 선정할 때, 내가 경험했던 문제를 선택했다. 자연스럽게 다른 사람들의 해결 방법을 들을 수 있었다. 애플의 Keynote 기능을 사용해 맥북과 아이패드를 연결해 스크린을 조작했다. 때문에 흐름이 끊기지 않게 스크린을 조작했다. 부족한 점 이번 발표에서 부족하다 생각하는 부분을 추려봤다. 처음으로 진행하는 세미나에, 20명 정도 되는 엔지니어들이 참석한다고 하니 긴장을 많이 했는데 긴장한 것에 비해서 준비를 조금 모자라게 한 것 같아 아쉬운 마음이 많이 든다. 다음 번에 더 잘하자!\nPPT 화면이 아니라 앞을 보도록 의식적으로 노력하자. 대본을 2일 정도 전에 작성해서, 숙지하는 시간을 더 가지도록 하자. Q\u0026amp;A에서 당황하지 말자. 질문이 들어올 부분을 미리 예상해 준비하자. 발표 내용에서 내가 확신이 없는 부분은 없어야 한다, 충분히 준비했다고 만족하지 말고 거듭 준비하자! 참고 자료 Main 자료 [Django 공식 문서 - Database transactions] [The trouble with transaction.atomic] [PYCON UK 2017: Handling Database Concurrency With Django] [How I Learned to Stop Worrying and Love atomic Banking Blunders and Concurrency Challenges] Sub 자료 [Difference Between Pessimistic Approach and Optimistic Approach in DBMS] [MySQL 공식 문서 - Transaction Levels] [Handling Race Conditions with the Django ORM] [레디스와 분산 락(1/2) - 레디스를 활용한 분산 락과 안전하고 빠른 락의 구현] ","date":"2022-12-26","permalink":"https://leeleelee3264.github.io/post/2022-12-26-kasa-concurrency-seminar/","tags":["Project"],"title":"[Seminar] [Django에서 동시성을 다루는 방법]을 주제로 인생 첫 세미나 진행하기"},{"content":"\n어떻게 해야 읽기 쉬우면서 재미있고 유용한 기술 블로그를 만들 수 있을까? Jekyll을 사용하던 내 기술 블로그에 Hugo를 도입하여 리뉴얼한 프로젝트에 대해서 다룬다.\n[티스토리 블로그]\n[Jekyll 블로그]\nIndex\nIntro: 기술 블로그, 이대로 괜찮은가? Hugo 도입기 읽고 싶은 기술 블로그 만들기 프로젝트 회고 Intro: 기술 블로그, 이대로 괜찮은가? 문서화의 중요성 요즘들어 개발자의 업무에서 개발 말고도 중요한 부분들이 많다는 생각이 자주 든다. 특히 문서화의 중요성을 다시금 환기하고 있다.\n2022년 1년간 회사에 들어와서 많은 문서들을 작성해왔다. 어떤 문서는 개발자들과 공유하기 위해 만들었고, 또 다른 문서는 기획자와 같은 비개발자들과 소통을 하기 위해 만들었다. 점점 문서로 소통하는 일이 많아지다보니 자연스럽게 문서가 담고 있는 내용도 중요하지만, 가독성과 구성도 신경을 써야 할 필요를 느꼈다.\n잘 작성된 문서는 매끄러운 소통을 하게 해주는 동시에 자료로의 역할도 하기 떄문에 가능한 읽기 좋고 잘 정리된 문서를 만들려고 노력하고 있다.\n기술 블로그 점검 그러던 중 내가 운영하던 기술 블로그의 포스팅들을 하나씩 살펴봤다. 이전에는 보이지 않았던 좋지 않은 가독성, 부자연스러운 구성들이 많이 눈에 들어왔다. 고민을 하던 중, 마침 기존에 사용하던 Jekyll에서 불편 사항들을 느끼는 것들도 많아서 대대적으로 블로그를 손보는 프로젝트를 진행했다.\n이제부터 2020년 1월부터 지금까지 어떤 플랫폼을 사용하여 블로그를 운영했고, 어떤 문제점이 있어 2022년 09월 16일부터 10월 08일, 약 한 달 동안 블로그 리팩토링 프로젝트를 진행했는지 다뤄보도록 하겠다.\n블로그 연대기 2020년: 티스토리 [티스토리 블로그 링크]\n티스토리를 선택한 이유\n익숙한 블로그 플랫폼이라 초기에 빠르게 기술 블로그를 만들 수 있다. Git 사용도 제대로 모르던 시절이라 Github page에 진입장벽이 느껴졌다. 호스팅과 검색어 노출 등을 티스토리 자체에서 관리해 줘, 따로 신경을 쓸 필요가 없다. 에디터 지원이 잘 되어있어 편하게 글을 쓸 수 있다. 티스토리의 문제점\nGithub Page를 사용해 기술 블로그를 만드는 게 유행이어서, 유행에 조금 뒤떨어졌다. [Picture 1] 직관적인 블로그 이름과 다소 도발적인 닉네임 [Picture 2] 현재까지도 꾸준한 티스토리 방문자 유입 2021년 09월 이후로 새로운 포스팅이 없는데도 방문자 수가 꾸준한 걸 보면, 손쉽게 많은 사람들이 보게 만들기 위해서는 티스토리가 좋은 선택지임이 틀림없다. 티스토리를 사용했을 때 호스팅과 검색어 노출을 신경쓰지 않았고, 에디터도 편리해서 블로그 포스팅을 하는 데 거부감이 없었다.\n사실 이번 리팩토링 프로젝트를 진행했을 때 다시 티스토리로 돌아갈까 고민하기도 했다. 방문자 유입이 많아야 글을 쓰는 재미와 보람도 있기 때문이다.\n[Picture 3] 좋지 않은 가독성 하지만 이때도 가독성은 신경을 쓰지 않아 읽기가 조금 불편했고 구성 또한 마찬가지였다. 하고 싶은 말만 가득 써둔, 냅다 기술 지식을 전하는 포스트였던 것이다. 얼마나 읽기도 불편하고 재미도 없는지, 사실은 나도 이때 써둔 글을 다시 읽으러 가거나 하지 않는다..\n2021-2022년: Jekyll [Jekyll 블로그 링크]\n티스토리를 잘 쓰던 중, 갑자기 Github Page를 사용해 기술 블로그를 운영하기로 마음먹는다. 계기는 단순하다. One Day One Commit을 해서 Github에 잔디심기를 하고 싶었다. 티스토리를 사용하면 매일 잔디를 심기가 어렵기 때문에 과감하게 포기하고, Github Page로 옮겨가는 프로젝트를 진행했다.\n티스토리 블로그에 [Jekyll로 이전하는 방법]에 대해서도 포스팅을 했다.\nJekyll를 선택한 이유\n마음에 드는 테마가 Jekyll로 구현이 되어있었다. Jekyll로 Github Page를 만드는 튜토리얼이 많았다. (사실 Jekyll로 하는 방법 밖에 없었다.) Jekyll의 문제점\nGithub Page, Jekyll 자체의 진입장벽이 있다. 검색 노출이 잘 안되어 방문자 수가 별로 없다. (Github Page의 문제점) 설치를 잘못해서 내 로컬에서 Jekyll을 실행할 수 없었다. 결과적으로 Github에 배포를 완료해야만 포스팅을 볼 수 있었다. 즉, 로컬에서 프리뷰를 할 수 없었다. [Picture 4] 좋지 않은 가독성2 (참고로 Jekyll은 Ruby로 작성되었다). 로컬에서 바로 바로 프리뷰를 보며 에디팅을 할 수 없었기 때문에 가독성은 더 엉망이 되었다. 그 당시에는 포스팅이 어떻게 보일지 미리 고민을 하지 않고 무작정 글을 쓰기만 했다. 그리고 이때에도 구성에 대해서 고민하지 않았다. 돌이켜보면 그 때는 재미있는 글, 읽고 싶은 글을 쓰는 것에 전혀 관심이 없었나 싶다.\nHugo 도입기 로컬에서 실행이 안된다는 치명적인 Jekyll을 계속 사용하기는 쉽지 않았다. 블로그 리팩토링을 진행하며 어떤 플랫폼으로 이전을 해야 할까 고민을 하던 중 Hugo를 선택했다.\nHugo를 선택한 이유로 Go를 기반으로 만들어졌다 가 있는데 마침 Go를 배워보고 싶어서 선택했다. 하지만 지금 와서 생각을 하면 Hugo를 쓴다고 해서 Go에 대해 알게 되거나 하는 것은 아닌 것 같다.\nHugo를 선택한 이유\nJekyll 대안으로 Github Page에 사용이 가능하다. (잔디심기를 계속 할 수 있다) Jekyll 만큼은 아니지만 꽤 많은 테마가 구현되어 있다. 페이지를 빌드하는 속도가 굉장히 빠르다. Go를 기반으로 만들어졌다. Hugo의 문제점\n검색 노출이 잘 안되어 방문자 수가 별로 없다. (Github Page의 문제점) 테마 선택 블로그 리팩토링에서 블로그 테마도 중요한 부분을 차지한다. 테마를 바꾸는 게 소소한 재미이기도 하고 블로그의 인상과 가독성을 크게 좌지우지 하기 때문에 심사숙고 해서 선택해야 했다. 한참 테마 쇼핑을 하다가 [Fuji] 라는 테마를 선택했다.\n[Picture 5] Fuji 테마 다크모드도 지원을 하고 테마 자체가 군더더기 없이 깔끔해서 마음에 들었다. 그리고 개발자가 Introduction을 상세하게 써줘서 그대로 보고 따라하기 좋아보였다. 또한 Tag를 설정하면 사이드 바에 노출이 되는 것도 좋았고, Archives 라는 메뉴에서 페이지네이션 없이 모든 포스트를 보여주는 것도 좋았다.\n가끔 내가 쓴 글을 찾아보면서 페이지네이션이 들어가 있어 넘기기 귀찮을 때가 많았는데 아주 편리하게 쓸 수 있을 거 같았다.\nHugo 설치 및 블로그 생성 Jekyll과 마찬가지로 Hugo로 블로그를 운영하기 위해서는 로컬에 Hugo를 설치해야 한다. Hugo는 공식문서가 굉장히 잘 되어있는데, [공식문서 quick-start] 를 보면서 하나씩 따라하면 빠르게 Hugo 블로그를 생성할 수 있다.\nGo 설치하기\n아직 Homebrew에서 Go 설치를 지원하지 않아서, 공식 홈페이지인 [Go 다운로드]에 가서 패키지 파일을 다운로드 받아야 한다. 다운로드 후, 압축을 풀고 .pkg 로 끝나는 파일을 클릭하면 설치 화면이 나와, 순차적으로 설치를 진행하면 된다.\n1 2 # 완료 후, 설치가 잘 되었나 버전을 찍어본다 go version Hugo 설치하기\n1 2 3 brew install hugo # 설치가 잘 되었나 버전을 찍어본다 hugo version 이제부터는 위에서 선택한 테마를 적용해서 나만의 블로그를 생성하는 방법에 대해 다뤄보겠다. 테마 사이트에 들어가보면 각각 테마의 Introduction에 해당 테마를 git의 submodule로 가져오라고 하고 있다. 내가 선택한 Fuji 테마의 깃허브 주소인 https://github.com/dsrkafuu/hugo-theme-fuji.git 예시로 사용하겠다.\n블로그 만들기\n1 2 3 # hugo 블로그를 만들어준다 hugo new site my-hugo-tech-blog cd my-hugo-tech-blog 블로그에 테마 적용하기\n1 2 3 4 5 git init git submodule add https://github.com/dsrkafuu/hugo-theme-fuji.git themes/fuji # 테마의 configuration 복사해준다 cp themes/fuji/exampleSite/config.toml . Hugo에서 블로그의 configuration은 config.toml 이라는 파일에서 관리가 된다. 각 테마 안에 config.toml 파일이 존재하는데 내가 만든 사이트에 해당 테마를 적용하고 싶다면 테마의 config.toml을 그대로 복사해서 나의 블로그 디렉터리에 넣어주면 된다.\n로컬에서 블로그 실행하기\n1 hugo server Webstorm에서 블로그 실행하기\n[Picture 6] Webstrom Run Configuration Jetbrains의 Webstorm은 Hugo를 지원하고 있다. Run Configuration 에서 Hugo를 위한 새로운 Configuration을 만들고 Command to run 항목에 hugo server 를 선택하면 Webstorm에서도 손쉽게 Hugo 블로그를 실행할 수 있다.\n새 페이지 만들기\n1 hugo new {파일 이름} 위의 커맨드로 설정된 테마가 적용된 hugo 페이지를 만들 수 있다.\nHugo로 Github Page에 베포하기 Jekyll은 Github에서 공식 지원을 하기 때문에 Github Page를 배포 하려면 github.io 레포지토리를 만들고 Push만 해주면 끝이었다. 하지만 Hugo는 공식 지원을 하지 않다보니 몇 가지 세팅을 해줘야 한다. 세팅이 끝나면 아래의 3가지 결과물이 나온다. 순서에 따라서 한 번 만들어보도록 하겠다.\n결과물\n블로그 레포지토리 (이전 단계에서 이미 생성) {username}.github.io 레포지토리 배포 스크립트 블로그 레포지토리 위에서 my-hugo-tech-blog 라는 이름의 Blog 레포지토리를 이미 로컬에 만들었다. github 사이트에서 블로그 레포지토리를 만들고, 연결만 해주면 된다.\n[Picture 7] Github에 만든 블로그 레포지토리 github 연결\n1 2 3 cd my-hugo-tech-blog git remote add origin https://github.com/my-hugo-tech-blog.git public git push {username}.github.io 레포지토리 {username} 은 github 계정의 username을 넣어준다. 나의 github 계정 username인 leeleelee3264를 예시로 사용하겠다. github 사이트에서 leeleelee.github.io 레포지토리를 만들어, 연결해준다.\n[Picture 7] Github에 만든 github.io 레포지토리 로컬에 github.io 레포지토리 생성 후, github 연결\n1 2 3 4 5 6 7 mkdir leeleelee3264.github.io cd leeleelee3264.github.io git init # 미리 만들어둔 github의 {username}.github.io 레포지토리에 연결한다. git remote add origin https://github.com/leeleelee3264@github.io.git public git push 배포 스크립트 레포지토리 2개를 모두 만들었다면 마지막으로 중요한 작업이 하나 더 남았다. 바로 github.io 레포지토리를 블로그 레포지토리의 서브 모듈로 추가해줘야 한다.\n배포를 위해서는 블로그 레포지토리에서 hugo로 블로그를 빌드해야 한다. hugo로 블로그를 빌드하면 public이라는 결과물 폴더가 만들어진다. 이 폴더를 github.io 레포지토리에 내보내야 Github Page로 배포가 된다. 처음에 이 과정을 잘 이해하지 못해 많은 시간을 소요했다.\nGithub Page 배포 과정\n블로그 레포지토리에서 hugo로 블로그 빌드하여 public 폴더 생성 public 폴더를 github.io 레포지토리로 내보내기 Github가 github.io의 내용물을 Github Page로 배포 [중요!] github.io 레포지토리를 블로그 레포지토리 서브 모듈로 추가\n1 2 3 4 cd my-hugo-tech-blog # public 폴더를 서브모듈로 추가해준다 git submodule add https://github.com/leeleelee3264/leeleelee3264.github.io.git public 이렇게 서브모듈로 추가를 해주면 포스팅을 작성 완료 했을 때 마다 hugo 블로그를 빌드하고 커밋하고, 서브 모듈인 github.io 레포지토리에도 커밋을 해줘야 한다. 매번 배포를 할 때 마다 두 개의 레포지토리를 커밋하기가 번거롭기 때문에 쉘 스크립트를 작성했다.\n배포 스크립트 blog_build.sh\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 #! /bin/sh echo \u0026#34;\u0026#34; echo \u0026#34;\u0026#34; echo \u0026#34;++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\u0026#34; echo \u0026#34;Build blog with hugo\u0026#34; echo \u0026#34;Author: LeeLee\u0026#34; echo \u0026#34;Date: 2022-09-17\u0026#34; echo \u0026#34;This script is for build blog and commit to git.\u0026#34; echo \u0026#34;++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\u0026#34; echo \u0026#34;\u0026#34; echo \u0026#34;\u0026#34; TODAY=$(date) # build blog hugo -t fuji # commit and push build result to github.io cd public || exit # shellcheck disable=SC2094 touch date.txt | date \u0026gt;\u0026gt; date.txt git add . git commit -m \u0026#34;Publish blog to github.io Date: $TODAY\u0026#34; git push origin master # commit and push posting resource cd .. git add . git commit -m \u0026#34;Add posting resource after publishing Date: $TODAY\u0026#34; git push 이제 배포를 할 일이 있으면 작성한 쉘 스크립트를 실행하기만 하면 된다.\n스크립트 실행\n1 ./blog_build.sh 읽고 싶은 기술 블로그 만들기 Hugo 를 도입한 게 기술적 리팩토링이었다면 이제는 글쓰기적 리팩토링인 읽고 싶은 기술 블로그 만들기 에 대해 언급하도록 하겠다.\n포스트 규칙 만들기 가독성 향상을 위해 모든 포스트가 따라야 하는 규칙, 즉 컨벤션을 만들고 기존에 작성했던 포스트들을 새로운 규칙을 적용하여 전면 수정을 진행했다. 결과적으로 통일감 있고, 포스트에 내용들이 번잡스럽게 담기지 않아 가독성이 많이 좋아졌다.\n규칙\n포스트 앞에는 항상 1줄 summary와 index를 작성한다. middle title, small title, image, example 사이에 적절하게 br를 두어 공간을 확보한다. image caption은 [Picture 1] 설명설명 식으로 작성한다. 여러 줄의 코드를 쓸 때에는 Hugo의 highlight를 사용한다. 넘버링이 default로 되어있다. 한 줄의 코드를 쓰거나 코드가 아닐 경우에 (url 등) markdown code block을 사용한다. link 를 할 때에는 링크에 대괄호를 써 [링크] 로 표기해 식별할 수 있도록 한다. 정렬을 할 때는 1,2,3,4 보다는 - 을 사용하는 dot 정렬을 사용한다. 규칙을 적용해 수정한 포스트 예시\n[Picture 7] 규칙 적용 포스트 셀장님의 피드백 본격적인 글쓰기 리팩토링을 하기 전에 기존에 작성했던 포스트 중 하나를 회사의 셀장님께 피드백을 부탁드렸다. 워낙 글을 잘 쓰시는 분이라 정말 필요한 피드백을 주셨다. 셀장님의 리뷰를 십분 반영해서 완성한 포스트가 [디지털 인증서 파헤치기 (2/2) - 심화: from CA to Chain of Trust]이다.\n피드백 핵심\n제목이 명확하면서 구미를 당겨야 한다. 정직한 이름의 Digital Certificate 보다는 Digital Certificate 쌩기초 이런 식으로 작명한다. 글의 흐름이 너무 길면 읽기 힘들다. 길면 끊어서 여러 편으로 작성한다. 웬만하면 4000자가 넘어가지 않도록 한다. 글이 너무 정보 전달만 하려 하면 안된다. 이야기를 한다는 느낌으로 글을 써야 한다. 왜 이러한 주제와 목차를 다루게 되었는지 배경 설명을 하면 좋다. 모든 포스트에 통용되는 규칙을 만드는 것은 가독성 향상에 큰 도움이 된다. 하지만 이런 규칙 외에도 필요한 것은 구성이다. 결국 블로그 구성이 엉망으로 되어있으면 읽기 싫어지기 때문이다. 셀장님의 피드백을 적극 받아드려 포스트의 구성을 수정했다. 대표적으로 이야기를 한다는 느낌으로 글은 전개하기 위해 배경을 설명하는 Intro를 도입했다.\nIntro의 도입\n[Picture 8] Intro 적용 포스트 프로젝트 회고 마음의 짐을 내려놓으며 막 경력을 시작하여 아무것도 모를 때 만들었던 기술 블로그라 미흡한 부분이 많았지만, 이번에 리팩토링을 거치면서 제법 보완을 해 뿌듯하다. 매번 내 블로그에 들어올 때 마다 마음이 묘하게 무거웠는데 이제 한 결 가벼워진 기분이다. 리팩토링의 가장 큰 수확은 내가 쓴 글을 나중에 읽어도 읽기 싫거나 지루하지 않다는 점이다.\n앞으로도 나뿐만 아니라 다른 사람들에게도 지루하지 않고, 나중에 생각이 나서 다시 들어와서 읽는 기술 블로그가 되도록 노력해야 겠다.\n새로운 과제: 조금 더 근면성실하게 포스팅 하기 2022년에는 겨우겨우 12개의 포스팅을 했다. 바쁘다는 핑계로 2021년보다 훨씬 못한 숫자의 포스팅을 하게 되어 안타깝다. 포스팅의 빈도가 낮아지니 더불어 잔디도 많이 심지 못했다. 2023년에는 딱 이것의 2배인 24개의 포스팅을 하고, 잔디도 더 많이 심도록 노력하겠다.\n[Picture 9] 듬성듬성 잔디 항상 블로그 포스팅에는 사소한 주제보다는 의미있는, 큰 규모의 글을 다뤄야 한다는 부담 때문에 더 자주 포스팅을 하지 못한다. 앞으로는 부담을 내려두고 정말 기록을 위한 사소한 주제를 다뤄야겠다.\n","date":"2022-12-22","permalink":"https://leeleelee3264.github.io/post/2022-12-22-tech-blog-readability/","tags":["Project"],"title":"[Project] 읽고 싶어지는 기술 블로그 만들기"},{"content":"\nBreet Slatkin의 저서 [Effective Python]을 요약한다. 이 포스트에서는 List \u0026amp; Dictionary를 다룬다.\nIndex\nBetter way 11 시퀸스를 슬라이싱하는 방법을 익혀라 Better way 12 스트라이드와 슬라이스를 한 식에 함께 사용하지 말라 Better way 13 슬라이싱보다는 나머지를 모두 잡아내는 언패킹을 사용하라 Better way 14 복잡한 기준을 사용해 정렬할 때는 key 파라메터를 사용하라 Better way 15 딕셔너리 삽입 순서에 의존할 때는 조심하라 Better way 16 in을 사용하고 딕셔너리 키가 없을 때 KeyError를 처리하기보다는 get을 사용하라 Better way 17 내부 상태에서 원소가 없는 경우를 처리할 때는 setdefault 보다 defaultdict를 사용하라 Better way 18 __missing__ 을 사용해 키에 따라 다른 디폴트 값을 생성하는 방법을 알아두라 Better way 11 시퀸스를 슬라이싱하는 방법을 익혀라 슬라이싱을 사용하면 최소한의 노력으로 시퀸스에 들어있는 아이템의 부분집합에 쉽게 접근할 수 있다. 모든 파이썬 클래스는 __getitem__ __setitem__ 을 구현해서 슬라이싱을 사용할 수 있다.\n슬라이싱 구문의 기본 형태는 리스트[시작:끝] 인데 끝 인덱스에 있는 원소는 포함되지 않는 걸 꼭 기억하자. 또한 슬라이싱을 할 때 리스트의 인덱스 범위를 넘어가면 Exception을 발생시키지 않고, 인덱스 범위 내의 리스트로 잘려진다.\n슬라이싱 예제\n1 2 3 4 5 6 7 8 a = [\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;, \u0026#39;d\u0026#39;, \u0026#39;e\u0026#39;, \u0026#39;f\u0026#39;, \u0026#39;g\u0026#39;, \u0026#39;h\u0026#39;] a[:] a[:5] a[:-1] # [\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;, \u0026#39;d\u0026#39;, \u0026#39;e\u0026#39;, \u0026#39;f\u0026#39;, \u0026#39;g\u0026#39;] a[-3:] # [\u0026#39;f\u0026#39;, \u0026#39;g\u0026#39;, \u0026#39;h\u0026#39;] a[2:-1] # [\u0026#39;c\u0026#39;, \u0026#39;d\u0026#39;, \u0026#39;e\u0026#39;, \u0026#39;f\u0026#39;, \u0026#39;g\u0026#39;] a[-3:-1] # [\u0026#39;f\u0026#39;, \u0026#39;g\u0026#39;] 음수 인덱스 사용시 주의할 점\n리스트가 0보다 클 때에는 잘 작동한다. 리스트가 0이면 somelist[-n:]이 결국 somelist[:] 와 같아져 전체 리스트가 복사된다. 슬라이싱 컨벤션\n리스트의 맨 앞부터 슬라이싱 할 떄에는 0을 생략하라. a[:5] 리스트의 끝까지 슬라이싱 할 때는 끝 인덱스를 생략하라. a[5:] 슬라이싱으로 대입 했을 때 슬라이스 대입에서는 슬라이스와 대입이 되는 리스트의 길이가 같을 필요가 없다. 슬라이스가 리스트보다 길면 리스트가 늘어나고, 슬라이스가 리스트보다 짧으면 리스트가 짧아진다.\n슬라이싱이 리스트보다 길 때\n1 2 3 4 5 a[2:3] = [47, 11] # 슬라이싱은 2 자리 하나 뿐인데 값은 2개가 있다. print(a) \u0026gt;\u0026gt;\u0026gt; [\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, 47, 11, \u0026#39;d\u0026#39;, \u0026#39;e\u0026#39;, \u0026#39;f\u0026#39;, \u0026#39;g\u0026#39;] 슬라이싱이 리스트보다 짧을 때\n1 2 3 4 5 a[2:7] = [99, 22, 14] # 슬라이싱은 2,3,4,5,6 자리인데 값은 3개 밖에 없다. print(a) \u0026gt;\u0026gt;\u0026gt; [\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, 99, 22, 14, \u0026#39;h\u0026#39;] 리스트 복사 슬라이싱으로 대입해서 call-by-value 로 복사가 될 때\n1 2 b = a[:] assert b == a and b is not a # 값은 같지만 같은 객체는 아니다. 원본에 변경이 있어도 복사본에는 변경이 없다.\n원본을 대입해서 call-by-reference 로 복사가 될 때\n1 2 b = a assert a is b # 같은 객체다. 원본에 변경이 있으면 그걸 참조하는 복사본에도 변경이 적용된다.\nBetter way 12 스트라이드와 슬라이스를 한 식에 함께 사용하지 말라 리스트[시작:끝:중간값] 으로 일정한 간격을 두고 슬라이싱을 할 수 있는 것을 스트라이드라고 한다. 스트라이드를 사용하면 시퀸스를 슬라이싱하면서 매 n 번째 원소만 가져올 수 있다.\n스트라이드 예제\n1 2 3 4 5 6 7 8 9 10 x = [1, 2, 3, 4, 5, 6] odds = x[::2] evens = x[1::2] print(odds) print(evens) \u0026gt;\u0026gt;\u0026gt; [1, 3, 5] [2, 4, 6] 혼란스러운 스트라이드\n1 2 3 x[2::2] x[-2::-2] x[-1:2:-2] 중요한 점은 슬라이싱 구문에 스트라이딩까지 들어가면 아주 혼란스럽다는 것이다. 각괄호 안에 수가 세 개나 들어 있으면 코드 밀도가 너무 높아서 읽기 어렵다. 특히 증가값이 음수인 경우는 더 그렇다.\n이런 혼란스러움을 방지하기 위해 시작값이나 끝값을 증가값과 함께 사용하지 말 것을 권한다. 즉, 스트라이드와 슬라이스를 한 식에 두지 않는 것이 좋다.\n스트라이드 권장 사항 증가값을 사용해야 하는 경우에는 양수값으로 만들고, 시작과 끝 인덱스를 생략하라. 시작이나 끝 인덱스와 함께 증가값을 사용해야 한다면 스트라이딩한 결과를 변수에 대입한 다음 슬라이싱하라. 스트라이드 권장 사항 예제\n1 2 y = x[::2] # 스트라이드 z = y[1:-1] # 슬라이드 Better way 13 슬라이싱보다는 나머지를 모두 잡아내는 언패킹을 사용하라 인덱스와 슬라이싱으로 목표한 원소 \u0026amp; 나머지 원소 처리하기\n1 2 3 oldest = car[0] second_oldest = car[1] others = car[2:] 파이썬을 처음 사용하는 사람은 목표한 원소와 나머지 원소를 가져올 때 슬라이싱을 자주 사용한다. 하지만 이런 방법은 인덱스와 슬라이스로 인해 시각적 잡음이 많다. 또한 인덱스 관련 오류를 낼 확률도 높아진다.\n별표 식과 언패킹으로 목표한 원소 \u0026amp; 나머지 원소 처리하기\n1 2 3 4 5 oldest, second_oldest, *others = car print(oldest, second_oldest, others) \u0026gt;\u0026gt;\u0026gt; 20 19 [15, 9, 8] 별표 식을 사용하면 언패킹 패턴의 다른 부분에 들어가지 못하는 모든 값을 별이 붙은 부분에 다 담을 수 있다. 별표는 여러 위치에서 사용이 가능하다. 단, 단독으로 사용하는 것은 안된다.\n여러 위치에서 사용하는 별표 식\n1 2 3 oldest, *others, youngest = car *others, second_youngest, youngest = car 별표 식은 항상 list 인스턴스가 된다. 언패킹하는 시퀸스에 남는 원소가 없으면 별표 식 부분은 빈 리스트가 된다. 이런 특징은 원소가 최소 N개 들어 있다는 사실을 미리 아는 시퀸스를 처리할 때 유용하다.\n별표 식 활용 별표 식의 장점은 언패킹할 리스트를 깔끔하게 가져올 수 있다는 것이다.\n인덱스와 슬라이스를 쓸 때\n1 2 3 4 csv = list(csv_file) header = csv[0] rows = csv[1:] 별표 식을 쓸 때\n1 2 3 csv = list(csv_file) header, *rows = csv_file 별표 식 주의점 별표 식은 항상 리스트를 만들어내기 때문에 별표 식을 사용해서 언패킹을 하고 리스트 연산을 할 경우 메모리를 다 사용해서 프로그램이 멈출 수 있다. 꼭 결과 데이터가 모두 메모리에 들어갈 수 있다고 확신할 때만 별표 식을 사용하도록 해야 한다.\nBetter way 14 복잡한 기준을 사용해 정렬할 때는 key 파라메터를 사용하라 정렬에 사용하고 싶은 애트리뷰트가 객체에 들어있는 경우가 많다. 이런 상황을 지원하기 위헤 sort() 는 key 라는 파라메터가 있다. key 함수에는 정렬 중인 리스트의 원소가 전달된다. key는 함수이기 때문에 lambda와 함께 사용한다.\nlambda로 key 사용하기\n1 tools.sort(key=lambda x:x.name) 여러 기준을 사용해 정렬해야 할 때 튜플은 기본적으로 비교가 가능하며 자연스러운 순서가 정해져있다. 이는 sort에 필요한 __lt__ 정의가 들어있다는 뜻이다. __lt__는 튜플의 각 위치를 이터레이션하면서 각 인덱스에 해당하는 원서를 한 번에 하나씩 비교한다.\n튜플 정렬\n1 2 3 4 5 6 drill = (4, \u0026#39;드릴\u0026#39;) sander = (4, \u0026#39;연마기\u0026#39;) assert drill[0] === sander[0] # 무게가 같다 assert drill[1] \u0026lt; sander[1] # o 보다 ㄷ 이 더 먼저와서 drill이 더 작다 assert drill \u0026lt; sander # 그러므로 드릴이 더 먼저다 비교하는 두 튜플의 첫 번째 위치에 있는 값이 서로 같으면 두 번째 위치에 있는 값을 비교하고, 또 같으면 세 번째 위치 값을 비교하고 결정이 날 때까지 이 과정을 반복한다.\n튜플 정렬을 활용한 애트리뷰트 비교\n1 tools.sort(key=lambda x : (x.weight, x.name)) 튜플 정렬을 활용한 애트리뷰트 비교 내림차순 ver\n1 tools.sort(key=lambda x : (x.weight, x.name), reverse=True) 여러 기준 사용할 때 정렬에 내림차순, 오름차순 모두 사용하기 - 연산자로 무게는 내림차순하고 이름은 오름차순 하기\n1 tools.sort(key=lambda x: (-x.weight, x.name)) 튜플을 활용한 여러 기준 정렬의 제약 사항은 모든 비교 기준의 정렬 순서가 같아야 한다. 예를들어 무게가 오름차순 정렬이라면 이름도 오름차순 정렬이어야 한다. But 만약 숫자 값일 경우 - 연산자를 사용해서 내림차순 정렬을 해, 정렬 방향을 혼합할 수 있다. 오직 숫자 값 일때만 가능하다.\n- 연산자가 통하지 않을 때\n1 2 tools.sort(key=lambda x: x.name) # name 기준 오름차순 tools.sort(key=lambda x: x.weight, reverse=True) # weight 기준 내림차순 최종적으로 리스트에서 얻어내고 싶은 정렬 기준 역순으로 정렬을 수행해야 한다. 즉 예제에서는 weight으로 내림차순 후 name으로 오름차순을 하고 싶었기 때문에 name으로 오름차순을 먼저 해주고 weight 내림차순을 해줬다.\n이러듯 제약사항이 있기 때문에 평소에는 튜플 + - 연산자를 사용하고, sort를 여러 번 호출하는 방법은 꼭 필요한 때에만 사용해야 한다.\nBetter way 15 딕셔너리 삽입 순서에 의존할 때는 조심하라 파이썬 3.5 이전에는 딕셔너리로 이터레이션을 수행할 때 삽입 순서에 상관 없이 키를 임의의 순서로 돌려줬다.\n딕셔너리 구현이 내장 hash 함수와 파이썬 인터프리터가 시작할 때 초기화되는 난수의 seed를 사용하는 해시 테이블 알고리즘을 사용했고, 그래서 인터프리터 실행 시마다 난수의 seed가 달라져 임의의 순서로 돌려줄 수 밖에 없었다.\n파이썬 3.6 부터는 딕셔너리가 삽입 순서를 보존하도록 변경 되었고, 3,7 부터는 아예 명세에 포함시켜 두었다.\n3.6 이후 부터 삽입 순서를 보장하는 딕셔너리\n1 2 3 4 5 6 7 8 9 baby_names = { \u0026#39;cat\u0026#39;: \u0026#39;kitten\u0026#39;, \u0026#39;dog\u0026#39;: \u0026#39;puppy\u0026#39;, } print(baby_names) \u0026gt;\u0026gt;\u0026gt; {\u0026#39;cat\u0026#39;: \u0026#39;kitten\u0026#39;, \u0026#39;dog\u0026#39;: \u0026#39;puppy\u0026#39;} 3.6 이후 부터 삽입 순서를 보장하는 딕셔너리 built-in 메서드\n1 2 3 4 baby_names.keys() baby_names.values() baby_names.items() baby_names.popitem() # 마지막에 삽입된 원소를 리턴하는 메서드 삽입 순서를 보장하면서 부터 딕셔너리에 빌트인 되어있는 메서드들 또한 삽입 순서가 보장이 된다.\n3.6 이후부터 삽입 순서를 보장하는 키워드 인자\n1 2 3 4 5 6 7 8 9 def my_func(**kwargs): for key, value in kwargs.items(): print(f\u0026#39;{key} = {value}\u0026#39;) my func(goose=\u0026#39;gosling\u0026#39;, kangaroo=\u0026#39;joey\u0026#39;) \u0026gt;\u0026gt;\u0026gt; goose = gosling kangaroo = joey 3.6의 변경은 dict 타입과 이 타입의 특정 구현에 의존하는 여러 다른 파이썬 기능에 영향을 미쳤고, dict 형식으로 인자를 전달하는 키워드 인자 **kwargs 도 순서를 보장하게 되었다.\n딕셔너리가 삽입 순러를 유지하는 방식은 파이썬 언어 명세의 일부가 되었기 떄문에 앞에 예제로 나왔던 기능은 코드에서 항상 이런 식으로 동작한다고 가정해도 안전하다.\n딕셔너리가 삽입 순서를 보장하지 않을 때 파이썬은 정적 타입 지정 언어가 아니고, 대부분의 경우 엄격한 클래스 계층보다는 객체의 동작이 객체의 실질적인 타입을 결정하는 덕 타이핑에 의존한다.\n덕 타이핑이란 객체가 실행 시점에 어떻게 행동하는지를 기준으로 객체의 타입을 판단하는 타입 지정방식이다. 하지만 실제 행동을 모두 검증하기는 어렵다. 때문에 실질적으로 이 말은 아무런 타이핑을 하지 않고, 런타임에 객체가 제공하는 애트리뷰트와 메서드가 없는 경우에는 그냥 오류를 내겠다는 말이다.\n파이썬에서는 list, dict 등의 표준 프로토콜을 흉내 내는 커스텀 컨테이너 타입을 쉽게 정의할 수 있고, 이럴 때 덕 타이핑으로 인해 문제가 야기될 수 있다. 딕셔너리가 삽입 순서를 보장하지 않는 문제도 이것의 일종이다.\n즉, dict 를 인자로 넣었을 때 순서를 보장했던 함수가 이 커스텀 컨테이너를 넣었을 때에도 순서를 보장한다고 생각하면 안된다. 덕 타이핑으로 타입 검사를 넘어갔을 뿐이지 이 컨테이너가 dict 이라는 말은 아니다.\n커스텀 컨테이너의 삽입 순서 미보장 예제 dict을 넣었을 떄 삽입 순서를 보장하는 함수\n1 2 3 4 5 6 7 8 def populate_ranks(votes, ranks): names = list(votes.keys()) names.sort(key=votes.get, reverse=True) for i, name in enumerate(names, 1): ranks[name] = i def get_winner(ranks): return next(iter(ranks)) populate_ranks는 dict을 순환하며 득표가 많은 순으로 삽입을 하고 있다. 따라서 get_winner 에서 제일 먼저 삽입된 원소를 뽑아옴으로 최다 득표 원소를 리턴할 수 있다.\n알파벳 순으로 순서를 보장하게 구현된 SortedDict\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 class SortedDict(MutableMapping) def __init__(self): self.data = {} def __getitem__(self, key): return self.data[key] def __setitem__(self, key, value): return self.data[key] = value def __iter__(self): # 이터레이션을 할 때 알파벳 순서로 돌려주는 함수 keys = list(self.data.keys()) keys.sort() for key in keys: yield key ... MutableMapping을 사용한 SortedDict는 __getitem__, __setitem__ 등을 구현하여 dict의 프로토콜을 지키므로 dict을 파라메터로 받는 populate_ranks와 get_winner를 사용할 수 있다.\n실제로 함수를 호출 했을 때\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 votes = { \u0026#39;otter\u0026#39;: 1281, \u0026#39;polar bear\u0026#39;: 587, \u0026#39;fox\u0026#39;: 863, } # 일반 dict를 사용할 때 ranks = {} populate_ranks(votes, ranks) winner = get_winner(ranks) print(winner) # sortedDict를 사용할 때 sorted_ranks = sortedDict() winner = get_winner(sorted_ranks) print(winner) 이렇게 해도 덕 타이핑으로 무사히 넘어가기 때문에 아무런 오류가 발생하지 않는다. 다만 원했던 결과가 나오지 않을 뿐이다. get_winner 에서 next(iter(ranks)) 로 값을 뽑아 올 때 SortedDict 내부에 구현된 __iter__ 때문에 득표와는 관련없이 알파벳 순으로 원소를 돌려주기 때문이다.\n해결방법 방법1: get_winner를 특정 순서가 보장되지 않는 형태로 변경하기 기존에는 get_winner가 득표수가 많은 순서대로 삽입을 진행했다고, 순서가 보장되었다고 판단을 하고 구현을 했다면 이제는 순서가 보장되지 않는 형태로 변경한다. 가장 보수적이고 가장 튼튼한 해법이다.\n예제\n1 2 3 4 def get_winner(ranks): for name, rank in ranks.item(): if rank == 1: return name 방법2: get_winner에서 ranks 타입 검사 넣기 get_winner가 우리의 예상에 맞게 동작하려면 ranks가 꼭 dict 타입으로 구현되어 있어야 한다. 그래서 ranks가 dict 타입이 맞는지 검사를 넣는다. 보수적인 접근 방법보다 실행 성능이 더 좋다.\n예제\n1 2 3 4 def get_winner(ranks): if not isinstance(ranks, dict): raise TypeError(\u0026#39;dict 인스턴스가 필요합니다.\u0026#39;) return next(iter(ranks)) 방법3: typing으로 타입을 명시하고 mypy로 검사 이렇게 타입을 명시 했는데 SortedDict() 타입을 넣으면 mypy에서 에러를 보내줘, 함수에 명시된 타입 이외의 타입으로 함수 호출하는 것을 예방할 수 있다. (mypy 에러가 나면 수정을 해야 하니까.) 정적 타입 안정성과 런타임 성능을 가장 잘 조합한 해법이다.\n1 2 3 4 5 def populate_ranks(votes: Dict[str, int], ranks: Dict[str, int]) -\u0026gt; None: ... def get_winner(ranks: Dict[str, int]) -\u0026gt; str: ... Better way 16 in을 사용하고 딕셔너리 키가 없을 때 KeyError를 처리하기보다는 get을 사용하라 딕셔너리의 세 가지 기본 연산은 키 또는 키에 연관된 값에 접근하고, 대입하고, 삭제하는 것이다. 그런데 딕셔너리의 내용은 동적이므로 어떤 키에 접근하거나 키를 삭제할 때 그 키가 딕셔너리에 없을 수도 있다. 이럴때 처리할 수 있는 방법이 3 가지가 있다.\nin을 사용\n1 2 3 4 5 6 if key in counters: count = counters[key] else: count = 0 counters[key] = count + 1 이렇게 처리하면 딕셔너리에서 키를 두 번 읽고, 키에 대한 값을 한 번 대입해야 한다.\nKeyError를 사용\n1 2 3 4 5 6 try: count = counters[key] except KeyError: count = 0 counters[key] = count + 1 키를 한 번만 읽고, 값을 한 번 만 대입하면 되기 때문에 in 을 사용할 때 보다 효율적이다.\nget을 사용 (권장)\n1 2 count = counters.get(key, 0) counters[key] = count + 1 키를 한 번만 읽고, 값을 한 번 만 대입한다. 하지만 KeyError 방식보다 코드가 훨씬 짧다.\nin 과 KeyError 를 사용해도 코드를 줄일 수 있는 방법이 있지만 가독성이 떨어진다. 따라서 간단한 타입의 값이 들어있는 경우 get 메서드를 사용하는 방법이 가장 코드가 짧고 깔끔하다.\n딕셔너리에 저장된 값이 리스트처럼 복잡한 값이라면? 리스트가 들어간다고 해도 처리할 수 있는 방법은 위와 같은 3가지이다.\n리스트 처리 방법\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 votes = { \u0026#39;바게트\u0026#39;: [\u0026#39;철수\u0026#39;, \u0026#39;순이\u0026#39;], \u0026#39;치아바타\u0026#39;: [\u0026#39;하니\u0026#39;, \u0026#39;유리\u0026#39;], } ## in ## if key in votes: names = votes[key] else: votes[key] = names = [] names.append(who) ## KeyError ## try: names = votes[key] except KeyError: votes[key] = names = [] names.append(who) ## get ## names = votes.get(key) if names is None: votes[key] = names = [] names.append(who) ## (심화) get 왈러스 연산자 사용하기 ## if (names := votes.get(key)) is None: votes[key] = names = [] names.append(who) 세가지 경우 모두 참조를 통해 딕셔너리에 넣은 빈 리스트의 내용을 변경할 수 있어 한 번 만 대입을 하면 된다.\ndict 에는 이런 연산을 간소화해주는 setdefault 메서드를 제공한다. 하지만 결론부터 말하자면 setdefault 사용을 권장하지 않는다.\nsetdefault\n1 2 names = votes.setdefault(key, []) names.append(who) setdefault 동작 방식\nsetdefault는 키를 사용해 딕셔너리의 값을 가져오려고 시도한다. 키가 없으면 제공받은 디폴트 값을 딕셔너리에 대입한 다음, 키를 사용하여 딕셔러니 값을 반환한다. 따라서 이 값은 새로 저장된 디폴트 값 일 수도 있고, 이미 딕셔너리에 있던 키 값일 수도 있다. 치명적인 단점은 메서드 이름인 setdefault가 메서드의 동작을 직접적으로 드러내지 못하는 것이다. 값을 얻는 동작도 포함이 되어있지만 이름이 set 이라서 마치 값을 넣어주는 동작만 할 것 같아 코드를 읽자마자 무슨 역할을 하려는지 모를 수 있다.\nBetter way 17 내부 상태에서 원소가 없는 경우를 처리할 때는 setdefault 보다 defaultdict를 사용하라 직접 만들지 않는 딕셔너리를 다룰 때 get 을 쓰거나 상황을 고려하여 setdefault를 쓰는 것이 좋다. 하지만 직접 딕셔너리 생성을 제어할 수 있다면, 예를 들어 클래스 내부에서 딕셔너리 인스턴스를 사용한다면 setdefault도 있지만 collections 내장 모듈에 있는 defaultdict 클래스 사용을 고려할 수 있다.\ndefaultdict 클래스는 키가 없을 때 자동으로 디폴트 값을 지정해준다.\nsetdefault를 사용하는 클래스 내부 딕셔너리\n1 2 3 4 5 6 7 8 9 10 11 12 13 class Visits: def __init__(self): self.data = {} def add(self, country, city): city_set = self.data.setdefault(country, set()) city_set.add(city) ## example ## visits = Visits() visits.add(\u0026#39;영국\u0026#39;, \u0026#39;런던\u0026#39;) visits.add(\u0026#39;캐나다\u0026#39;, \u0026#39;오타와\u0026#39;) 새로 만든 클래스의 add는 setdefault의 복잡도를 제대로 감춰, 더 나은 인터페이스를 제공한다. 하지만 여전히 setdefault의 이름은 직관적이지 못하고, 주어진 나라가 data 딕셔너리에 있든 없든 호출할 때마다 새로운 set 인스턴스를 만들기 때문에 효율적이지 못하다.\ndefaultdict를 사용하는 클래스 내부 딕셔너리\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 from collections import defaultdict class Visits: def __init__(self): self.data = defaultdict(set) def add(self, country, city): set.data[country].add(city) ## example ## visits = Visits() visits.add(\u0026#39;영국\u0026#39;, \u0026#39;런던\u0026#39;) visits.add(\u0026#39;영국\u0026#39;, \u0026#39;버스\u0026#39;) print(visits.data) \u0026gt;\u0026gt;\u0026gt; defaultdict(\u0026lt;class \u0026#39;set\u0026#39;\u0026gt;, {\u0026#39;영국\u0026#39;: {\u0026#39;버스\u0026#39;, \u0026#39;런던\u0026#39;}}) 이제 add 코드는 data 딕셔너리에 있는 키에 접근하면 항상 기존 set 인스턴스가 반환된다. 이전 코드에서는 add 메서드가 아주 많이 호출되면 집합 생성에 따른 비용도 커지는데, 이 구현에서는 불필요한 set이 만들어지는 경우는 없다.\nBetter way 18 __missing__ 을 사용해 키에 따라 다른 디폴트 값을 생성하는 방법을 알아두라 앞에서 키가 없는 경우 get 또는 setdefault, defaultdict 를 사용하는 방법을 알아봤지만 이 모든 것을 사용하기가 적당하지 않은 경우도 있다.\n저자가 제일 추천하는 방법인 defaultdict을 써서 문제를 해결해보도록 하겠다.\ndefaultdict\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 from collections import defaultdict def open_picture(profile_path): try: return open(profile_path, \u0026#39;a+b\u0026#39;) except OSError: print(\u0026#39;경로를 열 수 없습니다: {profile_path}\u0026#39;) raise pictures = defaultdict(open_picture) handle = pictures[path] handle.seek(0) image_data = handle.read() \u0026gt;\u0026gt;\u0026gt; Traceback ... TypeError: open_picture() missing 1 required positional argument: \u0026#39;profile_path\u0026#39; defaultdict 생성자에 전달한 함수는 인자를 받을 수 없다. 이로 인해 파일 경로를 사용해 open을 호출할 방법이 없다.\nsetdefault\n1 2 3 4 5 6 7 8 try: handle = pictures.setdefault(path, open(path, \u0026#39;a+b\u0026#39;)) except OSError: print(\u0026#39;경로를 열 수 없습니다: {profile_path}\u0026#39;) raise else: handle.seek(0) image_data = handle.read() 그렇다고 setdefault를 쓴다고 상황을 해결할 수 있지는 않다.\nsetdefault의 고질적인 문제로 딕셔너리에 경로가 있는지 여부와 관계없이 open이 항상 호출된다. 이로 인해 같은 프로그램상에 존재하던 열린 파일 핸들과 혼동될 수 있는 새로운 파일 핸들이 생길 수 도 있다.\n__missing__\n1 2 3 4 5 6 7 8 9 10 class Pictures(dict): def __missing__(self, key): value = open_picture(key) self[key] = value return value pictures = Pictures() handle = pictures[path] handle.seek(0) image_data = handle.read() dict 타입의 하위 클래스를 만들고 __missing__ 특별 메서드를 구현하면 키가 없는 경우를 처리하는 로직을 커스텀화 할 수 있다.\n__missing__ 동작 방식\npictures[path]라는 딕셔너리 접근에서 path가 딕셔너리에 없으면 __missing__ 메서드가 호출된다. __missing__ 은 키에 해당하는 디폴트 값을 생성해 딕셔너리에 넣어준 다음에 호출한 쪽에 그 값을 반환해야 한다. 그 이후 딕셔너리에 같은 key로 접근하면 이미 해당 원소가 딕셔너리에 들어 있으므로 __missing__ 이 호출되지 않는다. ","date":"2022-10-25","permalink":"https://leeleelee3264.github.io/post/2022-10-25-effevtive-python-betterway-11-to-15/","tags":["Book"],"title":"[Book] [Effective Python] 노트 정리 (2/10) - List \u0026 Dictionary"},{"content":"\n이 포스트에서는 scale up, scale out, 트래픽과 함께 기초적인 로드 밸런서 지식을 다룬다. 끝에서는 로드 밸런스와 리버스 프록시를 비교한다.\nIndex\nIntro: 터져버린 서버에 우아하게 대처하기 로드밸런서 (plus) 리버스 프록시 Intro: 터져버린 서버에 우아하게 대처하기 몇 년 전에 Spring Boot로 어드민 모니터링 사이트를 개발하는데 사용자 활동 데이터를 봐야하는 API 개발에 미숙하여 페이지에 페이지네이션을 넣지 않았다. 활동 데이터라 수시로 많은 데이터가 생성이 되는데 페이지네이션까지 넣지 않다보니 쿼리를 실행하는 것에 굉장히 오랜 시간이 걸렸다. 쿼리가 느린 것은 두 번째 문제이고 Heap이 터지는 경우가 자주 발생했다. Jar 파일을 실행할 때 Heap 공간을 너무 적게 준 것이다. 어떻게 해결을 해야 할까 고민 하던 중 당시의 팀장님이 아주 간단한 해결책을 주셨다. 서버를 늘려라.\n서버를 늘리는 방법 서버를 늘리는 것은 Scale Up, Scale Out 두 가지 방법으로 접근을 할 수 있다.\nScale Up\nScale Out\n정의\n서버 한 대의 하드웨어 성능을 업그레이드 한다.\n예시) RAM을 16에서 32로 올린다.\n동일한 성능의 서버를 한 대 더 추가한다.\n장점\n추가적인 설정 없이 하드웨어를 업그레이드 하면 되기 때문에 쉽다.관리가 쉽고 운영 이슈가 적다.Scale Up 보다 비용이 저렴하다. 확장성이 있고 유연성이 있다.서버를 필요한 만큼 띄워두고 더 필요할 때 더 띄우면 되기 때문에 pay as you grow 가 더 용이하다. 단점\n성능 향상에는 한계가 있고, 비용도 서버 한 대 추가하는 것보다 비싸다.하나의 서버가 해결하는 요청이 크기 때문에 서버가 하나만 내려가도 타격이 클 수 있다.기존에 떠 있는 서버의 스팩을 올리기 때문에 찰나의 다운 타임이 있을 수 있다.처음부터 트래픽 예측을 잘못해서 사양을 높게 잡으면 (over provision) 비용에 낭비가 발생한다.여러 대의 연결된 서버가 돌아감을 감안해서 스레드 동기화, 분산처리 등의 병렬 컴퓨팅 환경을 구성하고 유지해야 한다.아키텍처에 대한 높은 이해도가 요구된다.서버가 많아질수록 잠재적인 에러 원인이 늘어나기 때문에 운영이 어렵다. Scale Up 보다는 Scale Out 을 더 많이 하는 추세이지만, Scale Out이 항상 정답은 아니며, 상황과 서비스에 따라서 증설 방법을 결정해야 한다. 참고로 위에서 마주했던 문제를 해결하기 위해서는 Scale Up을 선택했다.\n당시에는 수동으로 Scale Up을 했지만 클라우드에서 제공하는 Auto Scaling을 사용했다면 더 우아한 대처가 될 수 있었을텐데 아쉬움이 남는다.\n트래픽의 종류 서버를 늘려야 하는 이유 중 제일 큰 이유는 몰린 트래픽을 처리하기 위해서이다. 그런데 Scaling을 살펴보던 중 트래픽에도 종류가 있다는 걸 발견했다. 바로 North-South 트래픽과 East-West 트래픽이다.\n[Picture 1] 트래픽 종류 East-West 트래픽 Server to Server 트래픽이다. 데이터 센터 내부에서 발생하는 내부 트래픽이다. North-South 트래픽 Client to Server 트래픽이다. 데이터 센터와 나머지 네트워크 사이에서 일어나는 트래픽이다. 내부 트래픽을 제외한 나머지 트래픽이라고 생각하면 된다. 요즘은 빅데이터와 MSA 클라우드 환경으로 인해 East-West 트래픽이 훨씬 많아졌다고 한다. 때문에 과거에는 North-South 트래픽을 관리하는 게 쟁점이었지만 요즘은 East-West 트래픽을 관리하는 게 쟁점이다.\n로드밸런서가 클라우드 시대의 필수 교양인 이유는? 서버를 아무리 증설한다고 해도 요청을 적절하게 분산해서 서버에게 전달해주는 로드밸런서 없이는 무용지물이다. 클라우드 시대에 서버를 Scale Up 하거나 Scale Out 하기가 굉장히 쉬워진 이 때, 로드밸런서에 대해 알고 있다면 더 효율적으로 서버를 운용하고 대용량의 트래픽을 잘 다룰 수 있을 것이다. 긴 Intro를 지나 이제 로드밸런서에 대해 알아보자.\n로드밸런서 일반적인 웹 트래픽 분산처리 구조에서 로드밸런서는 클라이언트와 서버 사이에 존재한다. 때문에 클라이언트의 요청을 제일 처음으로 만나게 된다. 로드밸런서가 클라이언트의 요청을 나눠 각 서버에 할당하면 웹서버 - WAS 서버로 전달이 되어 클라이언트에게 보낼 응답이 만들어진다.\n[Picture 2] 웹 트래픽 분산처리 구조 로드밸런서의 기능 클라이언트 요청들을 분산해서 서버 그룹에 넘겨준다. 또한 선택된 서버가 만들어낸 응답을 적합한 클라이언트에 리턴해준다. 서버의 핼스체크가 가능하기 때문에 만약 서버가 다운 되었다면, 이를 감지 해 다른 서버로 넘겨준다. 클라이언트가 볼 수 있는 에러를 줄여 UX를 향상시킬 수 있다고 한다. 로드밸런서와 session 몇몇 로드밸런서는 session을 영속적으로 관리한다. 세션을 관리함에 따라 특정 클라이언트에게서 온 모든 요청을 동일한 서버에 보낼 수 있다. HTTP는 이론상 stateless 하게 움직이지만, 많은 서비스에서는 주요 기능을 위해 state 를 저장한다.\n로드밸런서 종류 with OSI 7 계층 로드밸런서는 OSI 7 계층에 걸쳐 여러 종류가 있다. 트래픽은 네트워크를 타고 움직이며, 로드밸런서는 이 트래픽을 분산하는 역할이니 어떻게 보면 네트워크 프로토콜 마다 존재하는 게 당연하게 보인다. 로드밸런서의 네이밍 컨벤션 또한 L(Layer) + 계층 의 형태로 되어있다.\n[참고] OSI 7 계층 로드밸런서 종류 종류\n설명\nL2\nNetwork Access 계층에서 사용한다.Mac 주소 기반의 부하 분산을 한다. L3\nNetwork 계층에서 사용한다.IP 주소 기반의 부하 분산을 한다. L4\nTransport 계층에서 사용한다.Port 기반의 부하 분산을 한다. L7\nApplication 계층에서 사용한다.HTTP(URL, 쿠키 등) 기반의 부하 분산을 한다.HTTP HTTPS 뿐 아니라 FTP, DNS, DHCPv4, SMTP, client LDAP 등 더 많은 Application 계층의 프로토콜에 사용이 가능하다. 계층이 올라갈수록 정교한 로드밸런싱이 가능하고, 상위계층 스위치는 하위계층 스위치가 이해할 수 있도록 변환을 해줘야 하기 때문에 상위계층 스위치는 하위계층이 하는 일을 할 수 있다. 결국 해당 계층의 스위치는 자신이 속한 계층 말도고 다른 하위 층에서 사용할 수 있다. 예를 들어 L7을 L4 자리에 배치해도 된다. 다만 자원낭비가 될 뿐이다.\n로드밸런싱 알고리즘 Round-Robin (default) 가장 간단한 방식의 알고리즘으로, 로테이션을 돌려서 요청을 서버에게 할당한다. 서버의 특징 (성능, 유효성 등) 을 고려하지 않고 모두 동일하다 생각하고 할당한다. Least-Connection 새로운 요청이 오면 가장 적은 수의 active 커넥션을 가지고 있는 서버에게 요청을 할당한다. Round-Robin 처럼 서버의 특징을 고려하지 않고 할당한다. ip-Hash 새로운 요청을 어떤 서버에게 할당해야 하는지 판단하기 위해 hash function을 이용한다. 이때의 hash는 client ip 주소를 기반으로 한다. 여기서 부터는 Nginx에서 제공하지 않는 알고리즘이다.\nWeighted Round-Robin 서버의 성능을 고려하여 각 서버마다 weight를 설정한다. weight에 따라 성능이 더 좋은 서버가 있으면 더 많은 요청을 해당 서버에 할당한다. Weighted Least-Connection 서버들의 성능을 고려하여 weight를 주면서 가장 연결이 덜 된 서버에게 요청을 할당한다. Lease Response Time 가장 적은 수의 active 요청을 가지고 있으면서 가장 짧은 응답시간을 보이는 서버에게 요청을 할당한다. NLB \u0026amp; ALB aws에서 제공하고 있는 서비스이기 때문에 aws에 한정된 용어이다.\nNLB는 Network Load Balancer의 약자이고 ALB는 Application Load Balancer의 약자이다. 본 문서에는 다루지 않았으나 CLB 라는 용어도 있는데 Classic Load Balancer의 약자이다. 그리고 이 모든 Load Balancer 서비스를 묶어서 ELB 라고 하는데 Elastic Load Balancer의 약자이다.\n[Picture 3] NLB와 ALB를 한 번에 쓴다면? NLB 특징 TLS/TCP/UDP 트래픽을 담당하며 static ip를 사용하여 고정적이다. 별도의 판단 기준 없이 요청을 포워드 해주는 일만 한다. 단순한 만큼 퍼포먼스가 매우 좋다. 모든 어플리케이션을 동일하게 본다. 크러쉬가 나거나 오프라인인 어플리케이션에 요청을 할당할 때 도 있다. 어플리케이션의 유효성을 판단할 수 없다. TCP 계층을 기준으로 판단하기 때문이다. 서버에 ICMP ping을 날려서 응답이 오는지 안 오는지로 판단한다. 또는 3 way handshaking을 완벽하게 끝났는지로 판단한다. 용도 real-time 데이터 스트리밍 서비스 (비디오, 주식시세) 에 사용한다. HTTP 프로토콜이 아니거나, ALB가 통하지 않을 상황일 때 사용한다. ALB 특징 HTTP/HTTPS 트래픽을 담당하며 유연하다. HTTP 헤더 컨텐츠를 보고 요청을 어디로 라우트 해야 하는지 판단한다. 이를 Content Based Routing이라 한다. 어플리케이션의 유효성을 깊게 판단할 수 있다. 특정 페이지의 HTTP GET을 성공적으로 가져왔는지 판단한다. 리턴된 컨텐츠가 인풋 파라메터 기준으로 expected 된 게 맞는지 까지 검증한다. 용도 웹 어플리케이션에 사용할 수 있다. MSA에서 EC2나 도커의 앞에서 내부 로드밸런서로 사용할 수 있다. REST API 어플리케이션 앞에서 쓸 수 도 있는데 이보다는 AWS API Gateway를 쓰는 게 더 좋은 성능을 낸다. (plus) 리버스 프록시 Nginx를 다루면서 로드밸런서와 리버스 프록시가 많이 헷갈렸다. 클라이언트와 서버 사이에 위치하고, 요청을 포워딩하고,응답을 클라이언트에 전달한다는 개념이 유사해서 혼동이 왔었다. 때문에 둘의 개념을 한 번 다루고 가면 좋을 거 같아 리버스 프록시 내용을 추가했다.\n로드밸런서와 리버스 프록시의 차이 로드밸런서는 클라이언트의 요청을 분산하는 것에 집중하고 있다면 리버스 프록시는 클라이언트의 요청을 actual processing을 위해 서버로 포워딩해도 되는지 판단하는 것에 집중하고 있다.\nA load balancer receives user requests, distributes them accordingly among a group of servers, then forwards each server response to its respective user.\nA reverse proxy facilitates a user’s requests to a web server/application server and the server’s response.\n리버스 프록시의 기능 클라이언트 요청들을 판단해서 서버 그룹에 넘겨준다. 또한 선택된 서버가 만들어낸 응답을 적합한 클라이언트에 리턴해준다. 웹사이트의 public face와 같다. 리버스 프록시 주소는 하나의 공개된 웹사이트고, 이 주소는 사이트 네트워크의 가장 끝단에 위치하고 있다. 그래서 브라우저나 모바일의 요청이 모두 이 끝단에 모이게 된다. 로드밸런서는 여러 대의 서버가 있을 때 사용이 가능하지만 리버스 프록시는 하나의 웹서버 또는 하나의 WAS 서버만 있어도 사용이 가능하다. 리버스 프록시의 장점 보안 네트워크 밖으로 백엔드 서버가 노출이 안되기 때문에 malicious 한 클라이언트가 취약점에 대해 다이렉트로 접근할 수 없다. 많은 리버스 프록시가 서버를 DDoS 로 부터 보호하는 기능을 포함하고 있다. 특정 IP로부터 오는 트래픽을 블랙리스트로 설정해 거부하거나, 클라이언트가 사용하는 커넥션의 수를 제한하는 방식 등이 여기에 해당한다. 확장성과 유연성 증가 클라이언트는 리버스 프록시의 IP만 알고 있기 때문에 내부에서 마음대로 백엔드 인프라를 변경할 수 있다. load-balanced 환경에서 트래픽 볼륨에 따라서 서버를 늘이고 줄일 때 특히 유용하다. 리버스 프록시는 로드밸런싱도 한다. 웹 가속화 (acceleration) 압축: 클라이언트에게 응답을 보내기 전에 압축을 한다. 압축을 통해서 bandwidth를 줄이고 네트워크에서 빠르게 이동하도록 만든다. SSL Termination: 서버와 클라이언트 사이의 트래픽은 암호화가 되어야 하는데 때때로 암호화는 부담이 될 수 있다. 이때 리버스 프록시가 암호화와 복호화를 담당해서 부담을 덜어준다. 캐싱: 서버의 응답을 클라이언트에게 보내기 전에 리버스 프록시는 응답을 로컬에 저장한다. 그래서 동일한 요청이 들어왔을 때 서버로 가지 않고 캐싱한 응답을 전달한다. (plus) Nginx 설정시 참고하면 좋은 자료들 Load Balancer 설정하 How to setup an Nginx reverse proxy server example ","date":"2022-10-24","permalink":"https://leeleelee3264.github.io/post/2022-10-24-load-balancer/","tags":["Infra"],"title":"[Infra] 클라우드 필수 교양, 로드 밸런서"},{"content":"\n이 포스트에서는 인증서의 심화 개념인 Certificate Authority, root CA, CA bundle, chain of trust에 대해서 알아본다.\nIntro [디지털 인증서 쌩기초 파헤치기] 에서 인증서의 종류, PKI, X.509 와 같은 디지털 인증서의 기초에 대해서 알아봤다.\n기초에 대해서 어느 정도 파악했다면 한 단계 더 나아가서 디지털 인증서를 어떻게 발급받으며, 어떻게 적용하는 지를 알아보자. 또한 클라이언트와 서버가 커넥션을 맺을 때 어떻게 서버가 디지털 인증서를 사용하여 신원을 증명해 내는지를 알아보자.\n[Index 1] 에서는 인증서를 발급해주는 기관인 CA에 대해서 다룬다. 더 나아가 [Index 2] 에서는 CA에서 인증서를 발급 받기 위한 requirement를 준비하는 것을 다룬다. [Index 3] 에서는 발급받은 인증서를 서버에 적용하는 방법을 다룬다.\n서버에 인증서를 적용하게 되면, 이제 우리 서버는 신원을 증명하기 위해 클라이언트에게 디지털 인증서를 제공한다. 그럼 클라이언트는 여러 단계에 거쳐서 우리 서버의 신원을 증명하려는 노력을 한다. 이 과정을 Chain of Trust 라고 하며 [Index 4] 에서 다루고 있다. [Index 5] 에서는 본 포스트에서 다루는 내용을 한 눈에 볼 수 있는 모식도를 제공한다.\nIndex\nCA CA에서 인증서 생성하기 인증서 적용하기 Chain of Trust (plus) 인증서가 사용되는 프로세스 모식도 CA CA는 신뢰할 수 있는 기관에 의해 운영되는데, 주요 업무는 공개키 등록 시 본인 인증과 X.509와 같은 디지털 인증서 생성 및 발생 등이 있다.\nCA의 필요성 공개키와 비밀키만을 이용해 암호화는 수행하면 보안에 매우 취약해진다. [Picture 1] 에서 해커가 사용자 B에게 해커의 공개키를 사용하여 데이터를 보낼 경우 사용자 B는 사용자 A가 보낸 데이터라착각할 수 있다. 이와 같은 해커의 공격 방식을 MTM(Man in the Middle Attack)이라고 한다. 이와 같은 취약점을 해결하기 위해 CA(Certificate Authority) 라는 인증 노드를 사용하게 된다.\n[Picture 1] Man in Middle Attack PKI에서 CA가 사용되는 과정 사용자 A가 자신의 공개키를 CA에 등록한다. (본인 인증을 거친다) 사용자 B가 사용자 A의 공개키가 필요할 경우 CA에 가서 사용자 A의 공개키를 요청한다. CA는 사용자 A의 공개키를 암화화하여 사용자 B에게 전달한다. 사용자 B는 해당 공개키만을 사용자 A의 공개키라고 믿게 된다. [Picture 2] PKI에서 CA가 사용되는 과정 CA 업체 유명한 CA들은 [Picture 3] 같다. CA의 인증서를 대리 구매해주는 서비스들이 외국에도, 한국에도 있지만 직접 CA에 가서 사는 것이 더 좋다. CA에 직접 가서 구매했을 때 QnA나 대응도 더 빠르고, 자세하며 나중에 인증서를 관리할 때에도 더 편했다.\n[Picture 3] 유명한 CA\" CA에서 인증서 생성하기 CA에서 인증서를 생성하기 위해서는 SAN과 CSR을 제출해야 한다. CA 업체에 따라서 추가 서류를 제출해야 할 수 있으니 발급 전에 확인해야 한다.\nSAN Subject Alternative Name 의 약자이다. Optional 이고, 선택한 인증서의 가격에 따라서 보장하는 SAN의 개수에도 차이가 있다. 하나의 인증서로 여러개의 FQDN (Fully Qualified Domain Name)를 보장할 수 있다. 하나의 IP로 향햐는 도메인이 여러개가 있을 때 자주 사용이 된다. SAN 예시\nwww.digicert.com knowledge.digicert.com www.thawte.com CSR Certificate signing request 의 약자이다. CA에서 certificate 발급받기 위해 보내는 메세지이다. 가장 많이 사용되는 포맷은 PKCS #10이다. CSR 을 만들기 전에 PrivateKey를 만드는데, 이 PrivateKey가 바로 PKI에서 사용되는 그 PrivateKey로 아무한테도 노출하면 안된다. [gogetssl] 처럼 인증서 중간 유통 업체들이 때때로 CSR과 PrivateKey를 사용자에게 주기도 한다. OpenSSL로 CSR 생성하기 openssl req -new -newkey rsa:2048 -sha256 -nodes -keyout server.key -out server.csr Private key을 생성한다. 생성한 Private key를 사용하여 CSR을 생성한다. CSR을 생성할 때 [Picture 4] 에 대한 정보를 입력해야 한다. [Picture 4] CSR 입력 사항 Common Name SAN이 나오고 나서부터 Common Name은 실제적인 효력 보다는 과거의 레거시 형태로 남아있다. 이번에도 CSR을 작성할 때 OV를 만들어야 하기 때문에 Common Name에 도메인을 쓰지 않고 회사 이름을 넣었다. Comman Name에 도메인을 넣는다면 인증서가 설치되는 서버의 이름과 정확하게 매치해야 한다. 만약 서브 도메인을 위해 인증서를 발급했을 경우에는 full 서브도메인을 정확하게 명시해줘야한다.\nCommon Name 예시\n도메인: mydomain.com 서브 도메인: www common name: www.domain.com Common Name vs SAN common name은 단 하나의 엔트리만 입력을 할 수 있다. wildcard 또는 non-wildcard 이라도 단 하나만 입력을 할수 있다는 것은 변함이 없다 SAN은 common name의 이런 제약을 없애기 위해 생겨났다. SAN이 있기 때문에 multi-name SSL 인증서를 만들 수 있게 되었다. SAN에는 여러가지 값을 넣을 수 있고, Common Name과 중복도 가능하다. SAN이 서버 네임이 일치하는 가 확인하기 위한 유일한 필수 레퍼런스가 되었다. 인증서 관련 커맨드 웹사이트에서 사용하고 있는 인증서 가져오기 echo | openssl s_client -servername google.com -connect google.com:443 |\\\\n sed -ne '/-BEGIN CERTIFICATE-/,/-END CERTIFICATE-/p' \u0026gt; certificate.crt 인증서 열어보기 openssl x509 -in certificate.crt -noout -text [파싱 결과 예시] Certificate: Data: Version: 3 (0x2) Serial Number: 59:43:1c:7c:0e:b1:5c:49:0a:01:4e:60:34:b8:2c:b2 Signature Algorithm: sha256WithRSAEncryption Issuer: C=US, O=Google Trust Services LLC, CN=GTS CA 1C3 Validity Not Before: Apr 25 08:31:18 2022 GMT Not After : Jul 18 08:31:17 2022 GMT Subject: CN=*.google.com Subject Public Key Info: Public Key Algorithm: id-ecPublicKey Public-Key: (256 bit) pub: 04:81:63🆎d3:29:a2:15:6c:ee:b7:43:66:55:c5: 88:6e:70:9b:4d:43:40:66:ea:a4:fc:c0:06:8b:4c: fd:60:23:5f:f7:a0:e4:3c:5a:af:7f:e5:36:63:88: 55:dd:e2:60:41:6c:a1:27:3d:48:fb:2e:6a:21:6d: 01:aa:2e:25:7e ASN1 OID: prime256v1 NIST CURVE: P-256 X509v3 extensions: X509v3 Key Usage: critical Digital Signature X509v3 Extended Key Usage: TLS Web Server Authentication X509v3 Basic Constraints: critical CA:FALSE X509v3 Subject Key Identifier: 8B:09:31:88:DD:30:A6:59:D3:86:E5:3D:EA:06:6D:F3:C0:25:96:D5 X509v3 Authority Key Identifier: keyid:8A:74:7F:AF:85:CD:EE:95:CD:3D:9C:D0:E2:46:14:F3:71:35:1D:27 Authority Information Access: OCSP - URI:http://ocsp.pki.goog/gts1c3 CA Issuers - URI:http://pki.goog/repo/certs/gts1c3.der X509v3 Subject Alternative Name: DNS:*.google.com, DNS:*.appengine.google.com, DNS:*.bdn.dev, DNS:*.cloud.google.com, DNS:*.crowdsource.google.com, DNS:*.datacompute.google.com, DNS:*.google.ca, DNS:*.google.cl, DNS:*.google.co.in, DNS:*.google.co.jp, DNS:*.google.co.uk, DNS:*.google.com.ar, DNS:*.google.com.au, DNS:*.google.com.br, DNS:*.google.com.co, DNS:*.google.com.mx, DNS:*.google.com.tr, DNS:*.google.com.vn, DNS:*.google.de, DNS:*.google.es, DNS:*.google.fr, DNS:*.google.hu, DNS:*.google.it, DNS:*.google.nl, DNS:*.google.pl, DNS:*.google.pt, DNS:*.googleadapis.com, DNS:*.googleapis.cn, DNS:*.googlevideo.com, DNS:*.gstatic.cn, DNS:*.gstatic-cn.com, DNS:googlecnapps.cn, DNS:*.googlecnapps.cn, DNS:googleapps-cn.com, DNS:*.googleapps-cn.com, DNS:gkecnapps.cn, DNS:*.gkecnapps.cn, DNS:googledownloads.cn, DNS:*.googledownloads.cn, DNS:recaptcha.net.cn, DNS:*.recaptcha.net.cn, DNS:recaptcha-cn.net, DNS:*.recaptcha-cn.net, DNS:widevine.cn, DNS:*.widevine.cn, DNS:ampproject.org.cn, DNS:*.ampproject.org.cn, DNS:ampproject.net.cn, DNS:*.ampproject.net.cn, DNS:google-analytics-cn.com, DNS:*.google-analytics-cn.com, DNS:googleadservices-cn.com, DNS:*.googleadservices-cn.com, DNS:googlevads-cn.com, DNS:*.googlevads-cn.com, DNS:googleapis-cn.com, DNS:*.googleapis-cn.com, DNS:googleoptimize-cn.com, DNS:*.googleoptimize-cn.com, DNS:doubleclick-cn.net, DNS:*.doubleclick-cn.net, DNS:*.fls.doubleclick-cn.net, DNS:*.g.doubleclick-cn.net, DNS:doubleclick.cn, DNS:*.doubleclick.cn, DNS:*.fls.doubleclick.cn, DNS:*.g.doubleclick.cn, DNS:dartsearch-cn.net, DNS:*.dartsearch-cn.net, DNS:googletraveladservices-cn.com, DNS:*.googletraveladservices-cn.com, DNS:googletagservices-cn.com, DNS:*.googletagservices-cn.com, DNS:googletagmanager-cn.com, DNS:*.googletagmanager-cn.com, DNS:googlesyndication-cn.com, DNS:*.googlesyndication-cn.com, DNS:*.safeframe.googlesyndication-cn.com, DNS:app-measurement-cn.com, DNS:*.app-measurement-cn.com, DNS:gvt1-cn.com, DNS:*.gvt1-cn.com, DNS:gvt2-cn.com, DNS:*.gvt2-cn.com, DNS:2mdn-cn.net, DNS:*.2mdn-cn.net, DNS:googleflights-cn.net, DNS:*.googleflights-cn.net, DNS:admob-cn.com, DNS:*.admob-cn.com, DNS:*.gstatic.com, DNS:*.metric.gstatic.com, DNS:*.gvt1.com, DNS:*.gcpcdn.gvt1.com, DNS:*.gvt2.com, DNS:*.gcp.gvt2.com, DNS:*.url.google.com, DNS:*.youtube-nocookie.com, DNS:*.ytimg.com, DNS:android.com, DNS:*.android.com, DNS:*.flash.android.com, DNS:g.cn, DNS:*.g.cn, DNS:g.co, DNS:*.g.co, DNS:goo.gl, DNS:www.goo.gl, DNS:google-analytics.com, DNS:*.google-analytics.com, DNS:google.com, DNS:googlecommerce.com, DNS:*.googlecommerce.com, DNS:ggpht.cn, DNS:*.ggpht.cn, DNS:urchin.com, DNS:*.urchin.com, DNS:youtu.be, DNS:youtube.com, DNS:*.youtube.com, DNS:youtubeeducation.com, DNS:*.youtubeeducation.com, DNS:youtubekids.com, DNS:*.youtubekids.com, DNS:yt.be, DNS:*.yt.be, DNS:android.clients.google.com, DNS:developer.android.google.cn, DNS:developers.android.google.cn, DNS:source.android.google.cn X509v3 Certificate Policies: Policy: 2.23.140.1.2.1 Policy: 1.3.6.1.4.1.11129.2.5.3 X509v3 CRL Distribution Points: Full Name: URI:http://crls.pki.goog/gts1c3/QOvJ0N1sT2A.crl 1.3.6.1.4.1.11129.2.4.2: ......v.)y...99!.Vs.c.w..W}.` ..M]\u0026amp;\\%]......`........G0E.!......Y.Z...Z.s#...Al...\u0026amp;......Wi. m.-a..._^,...#....D.tZ.j..W.g....w...^.h.O.l..._N\u0026gt;Z.....j^.;.. D\\*s....`..!.....H0F.!..6:.?...f..m.}%.r..........E.....!..U....G...%.$D.mG.B.. Signature Algorithm: sha256WithRSAEncryption 5c:2b:62:ec:f6:ee:92:0c:28:98:92:af:35:f0:78:5b:75:f2: a2:c5:e9:56:04:da:31:ed:0c:92:16:3a:14:47:f9:60:7d:e4: 36:33:82:13:68:54:37:47:81:f8:b6:0e:66:a7:87:c4:f5:82: ca:58:62:a2:15:63:16:28:5b:8e:bc:e7:18:af:97:a2:f4:92: 92:e3:2f:69:df:ba:7a:80:92:20:14:22:4f:3d:26:69:c6:f8: 90:d1:2c:36:57:0a:5c:20:00:86:d2:bd:52:db:19:39:46:12: b0:65:1d:16:3d:f1:58:4b:d6:19:c0:4b:0d:eb:ad:0b:b9:1c: 03:ad:cb:d1:04:33:a2:2c:b8:33:f6:01:7c:71:7f:e8:8a:32: c1:74:9a:11:f7🆎b9:ff:f8:89:99:f3:f9:50:7b:31:c7:fa: fc:71:d1:c6:f2:b4:d2:82:93:84:ae:d8:eb:55:41:d4:de:9d: 7f:47:44:05:4a:fb:a7:09:b2:89:99:a7:7f:64:13:52:be:73: ee:00:b9:1c:ad:e1:44:48:41:a4:77:55:8d:0a:c8:b0:bb:69: fe:9a:84:a5:cd:2d:a9:61:3b:60:92:e4:43:d6:2b:79:d6:5a: 0d:db:f7:7f:7a:fc:7d:c3:59:e3:7d:d7:47:78:c2:b2:7d:6d: f2:7a:75:49 간단하게 인증서 조회를 하기 위해서는 online parser를 사용할 수 있다. [Online Parser] 파싱의 결과로 나온 field에 대한 설명은 [LeeLee- Digital Certificate] 포스트 에서 더 자세하게 확인할 수 있다.\n인증서 적용하기 CA bundle CA에서 인증서를 발급받으면 end-entity (내 application)의 인증서만 오지 않는다. Root CA의 인증서와 Intermediate 인증서가 함께 오는데 이를 CA bundle이라고 한다. CA bundle은 웹 브라우저 등의 클라이언트와 인증서의 호완성을 높히기 위해서 사용이 된다.\nCA bundle 예시\nRoot CA 인증서 + Intermediate 인증서 + end-entity 인증서 = Certificate Chain [Picture 5] CA bundle CA bundle은 *.ca-bundle 확장자의 zip 파일이나 root, intermediate 개별로 주어진다. CA bundle은 클라이언트에게 순차적으로 제공이 되어야 하기 때문에 대게는 CA에서 이미 하나의 bundle을 만들어서 제공해준다. 만약 개별로 주어졌을 경우에는 하나의 ca-bundle로 합쳐야 한다\nCA bundle 합치기 예시\nAddTrustExternalCARoot.crt –\u0026gt; Root CA Certificate COMODORSAAddTrustCA.crt –\u0026gt; Intermediate CA Certificate 1 COMODORSADomainValidationSecureServerCA.crt –\u0026gt; Intermediate CA Certificate 2 yourDomain.crt –\u0026gt; Your SSL Certificate cat ComodoRSADomainValidationSecureServerCA.crt ComodoRSAAddTrustCA.crt AddTrustExternalCARoot.crt \u0026gt; yourDomain.ca-bundle Google CA Bundle 예시\n브라우저의 브라우징 창에 🔒 모양을 누르면 해당 도메인에 대한 인증서의 종류, 상태 등을 볼 수 있는데 google.com의 인증서를 열어봤을 때에도 아래처럼 인증서가 계층을 이루고 있다.\nend-entity 인증서: *.google.com Intermediate 인증서: GTS CA 1C3 Root 인증서: GTS Root R1 [Picture 6] End Entity Certificate [Picture 7] Intermediate Certificate [Picture 8] Root Certificate Apply CA Bundle CA bundle을 서버 호스트 어디엔가 다운로드해두었다면 해당 도메인이 등록되어있는 Nginx 등의 웹서버에 CA Bundle가 있는 path를 적어주면 된다. 그럼 알아서 클라이언트에게 CA Bundle을 한꺼번에 넘겨준다. 클라이언트는 이 Bundle을 통해서 end-entity가 믿을 수 있는지 없는지 여부를 결정한다. 이와 같은 과정을 Chain of Trust라고 한다.\n인증서를 서버에 다운로드 하는 것을 ‘installed’ 한다고 표현하는데 실제로는 인증서를 서버에 설치한다기 보다는 특정 디렉터리에 넣어둔다.\nLet’s encrypt 를 인증서로 사용했을 때 Nginx의 configuration 예시\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 server { root /var/www/html; index index.html index.htm index.nginx-debian.html; server_name www.test.me test.me; location / { try_files $uri $uri/ =404; } listen [::]:443 ssl http2; # managed by Certbot listen 443 ssl http2; # managed by Certbot ssl_certificate /etc/letsencrypt/live/test.me/fullchain.pem; # managed by Certbot ssl_certificate_key /etc/letsencrypt/live/test.me/privkey.pem; # managed by Certbot include /etc/letsencrypt/options-ssl-nginx.conf; # managed by Certbot ssl_dhparam /etc/letsencrypt/ssl-dhparams.pem; # managed by Certbot } 14번 줄의 의 fullchain.pem이 하나로 압축이 된 CA bundle이다. Let’s encrypt에서는 처음부터 하나로 압축된 ca-bundle을 pem 형식으로 제공해준다.\nChain of Trust CA가 서명한 인증서의 내부 구조 CA가 서명을 end-entity의 인증서에 서명을 하게 되면 end-entity의 인증서에는 아래와 같은 정보가 추가되어 발급된다.\n누가 서명을 했는지 (Issuer name) Issuer의 Digital Signature (Issuer의 Private key로 서명) end-entity (Subject)의 public key [Picture 9] End Entity Certificate 내부 구조 Chain of Trust 과정 Chain of Trust는 브라우저가 신뢰할 수 있는 CA가 Issuer로 있는 인증서를 찾기 까지 모든 layer의 인증서를 탐색하는 과정이라고 생각하면 된다. 다른 말로 신뢰할 수 있는 CA가 발급한 인증서를 증명하는 과정이다. 참고로 브라우저에 Root CA의 인증서를 보내도 대부분은 그 인증서를 쓰지 않고, 브라우저에 이미 자체적으로 저장이 되어있는 Root CA 인증서를 사용한다.\nChain 시나리오 End-entity의 인증서 제출 End-entity는 브라우저가 신뢰할 수 있는 CA가 아니다. End-entity의 Issuer를 확인한다. Intermediate CA가 Issuer로 되어있다. Intermediate CA는 브라우저가 신뢰할 수 있는 CA가 아니다. intermediate CA의 Issuer를 확인한다. Root CA가 Issuer로 되어있다. Root CA는 브라우저가 신뢰할 수 있는 CA다. Root CA의 digital signature를 브라우저가 이미 가지고 있는 Root CA의 public key로 verify한다. Root CA digital signature verify 완료. Root CA → Intermediate CA → End-entity 모두 chain of trust를 통해서 믿을 수 있는 인증서가 된다. [Picture 10] Chain of Trust 과정 Chain 계층 [Picture 11] Chain 계층 Root Certificate AKA Trust Anchor 최상단인 Root는 보증해 줄 곳이 없기 때문에 Root CA는 self-signed을 한다. OS, 서드파티 웹 브라우저, 클라이언트 어플리케이션은 Root CA의 인증서를 사전에 설치해둔다. clinet는 chain of trust를 통해서 end-entity가 Root 에 인증이 되었는지 식별할 수 있다. Root가 end-entity에 직접 서명하는 일은 거의 없고, 중간에 하나 또는 여러 개의 Intermediate CA가 서명을 한다. Root의 Private key가 변경이 되었다면 해당 Private key로 발급받은 모든 인증서를 재발급 해야 한다. Intermediate Certificate 거의 모든 SSL 인증서 chain에는 최소 하나의 Intermediate CA가 들어가 있다. Intermediate CA는 Root CA의 신뢰성(trustworthy)의 확장에 필수적인 연결점이고, Root CA와 end-entity에 추가적인 보안 레벨이 된다. End-entity Certificate End-entity의 인증서는 보안, 확장성, CA compliance 등을 제공하지만, 인증서를 발급받은 대상 (subject)의 신뢰성을 보장하지는 못한다. End-entity는 크리티컬한 정보를 CSR에 담아 인증서를 발급해주는 CA(issuer)에게 제출하고, CA가 판단하기에 제출된 정보가 믿을 수 있다면 CA의 Private key로 서명을 한 인증서를 발급해준다. 이 인증서가 verified 또는 signed 되어있지 않다면 SSL connection은 실패한다. (plus)인증서가 사용되는 프로세스 모식도 브라우저와 서버의 request - response 과정 모식도\n[Picture 11] request - response 과정 모식도 브라우저가 youtube.com 을 요청한다. youtube가 인증서 번들을 브라우저에 제출한다. 브라우저는 자신의 CA public key 리스트로 서버가 제출한 인증서가 정말로 신뢰할 수 있는 CA에 의해 서명되었는지 확인한다. Intermediate CA는 지정된 Root CA가 아니기 때문에 브라우저의 입장에서는 믿을 수 없는 CA다. 제출된 인증서 번들을 타고 올라가서 Root CA까지 간다. 신뢰하기로 한 인증서에 명시되어있는 domain name이나 ip가 맞다면, 서버의 public key를 사용하여 대칭키를 생성하여 서버와 공유한다. 이 대칭키로 connection의 모든 트래픽을 암호화한다. 대칭키를 사용하는 것이 비대칭키를 사용하여 모든 커뮤니케이션을 암호화하는 것보다 효율적이기 때문에 대칭키르르 사용한다. 복호화는 private key를 가지고 있는 서버만 가능하다. CA-signed 인증서 발급 과정 모식도\n[Picture 12] CA-signed 인증서 발급 과정 모식도 Self-signed 인증서 발급 과정 모식도\n[Picture 13] Self-signed 인증서 발급 과정 모식도","date":"2022-08-27","permalink":"https://leeleelee3264.github.io/post/2022-08-27-digital-certificate-part-final/","tags":["Infra"],"title":"[Infra] 네트워크 필수교양, 인증서 (2/2) - 심화학습"},{"content":"\n이 포스트에서는 Django로 Myinfo oauth2 클아이언트를 만드는 프로젝트를 다룬다. 끝에서는 Python에서 jwcrypto와 Crypto를 이용해 PKI를 다루는 법을 알아본다.\n[github]\n[api document]\n[quick start]\nIndex\nMyinfo란? 프로젝트 목표 프로젝트 구현 프로젝트 회고 Myinfo란? 싱가폴 Mydata 서비스 정부가 주도한 Mydata 서비스가 몇 개의 나라에 있다고 하는데, 싱가폴 정부의 Singpass는 그 중에서도 모범사례로 뽑힌다고 한다. Singpass는 싱가포르의 15세 이상의 인구 중 97% 가 쓰고 있는 아주 활발한 서비스이다.\nSingpass에 있는 여러가지 서비스 중 Myinfo는 Person Data를 제공하는 서비스로, 한국의 카카오나 네이버 아이디 처럼 ouath2 로그인과 회원가입을 할 수 있다. [Picture 1] 에서 singpass에 대해서 조금 더 자세히 살펴볼 수 있다.\n[Picture 1] Introduce Singpass Myinfo oauth2 Myinfo는 [Picture 2] 와 같은 oauth2 구조를 가진다.\n[Picture 2] Myinfo oauth2 구조 Resource Owner Myinfo 사용자 Application 내가 구현하는 connector로, Myinfo 사용자의 데이터를 사용하는 주체 Identify Providers / Service Authorization Platform 인증서버 사용자의 인증정보와 권한 정보를 소유한 서버 Singpass 로그인 페이지 제공 Resource Server 사용자 데이터를 소유한 서버 인증 서버에 로그인 성공 후 접근 Myinfo Resource API 권한 인증 요청 [authorise api] GET /v3/authorise Singpass 로그인 페이지를 불러온다. 로그인 후 Singpass에서 사용자의 데이터를 불러오는 것에 대한 동의를 진행한다. 사용자가 동의했을 경우 authcode를 return 한다. Token 요청 [token api] POST /v3/token authocode를 사용하여 token을 요청한다. PKI를 사용하여 인증을 진행한다. 인증이 완료되면 token을 return 한다. 사용자 정보 요청 [person api] GET /v3/person/{sub} token 속의 access token을 사용하여 사용자 정보를 요청한다. 사용자 정보를 return 한다. 프로젝트 목표 Singpass가 제공하는 [Java]와 [node.js] 버전의 client connector 처럼 이번에 프로젝트로 python 버전의 connector를 만들었다. 아예 하나의 REST API 형태로 제공을 하기 위해 프레임워크로 Django를 선택했다.\n프로젝트 목표 프로젝트를 진행하면서 이루고자 한 목표는 아래와 같다. 대부분의 토이 프로젝트가 제대로 정리가 되어있지 않거나 코드가 엉망으로 짜여질 때가 많아서 이번에는 처음부터 확실하게 목표를 설정했다.\nCode Quality\nDDD 아키텍처로 서버를 구현한다. 최소 2회의 리팩토링을 진행한다. python lint(flake8, pylint, mypy)를 사용하여 최대한 python style을 고수한다. Pipenv를 사용해서 python 패키지를 관리한다. Documentation\nGithub README를 작성한다. API document를 작성한다. Project에 대한 블로그 포스팅을 작성한다. Dev Stack stack info Backend Language Python Backend Framework Django Code Architecture Domain Driven Desgin Python Package Managment Pipenv API Security PKI Version Control Github API Document GitBook 프로젝트 구현 Make Request Myinfo oauth2 구조를 반영하여, connector 서버의 호출 flow를 [Picture 3] 같이 설계했다.\n[Picture 3] connector 서버 호출 flow Step 1: Get myinfo login url Request GET /users/me/external/myinfo-redirect-login\ncurl -i -H 'Accept: application/json' \u0026lt;http://localhost:3001/user/me/external/myinfo-redirect-login\u0026gt; Response { \u0026quot;message\u0026quot;: \u0026quot;OK\u0026quot;, \u0026quot;data\u0026quot;: { \u0026quot;url\u0026quot;: \u0026quot;https://test.api.myinfo.gov.sg/com/v3/authorise?client_id=STG2-MYINFO-SELF-TEST\u0026amp;attributes=name,dob,birthcountry,nationality,uinfin,sex,regadd\u0026amp;state=eb03c000-00a3-4708-ab30-926306bfc4a8\u0026amp;redirect_uri=http://localhost:3001/callback\u0026amp;purpose=python-myinfo-connector\u0026quot;, \u0026quot;state\u0026quot;: \u0026quot;eb03c000-00a3-4708-ab30-926306bfc4a8\u0026quot; } } Step 2: Browse myinfo login url Request curl \u0026lt;https://test.api.myinfo.gov.sg/com/v3/authorise?client_id=STG2-MYINFO-SELF-TEST\u0026amp;attributes=name,dob,birthcountry,nationality,uinfin,sex,regadd\u0026amp;state=eb03c000-00a3-4708-ab30-926306bfc4a8\u0026amp;redirect_uri=http://localhost:3001/callback\u0026amp;purpose=python-myinfo-connector\u0026gt; Response [Picture 4] Myinfo Login Page Step 3: Login and check agree terms Login Check Login page in [Picture 5]\nAgree Terms [Picture 5] Myinfo Terms Agreement Page Step 4: Callback API get called by Myinfo Myinfo에서 Request를 하는 Step이다.\n로그인을 하고 terms에 동의를 하면 Myinfo에서 connector client의 callback API를 호출해 authcode를 넘겨준다.\nRequest GET /callback?{code}\ncurl \u0026lt;http://localhost:3001/callback?code=8932a98da8720a10e356bc76475d76c4c628aa7f\u0026amp;state=e2ad339a-337f-45ec-98fa-1672160cf463\u0026gt; Response [Picture 6] Callback Response Page\nFinal Step: Get Person data 자동화된 Step이다.\nCallback API의 응답인 callback 페이지는 자동으로 connector client의 person data API를 호출하도록 했다. 해당 API가 Myinfo에서 사용자 정보를 가져오는 마지막 단계이다.\nRequest GET /users/me/external/myinfo\ncurl -i -H 'Accept: application/json' \u0026lt;http://localhost:3001/user/me/external/myinfo\u0026gt; Response { \u0026quot;message\u0026quot;: \u0026quot;OK\u0026quot;, \u0026quot;sodata\u0026quot;: { \u0026quot;regadd\u0026quot;: { \u0026quot;country\u0026quot;: { \u0026quot;code\u0026quot;: \u0026quot;SG\u0026quot;, \u0026quot;desc\u0026quot;: \u0026quot;SINGAPORE\u0026quot; }, \u0026quot;unit\u0026quot;: { \u0026quot;value\u0026quot;: \u0026quot;10\u0026quot; }, \u0026quot;street\u0026quot;: { \u0026quot;value\u0026quot;: \u0026quot;ANCHORVALE DRIVE\u0026quot; }, \u0026quot;lastupdated\u0026quot;: \u0026quot;2022-07-14\u0026quot;, \u0026quot;block\u0026quot;: { \u0026quot;value\u0026quot;: \u0026quot;319\u0026quot; }, \u0026quot;source\u0026quot;: \u0026quot;1\u0026quot;, \u0026quot;postal\u0026quot;: { \u0026quot;value\u0026quot;: \u0026quot;542319\u0026quot; }, \u0026quot;classification\u0026quot;: \u0026quot;C\u0026quot;, \u0026quot;floor\u0026quot;: { \u0026quot;value\u0026quot;: \u0026quot;38\u0026quot; }, \u0026quot;type\u0026quot;: \u0026quot;SG\u0026quot;, \u0026quot;building\u0026quot;: { \u0026quot;value\u0026quot;: \u0026quot;\u0026quot; } }, \u0026quot;dob\u0026quot;: \u0026quot;1988-10-06\u0026quot;, \u0026quot;sex\u0026quot;: \u0026quot;M\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;ANDY LAU\u0026quot;, \u0026quot;birthcountry\u0026quot;: \u0026quot;SG\u0026quot;, \u0026quot;nationality\u0026quot;: \u0026quot;SG\u0026quot;, \u0026quot;uinfin\u0026quot;: \u0026quot;S6005048A\u0026quot; } } PKI Digital Signature Myinfo는 PKI Digital Signature를 필요로 한다. 해당 문서에서는 python에서 구현을 할 때 PKI를 사용하는 방법만을 다루기 때문에 PKI에 대한 더 자세한 설명은 링크로 첨부하겠다. [LeeLee- Digital Certificate]\npython 패키지로는 *jwcrypto*와 Crypto를 사용했다.\nPKI 시나리오 connector client private key\n[myinfo token api]를 호출할 때 [myinfo person api]를 호출 할 때 [myinfo person api] 응답 decrypt 할 때 myinfo에서 connector client의 public key로 응답을 암호화 했기 때문 myinfo public key\nmyinfo token api 응답 verify 할 때 myinfo에서 myinfo의 private key로 응답을 암호화 했기 때문 myinfo person api 응답 verify 할 때 myinfo에서 myinfo의 private key로 응답을 암호화 했기 때문 public, privateKey 불러오기 1 2 3 4 5 6 7 8 9 10 11 from jwcrypto import jwk PrivateKey = jwk.JWK PublicKey = jwk.JWK def _get_key(self, key: str) -\u0026gt; Union[PrivateKey, PublicKey]: encode_key = key.encode(\u0026#39;utf-8\u0026#39;) key_dict = jwk.JWK.from_pem(encode_key) return key_dict connector client private key로 서명하기 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 import base64 from Crypto.Hash import SHA256 from Crypto.PublicKey import RSA from Crypto.Signature import PKCS1_v1_5 Signature = str def _sign_on_raw_header( self, base_string: str, private_key: str, ) -\u0026gt; Signature: digest = SHA256.new() digest.update( base_string.encode(\u0026#39;utf-8\u0026#39;), ) pk = RSA.importKey(private_key) signer = PKCS1_v1_5.new(pk) signature = str(base64.b64encode(signer.sign(digest)), \u0026#39;utf-8\u0026#39;) return signature connector client private key로 응답 decrypt 하기 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 import json from jwcrypto import ( jwe, jwk, ) PrivateKey = jwk.JWK def _decrypt( self, encrypted_payload: str, key: PrivateKey, ) -\u0026gt; DecryptedPersonData: params = self._get_decrypt_params(encrypted_payload) encryption = jwe.JWE() encryption.deserialize(params, key) data = encryption.plaintext data_str = json.loads(data) return data_str myinfo public key로 응답 verify 하기 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 import json from jwcrypto import jwk PublicKey = jwk.JWK DecodedPersonData = dict def _decode( self, encoded_payload: str, key: PublicKey, ) -\u0026gt; DecodedPersonData: token = jwt.JWT() token.deserialize(jwt=encoded_payload, key=key) data = token.claims data_dict = json.loads(data) person = DecodedPersonData(**data_dict) return person 프로젝트 회고 문서화 정해둔 목표를 잘 이행한 기분이 들어서 뿌듯했다. 또한 항상 미흡했던 문서화를 꼼꼼하게 해 둔 거 같아 만족스럽다. 하지만 문서화를 하는 과정에서 어떻게 내가 말하고자 하는 바를 더 깔끔하게 글로 옮길 수 있을까 고민을 많이 했고, 아직도 부족한 부분이 많이 보인다. 리팩토링 코드 또한 2번 리팩토링을 진행했지만 포스팅을 위해서 다시 코드를 보니 또 리팩토링 해야겠다는 생각이 든다. 프로젝트를 할 때 1,2번의 리팩토링을 하고 프로젝트가 끝나고 2~3달 지나서 리팩토링을 1 번 진행하면 좋을 거 같다. Boilerplate 어떤 프로젝트를 하더라도 프레임워크 setting을 하는데 초기 시간을 많이 소요하는데, 앞으로 Django로 계속 프로젝트를 진행할 예정이라면 pre-setting이 어느 정도 되어있는 Django Boilerplate 를 만들어야겠다.\n","date":"2022-07-23","permalink":"https://leeleelee3264.github.io/post/2022-07-23-project-myinfo-connector-python/","tags":["Project"],"title":"[Project] Django로  Myinfo oauth2 클라이언트 만들기"},{"content":"\n이 포스트에서는 Python decorator를 사용해 메모리 퍼포먼스, 실행된 쿼리, 시간 측정 등을 하는 초간단 프로파일러를 만드는 법을 배워본다.\nIndex\nprofiler 구현 계기 profiler 구현 개선해야 하는 부분 profiler 구현 계기 Django 환경에서 unittest를 하며 간단하게 퍼포먼스를 측정하고 싶었다. 아주 간단하게 프로파일링을 하면 되기 때문에 Middleware 같은 것은 만들지 않았다. 대신에 Python의 decorator로 만들었다. 앞으로 더 심화되고 유익한 정보를 포함하고 있는 Profiler를 decorator 형식으로 만들면 유용할 것 같다.\n구현한 profiler\n메모리 사용량을 보기 위한 ram_profiler 함수에서 호출된 쿼리와 쿼리 실행시간을 보기 위한 query_profiler 함수 실행시간 을 보기 위한 elapsed_timer profiler 구현 ram_profiler 이 프로파일러를 구현하다가 알게 된 사실인데 Python에서 메모리나 CPU 사용량을 보려면 psutill builtin 패키지를 사용하는 경우가 많았다.\ndecorator는 함수 호출 전과 후의 메모리 사용량을 가져와, 함수가 호출이 되면서 얼추 어느 정도의 메모리를 사용했는지 비교할 수 있도록 한다.\nram_profiler 구현 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 def ram_profiler(fn): \u0026#34;\u0026#34;\u0026#34; 퍼포먼스 체크를 위한 메모리 프로파일러 메모리 사용량을 측정할 함수 위에 @ram_profiler 를 추가해주시면 됩니다. 체크하는 메모리는 아래와 같습니다. 1) 그냥 현재 memory usage 정보를 그대로 가져오는 경우 2) 현재 process id를 통해 해당 프로세스의 memory usage를 정확하게 비교하는 경우 ex) @ram_profiler def pre_allot_offering(offering: Offering): \u0026#34;\u0026#34;\u0026#34; def inner(*args, **kwargs): print(\u0026#39;\\n\u0026#39;) print(\u0026#34;===== memory usage check =====\u0026#34;) memory_usage_dict = dict(psutil.virtual_memory()._asdict()) memory_usage_percent = memory_usage_dict[\u0026#39;percent\u0026#39;] print(f\u0026#34;BEFORE CODE :: memory_usage_percent: {memory_usage_percent}%\u0026#34;) pid = os.getpid() current_process = psutil.Process(pid) current_process_memory_usage_as_kb = current_process.memory_info()[0] / 2. ** 20 print(f\u0026#34;BEFORE CODE :: Current memory KB : {current_process_memory_usage_as_kb: 9.3f} KB\u0026#34;) target_func = fn(*args, **kwargs) print(\u0026#34;==\u0026#34; * 20) print(f\u0026#34;== {fn.__name__} memory usage check ==\u0026#34;) memory_usage_dict = dict(psutil.virtual_memory()._asdict()) memory_usage_percent = memory_usage_dict[\u0026#39;percent\u0026#39;] print(f\u0026#34;AFTER CODE :: memory_usage_percent: {memory_usage_percent}%\u0026#34;) pid = os.getpid() current_process = psutil.Process(pid) current_process_memory_usage_as_kb = current_process.memory_info()[0] / 2. ** 20 print(f\u0026#34;AFTER CODE :: Current memory KB : {current_process_memory_usage_as_kb: 9.3f} KB\u0026#34;) print(\u0026#39;\\n\u0026#39;) return target_func return inner ram_profiler 결과 ===== memory usage check ===== BEFORE CODE :: memory_usage_percent: 79.7% BEFORE CODE :: Current memory KB : 197.250 KB ======================================== == pre_allot_offering memory usage check == AFTER CODE :: memory_usage_percent: 79.7% AFTER CODE :: Current memory KB : 197.312 KB query_profiler decorator는 쿼리를 관리하는 context를 임포트 해와서 타겟 함수를 실행하면서 호출되었던 쿼리들을 캡처해두었다가 함수가 끝이 나면 보여준다. ORM에서 실제로 어떤 쿼리가 실행되는지 봐야 할 때, 조건문에 따라 쿼리가 달라질 때, N + 1 쿼리를 잡아 낼 때 사용하기 좋다.\nquery_profiler 구현 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 def query_profiler(fn): \u0026#34;\u0026#34;\u0026#34; 호출이 된 query를 확인하기 위한 프로파일러 호출 된 query를 확인할 함수 위헤 @query_profiler 를 추가해주시면 됩니다. ex) @query_profiler def pre_allot_offering(offering: Offering): \u0026#34;\u0026#34;\u0026#34; def inner(*args, **kwargs): print(\u0026#39;\\n\u0026#39;) print(f\u0026#34;===== {fn.__name__} called query check =====\u0026#34;) with CaptureQueriesContext(connection) as context: target_func = fn(*args, **kwargs) for index, query in enumerate(context.captured_queries): sql = query.get(\u0026#39;sql\u0026#39;) time = query.get(\u0026#39;time\u0026#39;) print(f\u0026#39;CALLED QUERY :: [{index}]\u0026#39;) print(f\u0026#39;CALLED QUERY :: query: {sql}\u0026#39;) print(f\u0026#39;CALLED QUERY :: executed time: {time}\u0026#39;) print(\u0026#34;=====\u0026#34;) return target_func return inner query_profiler 결과 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 ===== pre_allot_offering called query check ===== CALLED QUERY :: [0] CALLED QUERY :: query: SAVEPOINT `s4377609600_x5` CALLED QUERY :: executed time: 0.001 ===== CALLED QUERY :: [1] CALLED QUERY :: query: SELECT `offering_subscription`.`id` # 실제 쿼리는 블라인드 CALLED QUERY :: executed time: 0.00 ===== CALLED QUERY :: [2] CALLED QUERY :: query: UPDATE `offering_subscription` SET # 실제 쿼리는 블라인드 CALLED QUERY :: executed time: 0.003 ===== CALLED QUERY :: [3] CALLED QUERY :: query: SELECT `offering_subscription`.`id`, # 실제 쿼리는 블라인드 ===== CALLED QUERY :: [4] CALLED QUERY :: query: SELECT `offering_subscription`.`id`, # 실제 쿼리는 블라인드 CALLED QUERY :: executed time: 0.002 ===== CALLED QUERY :: [5] CALLED QUERY :: query: UPDATE `offering_subscription` SET # 실제 쿼리는 블라인드 CALLED QUERY :: executed time: 0.002 ===== CALLED QUERY :: [6] CALLED QUERY :: query: RELEASE SAVEPOINT `s4377609600_x5` CALLED QUERY :: executed time: 0.001 ===== elapsed_timer decorator는 함수의 시작시간과 끝나는 시간을 측정하여 함수 실행시간이 어느정도인지를 계산한다.\nelapsed_timer 구현 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 def elapsed_timer(fn): \u0026#34;\u0026#34;\u0026#34; 퍼포먼스 체크를 위한 실행 시간 측정 타이머 실행 시간을 측정할 함수 위에 @elapsed_timer 를 추가해주시면 됩니다. 퍼포먼스 테스트가 끝나면 지워주시길 바랍니다. ex) @elapsed_timer def pre_allot_offering(offering: Offering): \u0026#34;\u0026#34;\u0026#34; def inner(*args, **kwargs): print(\u0026#39;\\n\u0026#39;) print(f\u0026#34;===== {fn.__name__} elapsed time check =====\u0026#34;) start = perf_counter() target_func = fn(*args, **kwargs) end = perf_counter() print(f\u0026#39;ELAPSED :: total: start: {start} sec - end: {end} sec\u0026#39;) duration = end - start print(f\u0026#39;ELAPSED :: duration: {duration} sec\u0026#39;) print(f\u0026#39;ELAPSED :: duration in minutes : {str(timedelta(seconds=duration))} mins\u0026#39;) print(\u0026#39;\\n\u0026#39;) return target_func return inner elapsed_timer 결과 1 2 3 4 ===== pre_allot_offering elapsd time check ===== ELAPSED :: total: start: 8.705878 sec - end: 8.72449375 sec ELAPSED :: duration: 0.018615750000000375 sec ELAPSED :: duration in minutes : 0:00:00.018616 mins (builtin) @profiler 메모리를 프로파일링하는 것에 한정한다면 Python에서 builtin으로 제공하는 memory_profiler 라는 것이 있다. 패키지 안의 @profiler 데코레이터를 사용하면 함수 안에서 코드가 실행이 될 때 한 줄 한 줄 얼마의 메모리를 사용했는지를 볼 수 있다.\nORM을 포함하고 있는 코드에도 사용을 해보려 했는데 recursive 하게 동작을 하다가 스택이 터져서 사용을 할 수 없었다. 함수가 한 줄 씩 호출되면서 메모리 사용량을 보기에는 정말 좋은 데코레이터 처럼 보이는데 꼭 다음에 써보도록 해야겠다.\n@profiler 사용 예시 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 from memory_profiler import profile @profile def main_func(): import random arr1 = [random.randint(1,10) for i in range(100000)] arr2 = [random.randint(1,10) for i in range(100000)] arr3 = [arr1[i]+arr2[i] for i in range(100000)] del arr1 del arr2 tot = sum(arr3) del arr3 print(tot) if __name__ == \u0026#34;__main__\u0026#34;: main_func() @profiler 결과 예시 1 2 3 4 5 6 7 8 9 10 11 12 13 Line # Mem usage Increment Line Contents ================================================ 3 37.0 MiB 37.0 MiB @profile 4 def main_func(): 5 37.0 MiB 0.0 MiB import random 6 37.6 MiB 0.3 MiB arr1 = [random.randint(1,10) for i in range(100000)] 7 38.4 MiB 0.3 MiB arr2 = [random.randint(1,10) for i in range(100000)] 8 39.9 MiB 0.5 MiB arr3 = [arr1[i]+arr2[i] for i in range(100000)] 9 39.9 MiB 0.0 MiB del arr1 10 38.0 MiB 0.0 MiB del arr2 11 38.0 MiB 0.0 MiB tot = sum(arr3) 12 37.2 MiB 0.0 MiB del arr3 13 37.2 MiB 0.0 MiB print(tot) 개선해야 하는 부분 Python의 Decorator는 함수의 시작 - 끝 부분에 추가 action을 할 수 있게 해주는 Wrapper이다. 함수 중간중간에 프로파일링을 해야 한다면 다른 방법으로 접근을 해야 할 것 같다.\n","date":"2022-06-29","permalink":"https://leeleelee3264.github.io/post/2022-06-30-python-profiler-decorator/","tags":["Project"],"title":"[Project] Python decorator를 사용해 초간단 프로파일러 만들기"},{"content":"\n이 포스트에서는 인증서의 기본 개념을 다룬다: CSR, SSL certificate, code signing certificate, FQDN, PKI, X.509\nIndex\n인증서에 대해 잘못 알고 있던 점들 인증서의 종류 인증서와 관련된 개념 인증서에 대해 잘못 알고 있던 점들 인증서의 종류 적절하지 않은 인증서 분류 web에서 HTTPS 통신을 하기 위해 사용하는 web secure용 SSL 인증서 digital signature(디지털 서명) 를 하기 위한 code signing 인증서 디지털 서명을 해야 했기 때문에 code signing 인증서를 발급받으려 했다. 하지만 SSL 인증서도 디지털 서명을 할 수 있었고, 목표인 server to server 커뮤니케이션에서 사용하는 신원 보장용 디지털 서명을 하기 위해서는 SSL 인증서를 발급해야 했다.\n[Picture 1] 적절하지 않은 인증서 분류 적절한 인증서 분류 SSL 인증서는 웹사이트, 정확히 말하면 브라우저에 한정해서 사용한다고 생각했는데 이는 너무 한정적인 생각이었다. SSL 인증서는 client와 server가 (browser to server / server to server) 데이터를 주고 받을 때 암호화를 하기 위해 쓰이기 때문에 브라우저에 한정적이지 않다.\n결국 인증서는 [Picture 2] 처럼 분류를 해야 한다.\n[Picture 2] 적절한 인증서 분류 Code Signing 인증서 발급받으려 했던 code signing 인증서는 어플리케이션에 서명을 해서 해당 어플리케이션 publisher의 신원을 보장하는데 사용하기 때문에 내가 필요했던 server to server 커뮤니케이션에서 사용되는 인증서가 아니다.\ncode signing 인증서로 어플리케이션에 서명을 하게 되면 [Picture 3] 와 같이 사용자에게 어플리케이션 publisher 에 대한 정보가 추가로 제공이 된다.\n[Picture 3] code signing 인증서로 서명한 application CSR 생성하기 모든 SSL 인증서는 domain validation을 한다. 이 생각에 사로잡혀서 Root CA가 서명한 인증서를 발급받기 위해 제출해야 하는 CSR는 인증서가 사용이 될 서버에서 만들어야 한다고 생각했다.\n예를 들어 leelee.me 이라는 루트 도메인에서 사용하는 인증서라고 했을 때 leelee.me 도메인을 호스팅하는 서버에서 CSR을 생성해야 한다고 생각한 것이다. 하지만 CSR을 다른 서버에서 생성했다면, CSR을 생성할 때 함께 만든 private key를 인증서를 사용할 서버로 옮기면 된다.\n인증서의 종류 SSL Certificate VS Code Signing Certificate 공통점\nX.509 형태의 디지털 인증서다. PKI 형식을 사용한다. 두 인증서가 없다면 사용자에게 보안 경고가 띄워진다. 발급하기 전에 CA에서 발급요청자를 검증한다. end user들이 해킹등의 사이버 범죄에 노출되는 것을 방지하기 위해 사용된다. 차이점\nSSL Certificate은 두 시스템에서 오고가는 data를 암호화한다. Code Signing Certificate은 소프트웨어 자체를 hash \u0026amp; sign 한다. 모든 코드에 디지털 서명을 한 것과 마찬가지이며 만약 중간에서 누가 코드를 수정한다면 해시값이 변경이 되어 end user가 코드를 다운받기 전에 alert를 띄워 사전에 설치를 못 하도록 막는다. [Picture 4] 인증서 다이어그램 SSL Certificate SSL Certificate는 크게 2가지 기준으로 분류할 수 있다.\n검증히려는 대상 커버하는 도메인의 개수 검증하려는 대상 검증 대상에 따라서 암호화하는 방식이 달라지거나 하지 않는다. 따라서 저렴하게 발급받은 DV와 비싸게 발급받은 EV가 결국 보안의 관점에서는 큰 차이가 없다.\n하지만 검증받은 대상의 identiy가 다르기 때문에 DV는 작은 서비스에서 사용하기가 용이하고 그 외에 e-commerce나 중요한 정보를 주고 받아야 하는 서비스의 경우에는 다른 인증서를 써야 한다.\nDomain Validation 도메인 소유권을 확인한다. 손쉽게 발급받을 수 있다. (최대 1일) 가장 많이 사용되는 SSL Certificate의 형태다. Let’s encrypt에서 발급하는 인증서도 DV로, OV나 EV 처럼 다른 인증을 받을 때는 사용할 수 없다. Organization Validation 도메인 소유권 + Organization 존재를 확인한다. 회사 같은 경우, 실제로 존재하는 회사인지를 확인하기 위해 관련 서류를 제출하기도 한다. 발급하는데 몇 시간 또는 며칠 정도가 소요된다. Extended Validation 도메인 소유권 + Organization 존재 확인 + 물리적인 추가 절차를 확인한다. SSL certification industry’s governing consortium 가이드라인을 준수해야 하는 등 발급 절차가 복잡하다. 커버하는 도메인의 개수 인증서에는 도메인이라는 말이 자주 등장하게 된다. 하지만 도메인이라는 단어 자체가 뜻하는 의미가 모호할 수 있기 때문에 혼선방지 차원에서 인증서에서는 FQDN 이라는 단어를 더 선호한다.\nFQDN은 Fully-Qualified Domain Name 의 약자로 도메인 전체 이름을 표기하는 방식을 뜻한다.\nFQDN 예시\n# 아래는 서로 동일한 도메인이다. www.leelee.co.kr leelee.co.kr www.sub.leelee.co.kr # 아래는 서로 다른 FQDN 이다. www.leelee.co.kr leelee.co.kr Single Domain 단 하나의 FQDN을 커버한다. Multiple Domain 여러개의 FQDN을 커버한다. 인증서를 등록할 때 SAN (Subject Alternative Name) 항목에 다수의 도메인을 입력한다. Wildcard Domain 하나의 Root Domain과 무제한의 서브도메인을 커버한다. wildcard는 하나의 전체도메인을 입력하지 않기 때문에 FQDN이라 하지 않았다. CA 업체에서는 비용에 따라서 커버하는 서브도메인 개수에 제한을 두기도 한다. * 을 사용해서 도메인을 커버한다. 커버하는 도메인 예시\n# Singple Certificate www.leelee.co.kr # Multi-Domain Certificate www.leelee.co.kr leelee.co.kr www.sub.leelee.co.kr # Wildcard Certificate *.leelee.co.kr 인증서와 관련된 개념 PKI PKI는 Public Key Infrastructure 의 약자로 공개키 기반구조라고 하며 디지털 인증서를 생성, 관리, 배포, 사용, 저장 및 파기, 공개키 암호화의 관리에 필요한 정책 등을 뜻한다. PKI는 아래의 항목을 전체로 하고 있다.\n비대칭 키 알고리즘을 이용한다. Private key, Public key pair 를 생성한다. Private key는 개인만 소유하며 밖으로 공개해서는 안된다. Public key는 공개하는 키로, 누구에게든 공개해도 된다. 비대칭 키 알고리즘 Private key, Public key 두 개의 키를 사용한다. 이처럼 Encryption(암호화)와 Decryption(복호화)에 두 개의 다른 키를 쓰기 때문에 비대칭 키라고 한다. 대표적인 알고리즘으로는 RSA가 있다.\n적용 시나리오 Public key를 이용해 암호화 : 데이터 보안 사용자 B가 사전에 공유받은 사용자 A의 public key를 이용하여 데이터를 암호화한다. 사용자 A의 public key로 암호화된 데이터는 오직 사용자 A의 private key로 복호화 할 수 있다. 사용자 A의 private key는 사용자 A만 소유하고 있기 때문에 사용자 B는 사용자 A만 볼 수 있는 데이터를 전송하게 된 것이다. [Picture 5] Public key를 이용한 암호화 Private key를 이용해 암호화 : 신원인증 사용자 A의 private key로 암호화 된 데이터는 사용자 A의 public key를 이용해야만 복호화를 할 수 있다. 데이터가 사용자 A의 public key로 복호화가 안된다면 사용자A가 보냈다는 것을 인증할 수 없다. 사용자 A의 public key로 데이터를 복호화할 수 있다면 사용자 A가 보냈다는 것을 인증할 수 있다. [Picture 6] Private key를 이용한 암호화 해커가 암호화된 데이터를 도청할 경우 사용자 A의 private key가 없기 때문에 해커가 중간에서 데이터를 복호화ㅊ할 수 없다. [Picture 7] 해킹 시도 PKI에서 가능한 두 가지 방식의 네트워크 보안 public key를 이용해 암호화 하면 원하는 상대방에게만 데이터를 공개할 수 있다. from 사용자 B → to 사용자 A private key를 이용해 암호화 하면 신원 인증을 할 수 있다. from 사용자 A → to 사용자 B X.509 X.509는 디지털 인증서 생성 관련 국제 표준을(format)을 의미한다. X.509를 사용하는 네트워크 노드들은 전세계적으로 약속된 x.509 국제 표준을 방식으로 디지털 ID를 생성해 서로의 신원을 증명할 수 있다.\nX.509는 인터넷의 다양한 분야에서 신원 확인을 위해 광범위하게 사용되고 있는 가장 유명한 디지털 신원 증명 방식이다.\nX.509 version 3 인증서 양식 [Picture 8] X.509 인증서 양식 Key usage extension 인증서에 포함되어있는 public key의 목적을 나타낸다. Key usage extension을 설정해 public key의 사용처를 제한할 수 있다.\nKey usage extension 예시\n인증서가 서명을 하거나 signature를 검증하기 위해 사용된다면 Key usage extension으로 Digital signature와 Non-repudication 를 설정한다. 인증서가 key 관리를 위해서만 사용이 된다면 key encipherment를 설정한다. [Picture 9] Key usage extension 종류 Extended key usage Key usage extension이 기본적인 인증서의 사용 목적(purpose)를 나타내는 것이라면 Extended key usage는 인증서의 additional한 사용 목적을 나타낸다. 보통 Extended key usage는 end entity의 인증서에만 표시가 된다.\nExtended key usage 예시\ncritical이라면 인증서는 설정되어있는 용도로만 사용을 해야 한다. 해당 인증서를 다른 용도로 사용 했을 경우에는 CA 정책에 위반된다. non-critical 이라면 설정되어있는 용도 외의 다른 용도로 사용 했을 경우에도 CA의 제한에 걸리지 않는다. 또한 다수의 kye/certificate을 가지고 있는 entity라면 맞는 key/certificate를 찾는데에도 사용이 될 수 있다. [Picture 10] Extend key usage extension 종류 CSR Certificate Signing Request의 약자이다. CA에 인증서를 서명해달라고 요청할 때 사용이 된다. 인증서 발급할 때만 사용이 되고, 인증서가 발급된 후에는 별도로 사용 되지 않는다. Private Key PKI의 핵심이 되는 비밀키이다. 만료 기간이 별도로 존재하지 않는다. Public Key PKI의 또 다른 핵심이 되는 공개키이다. Public Key라고 따로 파일이 존재하기 보다는 Certificate에 포함이 되어있다. 때문에 public key와 certificate을 혼용해서 쓴다. 만료 기간이 별도로 존재하지 않는다. Certificate public key와 각종 정보를 담고 있는 인증서로, x.509 형식으로 되어있다. 만료 기간이 존재한다. 때문에 인증서가 만료되면 새로운 인증서를 발급받아야 한다. PKI File Extension PEM (Privacy Enhanced Mail) 형식 은 가장 흔하게 사용되는 X.509 인증서 형식인데 .crt, .pem, .cer, .key 확장자 모두가 PEM 형식이다. \u0026mdash;\u0026ndash;BEGIN CERTIFICATE\u0026mdash;\u0026ndash; 로 시작한다.\n.cert *.crt Certificate를 위한 확장자다. Base64 encoded X.509 certificate DER encoded X.509 certificate 해당 확장자들은 Private key를 지원하지 않는다. .key Private key 확장장다. .pem Certificate를 위한 확장자다. .crl Certificate Revoke List를 위한 확장자다. Certificate Revoke List는 폐기된 인증서의 목록이며, CA는 CRL을 통해 폐기된 인증서를 관리한다. .csr Certificate Singing Request를 위한 확장자다. .der Certificate을 위한 인증서 확장자인데, DER encoded X.509 Certificate에 한정된다. 해당 확장자는 Private key를 지원하지 않는다. 보통의 인증서들과 달리 ----BEGIN CERTIFICATE----- 로 시작하지 않는다. Java contexts에서 자주 사용이 된다. .der 인증서 예시\n3082 07fd 3082 05e5 a003 0201 0202 1068 1604 dff3 34f1 71d8 0a73 5599 c141 7230 0d06 092a 8648 86f7 0d01 010b 0500 3072 310b 3009 0603 5504 0613 0255 5331 0e30 0c06 0355 0408 0c05 5465 7861 7331 1030 0e06 0355 0407 0c07 486f 7573 746f 6e31 1130 0f06 0355 040a 0c08 5353 4c20 436f 7270 312e 302c 0603 5504 030c 2553 534c 2e63 6f6d 2045 5620 5353 4c20 496e 7465 726d 6564 6961 7465 2043 4120 5253 4120 ","date":"2022-06-15","permalink":"https://leeleelee3264.github.io/post/2022-06-15-digital-certificate-part-one/","tags":["Infra"],"title":"[Infra] 네트워크 필수교양, 인증서 (1/2) - 쌩기초"},{"content":"\nBreet Slatkin의 저서 [Effective Python]을 요약한다. 이 포스트에서는 PEP8, bytes and str, f formatter, range, enumerate, zip를 다룬다.\nIndex\nBetter way 1 사용중인 파이썬의 버전을 알아두라 Better way 2 PEP8 스타일 가이드를 따르라 Better way 3 bytes와 str의 차이를 알아두라 Better way 4 C 스타일 형식 문자열을 str.format과 쓰기보다는 f-문자열을 통한 인터플레이션을 사용하라 Better way 5 복잡한 식을 쓰는 대신 도우미 함수를 작성하라 Better way 6 인덱스를 사용하는 대신 대입을 사용해 데이터를 언패킹하라 Better way 7 range보다는 enumerate를 사용하라 Better way 8 여러 이터레이터에 대해 나란히 루프를 수행하려면 zip을 사용하라 Better way 9 for나 while 루프 뒤에 else 블록을 사용하지 말라 Better way 10 대입식을 사용해 반복을 피하라 Better way 1 사용중인 파이썬의 버전을 알아두라 파이썬 버전 콘솔 명령어 예시\n1 python --version 파이썬 버전 코드 예시\n1 2 3 4 5 6 7 import sys print (sys.version_info) \u0026gt;\u0026gt;\u0026gt; sys.version_info(major=3, minor=8, micro=12, releaselevel=\u0026#39;final\u0026#39;, serial=0) print(sys.version) \u0026gt;\u0026gt;\u0026gt; 3.8.12 (default, Oct 22 2021, 17:47:41) \u0026gt;\u0026gt;\u0026gt;[Clang 13.0.0 (clang-1300.0.29.3)] 파이썬2 파이썬2는 2020년 1월 1일부로 수명이 다했다. 이제 더이상 버그수정, 보안 패치가 이뤄지지 않는다. 공식적으로 지원을 하지 않은 언어이기 때문에 사용에 대한 책임은 개발자에게 따른다.\n파이썬2로 작성된 코드를 사용해야 한다면 2to3, six와 같은 라이브러리의 도움을 받아야 한다.\nsix 사용 예시\n1 2 3 4 return ( six.text_type(user.pk) + six.text_type(timestamp) + six.text_type(user.username) + six.text_type(user.uuid) + six.text_type(user.type) ) Better way 2 PEP8 스타일 가이드를 따르라 PEP8는 파이썬 코드를 어떤 형식으로 작성할지 알려주는 스타일 가이드다. 문법만 올바르다면 어떤 방식으로 원하든 파이썬 코드를 작성해도 좋지만, 일관된 스타일을 사용하면 코드에 더 친숙하게 접근하고, 코드를 더 쉽게 읽을 수 있다.\nPEP8 스타일 가이드 공백 탭 대신 스페이스를 사용해 들여쓰기를 하라. 문법적으로 중요한 들여쓰기에는 4칸 스페이스를 사용하라. 라인 길이는 79개 문자 이하여야 한다. 긴 식을 다음 줄에 이어서 쓸 경우에는 일반적인 들여쓰기보다 4스페이스를 더 들여써야 한다. 파일 안에서 각 함수와 클래스 사이에는 빈 줄 두 줄 넣어라. 클래스 안에서 메서드와 메서드 사이에는 빈 줄을 한 줄 넣어라. dict에서 키와 콜론(:) 사이에는 공백을 넣지 않고, 한 줄 안에 키와 값을 같이 넣는 경우에는 콜론 다음에 스페이스를 하나 넣는다. 변수 대입에서 = 전후에는 스페이스를 하나씩만 넣는다. 타입 표기를 덧붙이는 경우에는 변수 이름과 콜론 사이에 공백을 넣지 않도록 주의하고, 콜론과 타입 정보 사이에는 스페이스를 하나 넣어라. 공백 예시\n1 2 def add(a: int, b: int): return a + b 명명 규약 (name convention) 함수, 변수, 애트리뷰트 명명 규약 함수, 변수, 애트리뷰트는 lowercase_underscore 처럼 소문자와 밑줄을 사용해야 한다. snake case 모듈 수준의 상수는 ALL_CAPS처럼 모든 글자를 대문자로 하고, 단어와 단어 사이를 밑줄로 연결한 형태를 사용한다. private 명명 규약 보호해야(protect) 하는 인스턴스 애트리뷰트는 일반적인 애트리뷰트 이름 규칙을 따르되, 밑줄로 시작한다. private 인스턴스 애트리뷰는 밑줄 두 줄로 시작한다. 밑줄 두 줄은 maigc method를 위한 컨벤션인줄 알았는데, 클래스에서 private 메소드와 애트리뷰트를 밑줄 두줄로 만든다. 파이썬에서 private, protect 모두 정식 지원을 하지 않기 때문에, 가독성을 위한 네임 컨벤션에 더 가깝다. 클래스 명명 규약 클래스와 예외는 CapitalizedWord 처럼 여러 단어를 이어 붙이되, 각 단어의 첫 글자를 대문자로 만든다. PascalCase, CamelCase 클래스에 들어있는 인스턴스 메서드는 호출 대상 객체를 가리키는 첫번쨰 인자의 이름으로, 반드시 self를 사용해야한다. 자기자신을 가르키기 때문에 클래스 메서드는 클래스를 가르키는 첫 번째 인자의 이름으로 반드시 cls를 사용해야 한다. 클래스 자체를 가르키기 때문에 함수, 변수 명명 예시\n1 2 3 4 _count = 100 def _add(a, b): return a + b private 인스턴스 명명 예시\n1 2 3 4 __count = 100 def __add(a, b): return a + b 식과 문 읽기 쉬운 식 작성 긍정적인 식을 부정하지 말고, 부정을 내부에 넣어라. 한 줄짜리 if 문이나 한 줄짜리 for, while 루프, 한 줄 짜리 except 복합문을 사용하지 말라. 명확성을 위해 각 부분을 여러 줄에 나눠 배치하라. 식을 한 줄 안에 다 쓸 수 없는 경우, 식을 괄호로 둘러싸고 줄바꿈과 들여쓰기를 추가해서 읽기 쉽게 만들라. 여러 줄에 걸쳐 식을 쓸 때는 줄이 계속된다는 표시를 하는 \\ 문자 보다는 괄호를 사용하라. editor를 사용하다보면 이렇게 \\ 되는 형식을 많이 사용하는데, PEP8에서 권장하지 않은 형태였다니, 앞으로 ()를 써보도록 노력하겠다. 빈 컨테이너 확인 빈 컨테이너나 시퀸스([], ‘’)등을 검사할 때는 길이를 0과 비교하지 말라. 빈 컨테이너나 시퀸스 값이 암묵적으로 False 취급된다는 사실을 활용해라. 파이썬을 자바와 동실시 해서, 얼마전에 str ≠ ‘’ 로 비어있는 여부를 검사한 적이 있다. 파이썬에서 비어있는 str는 False 취급이라는 것을 꼭 기억하자. 마찬가지로 비어 있지 않은 컨테이너나 시퀸스([1], ‘hi’) 등을 검사할 때는 길이를 0과 비교하지 말라. 컨테이너가 비어있지 않은 경우 암묵적으로 True 평가가 된다는 사실을 활용해라. 내부 부정 예시\n1 2 3 4 5 6 7 8 # 긍정적인 식을 부정 if not a is b: # 부정을 내부에 넣음 if a is not b: # 비교 대상이 없이 자기 자신만 있을 때는 이렇게 해도 된다. if not a: 빈 컨테이너 확인 예시\n1 2 3 4 5 6 7 8 9 10 11 12 d = {} st = \u0026#39;\u0026#39; if not d: if not st: # Don\u0026#39;t do 1 if str == \u0026#39;\u0026#39;: # Dont\u0026#39;do 2 if len(d) == 0: 임포트 임포트 문은 평소에 정말 신경을 쓰지 않았는데, 앞으로는 editor에서 어떻게 임포트를 하는지 보고, 신경 써서 작성하도록 하자.\nimport 문을 항상 파일 맨 앞에 위치시켜라. 모듈을 임포트할 때는 절대적인 이름을 사용하고, 현 모듈의 경로에 상대적인 이름은 사용하지 말라. 반드시 상대적인 경로로 임포트를 해야 하는 경우에는 명시적인 구문을 사용하라. 임포트 순서 임포트를 적을 때는 아래와 같은 순서로 섹션을 나누고, 각 세션은 알파벳 순서로 모듈을 임포트하라. 표준 라이브러리 모듈 서드 파티 모듈 여러분이 만든 모듈 임포트 예시\n1 2 3 4 5 6 # bar 패키지에서 foo 모듈을 임포트 하려고 할 때 # Do from bar import foo # Don\u0026#39;t do import foo 상대 경로 임포트 예시\n1 from . import foo 결론 개인 프로젝트를 할 때에도 pylint, flake8 등의 파이썬 lint를 이용해서 스타일을 준수하도록 노력하자.\nBetter way 3 bytes와 str의 차이를 알아두라 파이썬에는 문자열 데이터의 시퀸스를 표현하는 두 가지 타입 bytes와 str이 있다.\nbytes 부호가 없는 8바이트 데이터가 그대로 들어가거나, 아스키 인코딩을 사용해 내부 문자를 표시한다.\nbytes 예시\n1 2 a = b\u0026#39;h\\x6511o\u0026#39; c = b\u0026#39;eojwpkmcdlklksm\u0026#39; str 사람이 사용하는 언어의 문자를 표현하는 유니코드 코드 포인트가 들어간다.\n인코딩 str에는 직접 대응하는 이진 인코딩이 없고, bytes에는 직접 대응하는 텍스트 인코딩이 없다.\n때문에 함수를 호출해야 한다.호출 할 때 여러분이 원하는 인코딩 방식을 명시적으로 지정할 수 도 있고 시스템 디폴트 인코딩을 사용할 수 있는데 일반적으로 utf-8이 시스템 디폴트다.\n인코딩 시나리오 유니코드 데이터를 이진 데이터로 변환해야 할 때: str의 encode 메서드 호출 이진 데이터를 유니코드 데이터로 변환해야 할 때: bytes의 decode 메서드 호출 유니코드 샌드위치 유니코드 데이터를 인코딩하거나 디코딩하는 부분을 인터페이스의 가장 먼 경계 지점에 위치시켜라. 이런 방식을 유니코드 샌드위치라고 부른다.\n프로그램의 핵심 부분은 유니코드 데이터가 들어 있는 str를 사용해야 하고, 문자 인코딩에 대해 어떤 가정도 해서는 안된다.\n핵심 함수에는 이미 인코딩이 된 str이 인자로 전달되어야 한다. 이런 접근을 하면 다양한 텍스트 인코딩으로 입력 데이터를 받아들일 수 있고, 출력 텍스트 인코딩은 한 가지로 엄격하게 제한할 수 있다. [Picture 1] 유니코드 샌드위치 블랙박스 str 반환 유니코드 샌드위치 예시\n1 2 3 4 5 6 7 def to_str(bytes_or_str): if isinstance(bytes_or_str, bytes): value = bytes_or_str.decode(\u0026#39;utf-8) else: value = bytes_or_str return value bytes 반환 유니코드 샌드위치 예시\n1 2 3 4 5 6 7 def to_bytes(bytes_or_str): if isinstanceof(bytes_or_str, str): value = bytes_or_str.endode(\u0026#39;utf-8\u0026#39;) else value = bytes_or_str return value 파이썬에서 bytes와 str을 다룰 때 기억해야 하는 점 연산 bytes와 str이 똑같이 작동하는 것처럼 보이지만 각각의 인스턴스는 서로 호환되지 않기 때문에 전달중인 문자 시퀸스가 어떤 타입인지 알아야 한다.\n연산 불가항목 예시\n1 2 3 bytes + str bytes \u0026gt; str b\u0026#39;foo\u0026#39; == \u0026#39;foo\u0026#39; \u0026gt;\u0026gt;\u0026gt; False 파일 (내장함수 open을 호출해 얻은) 파일 핸들과 관련한 연산들이 디폴트로 유니코드 (str) 문자열을 요구하고, 이진 바이트 문자열을 요구하지 않는다.\n이진 쓰기 모드 (’wb’)가 아닌 텍스트 쓰기 모드 (’w’) 로 열면 오류가 난다. bytes로 파일 읽기, 쓰기를 하고 싶다면 이진 읽기모드 (’rb’) 또는 이진 쓰기모드 (’wb’)를 써야 한다.\n파일 불가항목 예시\n1 2 3 4 5 with open(\u0026#39;data.bin\u0026#39;, \u0026#39;w\u0026#39;) as f: f.write(b\u0026#39;\\xf1\\xf2\\xf3\u0026#39;) \u0026gt;\u0026gt;\u0026gt; Traceback ... TypeError: write() argument must be str, not bytes. 파일 시스템의 디폴트 텍스트 인코딩을 bytes.encode(쓰기), str.decode(읽기) 에서 사용하는데 utf-8이기 때문에 이진데이터의 경우 utf-8로 읽지 못하는 경우가 생겨 에러가 발생할 수 있다.\n이런 현상을 막고자, utf-8로 인코딩을 못하는 경우에는 읽어올 때 인코딩을 아예 명시해주는 경우도 있다.\n인코딩 명시 예제\n1 2 with open(\u0026#39;data.bin\u0026#39;, \u0026#39;r\u0026#39;, encoding=\u0026#39;cpl252\u0026#39;) as f: data = f.read() 결론 시스템 디폴트 인코딩을 항상 검사하도록 하자.\n1 python3 -c \u0026#39;import locale; print(locale.getpreferredencoding())\u0026#39; Better way 4 C 스타일 형식 문자열을 str.format과 쓰기보다는 f-문자열을 통한 인터플레이션을 사용하라. 형식화는 미리 정의된 문자열에 데이터 값을 끼워 넣어 사람이 보기 좋은 문자열로 저장하는 과정이다. 파이썬에서는 f-문자열을 통한 인터플레이션 빼고는 각자의 단점이 있다.\n형식 문자열 % 형식화 연산자를 사용한다. 왼쪽에 들어가는 미리 정의된 텍스트 템플릿을 형식 문자열이라고 부른다. C 함수에서 비롯된 방법이다.\n형식 문자열 예시\n1 print(\u0026#39;이진수: %d, 십육진수: %d\u0026#39;, %(a, b)) 형식 문자열 문제점 형식화 식에서 오른쪽에 있는 값들의 순서를 바꾸거나, 타입을 바꾸면 포멧팅이 안되어 오류가 발생한다. 형식화를 조금이라도 변경하면 식이 매우 복잡해 읽기가 힘들다. 같은 값을 여러번 사용하고 싶다면 오른쪽 값을 여러번 복사해야 한다. 내장 함수 format과 str.format 파이썬 3부터는 %를 사용하는 오래된 C스타일 형식화 문자열보다 더 표현력이 좋은 고급 문자열 형식화 기능이 도입됐다. 이 기능은 format 내장 함수를 통해 모든 파이썬 값에 사용할 수 있다.\nformat 예시\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 key = \u0026#39;my_var\u0026#39; value = 1.234 formatted = \u0026#39;{} = {}\u0026#39;.format(key, value) print(formatted) \u0026gt;\u0026gt;\u0026gt; my_var = 1.234 # format 메서드에 전달된 인자의 순서를 표현하는 위치 인덱스를 전달할 수도 있다. formatted = \u0026#39;{1} = {0}\u0026#39;.format(key, value) print(formatted) \u0026gt;\u0026gt;\u0026gt; 1.234 = my_var # Better way 5 형식화 문자열 안에서 같은 위치 인덱스를 여러 번 사용할 수도 있다. formatted = \u0026#39;{}는 음식을 좋아해. {0}가 요리하는 모습을 봐요\u0026#39;.format(name) print(formatted) \u0026gt;\u0026gt;\u0026gt; 철수는 음식을 좋아해. 철수가 요리하는 모습을 봐요. format 문제점 C 스타일의 형식화와 마찬가지로, 값을 조금 변경하는 경우에는 코드 읽기가 어려워진다. 가독성 면에서 거의 차이가 없으며, 둘 다 읽기에 좋지 않다. 인터폴레이션을 통한 형식 문자열 (f-문자열) 위의 문제들을 완전히 해결하기 위해 파이썬 3.6 부터는 인터폴레이션을 통한 형식 문자열 (짧게 f-문자열)이 도입되었다. 이 새로운 언어 문법에서는 형식 문자열 앞에 f 문자를 붙여야 한다. 바이트 문자열 앞에 b 문자를 붙이고, raw 문자열 앞에 r문자를 붙이는 것과 비슷하다.\nf-문자열은 형식화 식 안에서 현재 파이썬 영역에서 사용할 수 있는 모든 이름을 자유롭게 참조할 수 있도록 허용함으로써 이런 간결함을 제공한다.\nf-문자열 예시\n1 2 3 4 5 6 7 8 key = \u0026#39;my_var\u0026#39; value = 1.234 formatted = f\u0026#39;{key} = {value}\u0026#39; print(formatted) \u0026gt;\u0026gt;\u0026gt; my_var = 1.234 파이썬에서 제공하는 형식화 문법의 차이 예시\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 # f-문자열 f_string = f\u0026#39;{key:\u0026lt;10} = {value: .2f}\u0026#39; # c 스타일 c_tuple = \u0026#39;%-10s = %.2f\u0026#39; % (key, value) # c 스타일 + 딕셔너리 c_dict = \u0026#39;%{key}-10s = %{value}.2f\u0026#39; % {\u0026#39;key\u0026#39;: key, \u0026#39;value\u0026#39;: value} # str.format str_args = \u0026#39;{:\u0026lt;10} = {value: .2f}\u0026#39;.format(key, value) # str.format + 키워드 인자 str_kw = \u0026#39;{key:\u0026lt;10} = {value:.2f}\u0026#39;.format(key=key, value=value) 결론 f-문자열은 간결하지만, 위치 지정자 안에 임의의 파이썬 식을 직접 포함시킬 수 있으므로 매우 강력하다. 값을 문자열로 형식화해야 하는 상황을 만나게 되면 다른 대안 대신 f-문자열을 택하라.\n복잡한 식을 쓰는 대신 도우미 함수를 작성하라. 파이썬은 문법이 간결하므로 상당한 로직이 들어가는 식도 한 줄로 매우 쉽게 작성할 수 있다. 예를 들어 URL의 query string를 파싱하고 싶다고 하자.\nquery string 파싱 예시\n1 2 3 4 5 6 7 8 from urllib.parse import parse_qs my_values = parse_qs(\u0026#39;빨강=5\u0026amp;파랑=0\u0026amp;초록=\u0026#39;, keep_blank_values=True) print(repr(my_values)) \u0026gt;\u0026gt;\u0026gt; {\u0026#39;빨강\u0026#39;: [\u0026#39;5\u0026#39;], \u0026#39;파랑\u0026#39;: [\u0026#39;0\u0026#39;], \u0026#39;초록\u0026#39;: [\u0026#39;\u0026#39;] } 이런 때에 파라미터가 없거나 비어 있을 경우 0이 디폴트 값으로 대입되면 좋을 것이다. 여러 줄로 작성해야 하는 if 문(if statement)를 쓰거나 도우미 함수를 쓰지 않고, if 식(if expression)으로 한줄로 처리 할 수 있다.\n뿐만 아니라, 모든 파라메터 값을 정수로 변환해서 즉시 수식에서 활용하기를 바란다고 해보겠다. 그럼 식은 아래와 같아진다.\n한 줄 대입, 변환 예시\n1 2 3 red = int(my_values.get(\u0026#39;빨강\u0026#39;, [\u0026#39;\u0026#39;])[0] or 0) green = int(my_values.get(\u0026#39;초록\u0026#39;, [\u0026#39;\u0026#39;])[0] or 0) opacity = int(my_values.get(\u0026#39;투명도\u0026#39;, [\u0026#39;\u0026#39;])[0] or 0) 현재 이 코드는 너무 읽기 어렵고 시각적 잡음도 많다. 즉, 코드를 이해하기 쉽지 않으므로 코드를 새로 읽는 사람이 이 식이 실제로 어떤 일을 하는지 이해하기 위해 너무 많은 시간을 투자해야 한다.\n코드를 짧게 유지하면 멋지기는 하지만, 모든 내용을 한줄에 우겨 넣기 위해 노력할 만큼의 가치는 없다.\n명확하게 바꾼 코드 예시\n1 2 3 4 5 6 7 def get_first_int(values, key, default=0): found = values.get(key, [\u0026#39;\u0026#39;]) if found[0]: return int(found[0]) return default 식이 복잡해지기 시작하면 바로 식을 더 작은 조간으로 나눠서 로직을 도우미 함수로 옮길지 고려해야 한다. 특히 같은 로직을 반복해 사용할 때는 도우미 함수를 꼭 사용하라. 아무리 짧게 줄여 쓰는 것을 좋아한다 해도, 코드를 줄여 쓰는 것보다 가독성을 좋게 하는 것이 더 가치 있다.\n결론 파이썬의 함축적인 문법이 지저분한 코드를 만들어내지 않도록 하라. 반복하지 말라는 뜻의 DRY 원칙을 따르라.\nBetter way 6 인덱스를 사용하는 대신 대입을 사용해 데이터를 언패킹하라 파이썬에서는 언패킹 구문이 있다. 언패킹 구문을 사용하면 한 문장 안에서 여러 값을 대입할 수 있다. 교재에서는 튜플을 사용할 때 인덱스 말고 언패킹을 사용하는 예제를 보여주고 있다. 언패킹은 튜플 인덱스를 사용하는 것보다 시각적인 잡음이 적다.\nTuple 언패킹 예시\n1 2 3 4 5 item = (\u0026#39;호박\u0026#39;, \u0026#39;식혜\u0026#39;) first, second = item print(first, \u0026#39;\u0026amp;\u0026#39;, second) \u0026gt;\u0026gt;\u0026gt; 호박 \u0026amp; 식혜 언패킹을 이용해서 임시변수 없이 값 맞 바꾸기\n1 2 3 4 5 def bubble_sort(a): for _ in range(len(a)): for i in range(1, len(a)): if a[i] \u0026lt; a[i-1]: a[i-1], a[i] = a[i], a[i-1] # 맞바꾸기 작동원리 대입문의 오른쪽 a[i], a[i=1]이 계산된다. 그 결과값이 이름없는 새로운 tuple에 저장된다. 대입문의 왼쪽에 있는 언패킹 a[i-1], a[i]를 통해 이름없는 tuple에 있는 값이 a[i-1], a[i]에 저장된다. 이름없는 tuple이 사라진다. for 루프와 같은 컴프리헨션, 제너레이터의 리스트 원소를 언패킹하는 방법\n1 2 3 4 snacks = [(\u0026#39;베이컨\u0026#39;, 350), (\u0026#39;도넛\u0026#39;, 240), (\u0026#39;머핀\u0026#39;, 190)] for name, calories in snacks: print(f\u0026#39;{name}은 {calories} 입니다.\u0026#39;) Better way 7 range보다는 enumerate를 사용하라 리스트를 이터레이션하면서 리스트의 몇 번째 원소를 처리 중인지 알아야 할 때가 있다. range를 사용하면 list의 길이를 알아야 하고, 인덱스를 사용해 배열 원소에 접근해야 한다. 단계가 여러개라 결국 코드가 투박해진다.\nrange\n1 2 3 for i in range(len(flavor_list)): flavor = flavor_list[i] print(f\u0026#39;{i}: {flavor}) enumerate는 이터레이터를 제너레이터로 감싼다. 그리고 호출이 될 때 마다 루프 인덱스와 이터레이터의 다음 값으로 이뤄진 튜플 쌍을 리턴한다. 튜플 형태로 넘겨주니 Better way 6처럼 언패킹을 이용하기도 좋다.\nenumerate\n1 2 for i, flavor in enumerate(flavor_list): print(f\u0026#39;{i}: {flavor}) enumarate에서 시작할 루프 인덱스 설정해주기\n1 2 3 4 5 6 for i, flavor in enumerate(flavor_list, 100): print(f\u0026#39;{i}: {flavor}) \u0026gt;\u0026gt;\u0026gt; 100: 바나나 101: 딸기 Better way 8 여러 이터레이터에 대해 나란히 루프를 수행하려면 zip을 사용하라 enumarate를 사용하여 나란히 루프를 수행\n1 2 3 4 5 for i, name in enumerate(names): count = counts[i] if count \u0026gt; max_count: longest_name = name max_count = count zip을 사용하여 나란히 루프를 수행\n1 2 3 4 for name, count in zip(names, counts): if count \u0026gt; max_count: longest_name = name max_count = count 두 리스트를 동시에 이터레이션 할 경우 내장함수 zip을 사용하면 훨씬 더 깔끔하다.\n내장함수 zip zip은 둘 이상의 이터레이터를 제너레이터를 사용해 묶어준다. zip 제너레이터는 각 이터레이터의 다음 값이 들어있는 튜플을 반환한다. zip은 자신이 감싼 이터레이터 원소를 하나씩 소비하기 때문에 메모리를 다 소모해서 프로그램이 중단되는 위험 없이 아주 긴 입력도 처리할 수 있다. 여러 이터레이터의 길이가 같지 않을 경우 길이가 같지 않은 리스트를 zip을 사용하여 나란히 루프를 수행\n1 2 3 4 5 6 7 8 9 10 name = [\u0026#39;Cecilia\u0026#39;, \u0026#39;남궁민수\u0026#39;, \u0026#39;김민철\u0026#39;, \u0026#39;Rosalind\u0026#39;] counts = [7, 4, 3] for name, count in zip(names, counts): print(name) \u0026gt;\u0026gt;\u0026gt; Cecilia 남궁민수 김민철 zip은 자신이 감싼 이터레이터 중 어느 하나가 끝날때까지 튜플을 내놓는다. 따라서 출력은 가장 짧은 입력의 길이와 같다. 리스트들의 길이가 같지 않을 것으로 예상한다면 itertools 내장모듈에 들어있는 zip_longest 를 사용하자.\n길이가 같지 않은 리스트를 zip_longest를 사용하여 나란히 루프를 수행\n1 2 3 4 5 6 7 8 9 10 import itertools for name, count in itertools.zip_longest(names, counts, fillvalue=-100): print(f\u0026#39;{name}: {count}\u0026#39;) \u0026gt;\u0026gt;\u0026gt; Cecilia: 7 남궁민수: 4 김민철: 3 Rosalind: -100 zip_longest를 사용했을 때 존재하지 않은 값은 fillvalue 를 통해 값을 설정해 줄 수 있다. 디폴트 값은 None이다.\nBetter way 9 for나 while 루프 뒤에 else 블록을 사용하지 말라 파이썬에서는 루프가 반복 수행하는 내부 블록 바로 다음에 else 블록을 추가할 수 있다. 이 else 블록은 루프가 break를 통해 빠져나가지 않고 끝까지 루프를 했다면 실행이 된다.\n서로소 구하기: 루프 뒤에 else를 사용\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 a = 4 b = 9 for i range(2, min(a, b) + 1): print(\u0026#39;검사중\u0026#39;, i) if a % i == 0 and b % i == 0: print(\u0026#39;서로소 아님\u0026#39;) break else: print(\u0026#39;서로소\u0026#39;) \u0026gt;\u0026gt;\u0026gt; 검사 중 2 검사 중 3 검사 중 4 서로소 if/else문에서 else는 이 블록 앞의 블록이 실행되지 않으면 이 블록을 싱행하라 인데 루프 뒤애 오는 else는 루프가 정상적으로 완료되었다면 실행되기 때문에 마치 and와 같은 역할을 하고 있다. 이름과 역할이 달라 직관적이지 못하다.\n루프가 한 번도 돌지 않아도 실행되는 else\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 for x in []: print(\u0026#39;이 줄은 실행되지 않음\u0026#39;) else: print(\u0026#39;For Else block!\u0026#39;) \u0026gt;\u0026gt;\u0026gt; For Else block! while False: print(\u0026#39;이 줄은 실행되지 않음\u0026#39;) else: print(\u0026#39;While Else block!\u0026#39;) \u0026gt;\u0026gt;\u0026gt; While Else block! 또한 빈 시퀸스나 while 루프 조건이 처음부터 False인 경우에도 else 블록이 실행된다. 빈 시퀸스이거나 while 조건이 처음부터 False이면 루프가 단 한 번도 실행되지 못하는데 말이다.\n서로소 구하기 리팩토링 루프 뒤 else를 사용해서 만든 서로소 구하기는 직관적이지 못해 더 직관적이게 도우미 함수를 작성하여 서로소를 구해본다.\n서로소 구하기: 원하는 조건을 찾자마자 함수를 반환\n1 2 3 4 5 6 7 8 def coprime(a, b): for i in range(2, min(a, b) + 1): if a % i == 0 and b % i == 0: return False return True assert coprime(4, 9) assert not coprime(3, 6) 서로소 구하기: 원하는 대상을 찾았는지 나타내는 결과 변수를 도입\n1 2 3 4 5 6 7 8 def coprime_alternate(a, b): is_copirme = True for i in range(2, min(a, b) + 1): if a % i == 0 and b % i == 0: is_coprime = False break return is_coprime 절대로 루프 뒤에 else 블록을 사용하지 말아야 하는 이유 else 블록을 사용함으로써 얻을 수 있는 표현력보다는 미래에 이 코드를 이해하려는 사람들이 느끼게 될 부담감이 더 크다. 파이썬에서 루프와 같은 간단한 구성 요소는 그 자체로 의미가 명확해야 한다. Better way 10 대입식을 사용해 반복을 피하라 대입식은 파이썬 언어에서 고질적인 코드 중복 문제를 해결하기 위해 파이썬 3.8에서 새롭게 도입된 구문으로 왈러스 연산자 (walrus) 라고도 부른다. 연산자 기호 := 이 마치 바다코끼리 (walrus)의 눈과 엄니를 닮아서 이런 별명이 생겼다.\n[Picture 1] 바다코끼리 왈러스 연산자의 장점은 값을 대입할 수 없어 대입문이 쓰일 수 없는 if 조건문 과 같은 위치에 사용할 수 있다는 것이다. 즉, 원래라면 값을 대입할 수 없는 자리에 왈러스 연산자를 씀으로 값을 대입할 수 있다.\n왈러스 연산자가 하는 일\nStep 1: 대입 Step 2: 대입된 값을 평가 월러스 연산자\n1 2 3 4 5 # 대입문 a = b # 왈러스 연산자 a := b 왈러스 연산자 용법 왈러스 연산자를 쓰지 않았을 때\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 fresh_fruit = { \u0026#39;사과\u0026#39;: 10, \u0026#39;바나나\u0026#39;: 8, \u0026#39;레몬\u0026#39;: 5, } def make_lemonade(count): ... def out_of_stock(): ... count = fresh_fruit.get(\u0026#39;레몬\u0026#39;, 0) if count: make_lemonade(count) else: out_of_stock() count 변수는 if 문의 첫 번째 블록 안에서만 쓰인다. 하지만 if 앞에서 count를 정의하면서 마치 else 블록이나 그 이후의 코드에서 count 변수에 접근을 할 필요가 있어 보인다. 따라서 실제보다 count 변수가 중요해보인다.\n파이썬에서는 이런 식으로 값을 가져와서 그 값이 0이 아닌지 검사한 후 사용하는 패턴이 많다. 왈러스 연산자를 사용해서 가독성을 높이도록 하자.\n왈러스 연산자를 썼을 때\n1 2 3 4 if count := fresh_fruit.get(\u0026#39;레몬\u0026#39;, 0): make_lemonade(count) else: out_of_stock() count가 if문의 첫 번째 블록에서만 의미가 있다는 점이 명확하게 보여 코드가 더 읽기 쉬워졌다. 덩달아 코드도 조금 더 짧아졌다.\n왈러스 연산자를 이용해 더 중요한 변수에 힘 주기 이 패턴은 더 중요한 변수에 힘을 주기에도 좋지만, if condition에서 값을 비교하고, 해당 스코프 안에서 그 값을 함수 호출할 때 사용했다는 것을 기억하자.\n왈러스 연산자를 쓰지 않았을 때\n1 2 3 4 5 6 pieces = 0 count = fresh_fruit.get(\u0026#39;바나나\u0026#39;, 0) if count \u0026gt;= 2: pieces = slice_bananas(count) else: ... pieces 가 훨씬 중요한 변수인데 if 앞에서 count 를 정의해서 마치 count 도 중요해보인다.\n왈러스 연산자를 썼을 때\n1 2 3 4 5 pieces = 0 if (count := fresh_fruit.get(\u0026#39;바나나\u0026#39;, 0)) \u0026gt;= 2: pieces = slice_bananas(count) else: ... 왈러스 연산자를 이용해 우아하게 switch/case 흉내내기 파이썬에는 switch/case 문이 없기 때문에 일반적으로 if, elif, else 문을 사용해서 흉내를 내는 것이 일반적이다. 왈러스 연산자를 쓰지 않는다면 대입하고 평가하는 것이 나눠져있기 때문에 지져분해진다.\n왈러스 연산자를 쓰지 않았을 때\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 count = fresh_fruit.get(\u0026#39;바나나\u0026#39;, 0) if count \u0026gt;= 2: pieces = slice_bananas(count) to_enjoy = make_smoothies(pieces) else: # 왜 elif를 안쓰고 else를 썼나 했더니 elif를 쓰면 값을 대입 못한다.. . count = fresh_fruit.get(\u0026#39;사과\u0026#39;, 0) if count \u0026gt;= 4: to_enjoy = make_cider(count) else: count = fresh_fruit.get(\u0026#39;레몬\u0026#39;, 0) if count: to_enjoy = make_lemonade(count) else: to_enjoy = \u0026#39;아무것도 없음\u0026#39; 왈러스 연산자를 썼을 때\n1 2 3 4 5 6 7 8 9 if (count := fresh_fruit.get(\u0026#39;바나나\u0026#39;, 0)) \u0026gt;= 2: pieces = slice_bananas(count) to_enjoy = make_smoothies(pieces) elif (count := fresh_fruit.get(\u0026#39;사과\u0026#39;, 0)) \u0026gt;= 4: to_enjoy = make_cider(count) elif count := fresh_fruit.get(\u0026#39;레몬\u0026#39;, 0): to_enjoy = make_lemonade(count) else: to_enjoy = \u0026#39;아무것도 없음\u0026#39; 왈러스 연산자를 이용해 우아하게 while문 사용하기 왈러스 연산자를 쓰지 않았을 때\n1 2 3 4 5 6 7 8 bottles = [] fresh_fruit = pick_fruit() while fresh_fruit: for fruit, count in fresh_fruit.items(): batch = make_juice(fruit, count) bottles.extend(batch) fresh_fruit = pick_fruit() 이 코드는 fresh_fruit = pick_fruit() 호출을 두 번 하고 있기 때문에 반복적이다.\nwhile을 무한루프로 만들고 중간에 break를 두는 식으로 fresh_fruit = pick_fruit() 를 한 번 만 호출 할 수 도 있겠지만 while 루프를 맹목적인 무한루프로 만들고, 루프 흐름 제어가 모두 break 문에 달려있어 while 루프의 유용성이 줄어든다.\n왈러스 연산자를 썼을 때\n1 2 3 4 5 bottles = [] while fresh_fruit := pick_fruit(): for fruit, count in fresh_fruit.items(): batch = make_juice(fruit, count) bottles.extend(batch) 왈러스 연산자 사용을 고려해야 할 때 몇 줄로 이뤄진 코드 그룹에서 같은 식이나 같은 대입문을 여러 번 되풀이하는 부분을 발견하면 가독성을 향상시키기 위해 대입식을 도입하는 것을 고려해 봐야 한다.\n","date":"2022-05-06","permalink":"https://leeleelee3264.github.io/post/2022-05-06-effective-python-betteryway-1-to5/","tags":["Book"],"title":"[Book] [Effective Python] 노트 정리 (1/10) - Think like Python"},{"content":"\nBill Lubanovic의 저서 [Introducing Python]를 요약한다. 이 포스트에서는 이와 같은 것들을 다룬다.: list, tuple, dictionary and set. Also covers code structure and module, package.\nIndex\n파이 맛보기 파이 재료: 숫자, 문자열 변수 파이 채우기: 리스트, 튜플, 딕셔너리, 셋 파이 크러스트: 코드 구조 파이 포장하기: 모듈, 패키지, 프로그램 파이 맛보기 파이썬은 인터프리터 언어다. 파이썬이 개발을 빠르게 할 수 있다는 이유도 거대한 크기의 기계어를 만들어내지 않고 실행이 가능하기 때문으로 추정된다.\n인터프리터 언어 대표적인 인터프리터 언어는 파이썬, 자바 스크립트가 있다. 인터프리터는 소스코드를 바로 실행한다. 인터프리터는 바로바로 실행을 하다보니 처음에 속도가 비교적 빠르다. 컴파일 언어 대표적인 컴파일 언어는 자바다. 컴파일 언어는 처음 프로그램을 시작할 때 모든 코드를 기계어로 바꾼다 (컴파일 한다). 컴파일을 하기 때문에 처음에 프로그램을 실행할 때 시간이 오래 걸린다. 하지만 한 번 컴파일을 하면 실행시 기계어를 불러와 더 빨리 실행을 할 수 있어 매번 실행시마다 번역을 거치는 인터프리터 언어보다 속도가 더 빨라진다. 파이 재료: 숫자, 문자열 변수 파이썬은 문자를 변형 할 수 없다. st[0] = \u0026rsquo;e\u0026rsquo; 이런식으로 변경을 할 수 없다. 즉, 불변객체이다.\n문자열도 중간에 수정을 할 수 없고 아예 객체를 새로 만드는 replace() 메서드를 이용해서 문자열을 변경해야 한다. 문자열을 불변으로 만드는 이유는 Java와 마찬가지로 프로그래밍을 편리하게 하기 위해서다.\n슬라이싱을 이용한 문자 reverse 예시\n1 2 st = \u0026#34;eererewrvfgrhfos\u0026#34; re_st = st[::-1] # sofhrgfvrwereree 파이 채우기: 리스트, 튜플, 딕셔너리, 셋 파이썬에서 기본으로 제공하는 자료구조에 대해 알아본다.\ndel 자료구조에서 항목을 삭제하는 커맨드는 del 인데, 이 del은 자료구조의 함수가 아닌 파이썬 구문이다. del은 객체로부터 이름을 분리하고 객체의 메모리를 비워준다.\ndel 예시\n1 2 3 del full[2] # full.del(2) 처럼 쓰지 못한다. 리스트 리스트는 변경 가능하다. 항목을 할당하고, 자유롭게 수정 삭제를 할 수 있다.\n리스트 예시\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 # 생성 empty = [] empyt2 = list() full = [1, 2, 3] # 리스트의 끝에 항목 추가하기 append() full.appned(4) # 리스트 병합하기 extend() added = [5, 6, 7, 8] full.extend(added) # [1, 2, 3, 4, 5, 6, 7, 8] full += added # [1, 2, 3, 4, 5, 6, 7, 8] # append 를 쓰면 리스트 자체가 추가된다 full.appned(added) # [1, 2, 3, 4, [5, 6, 7, 8]] # 리스트 정렬 # sort()는 리스트 자체를 내부적으로 정렬한다. # sorted() 는 리스트의 정렬된 복사본을 반환한다. full.sort() new_sort = full.sorted() 튜플 튜플은 불변한다. 튜플에 항목을 할당하고 나서는 바꿀 수 없다. 때문에 튜플을 상수 리스트라 볼 수 있다.\n튜플 예시\n1 2 3 4 5 6 7 8 9 10 # 생성 empty_tuple = () # 콤마로 값을 나열해도 튜플을 만들 수 있다. empty_tuple = 1, 2, 3 empty_tuple = (1, 2, 3) # 튜플의 나열하는 특성을 이용해서 객체 생성없이 swap하기 password = \u0026#39;12\u0026#39; icecream = \u0026#39;sweet\u0026#39; password, icecream = icecream, password 리스트가 아닌 불변객체라 함수 지원이 더 적은 튜플을 사용하는 이유 더 적은 공간을 사용한다. 실수로 값을 바꿀 위험이 없다. 튜플을 딕셔너리 키로 사용이 가능하다. 네임드 튜플 은 객체의 대안이 될 수 있다. 함수의 인자들은 튜플로 전달된다. 딕셔너리 딕셔너리 예시\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 # 생성 empty_dict = {} # dict() 를 이용해 두 값 쌍으로 이뤄진 자료구조를 딕셔너리로 변환할 수 있다. lol = [[\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;], [\u0026#39;c\u0026#39;, \u0026#39;d\u0026#39;]] lol2 = lol #{\u0026#39;a\u0026#39;: \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;: \u0026#39;d\u0026#39;} lol3 = (\u0026#39;ab\u0026#39;, \u0026#39;cb\u0026#39;) lol4 = lol #{\u0026#39;a\u0026#39;: \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;: \u0026#39;d\u0026#39;} # 딕셔너리 결합하기 update() em = {\u0026#39;a\u0026#39;: \u0026#39;b\u0026#39;} em2 = {\u0026#39;c\u0026#39;: \u0026#39;d\u0026#39;} em.update(em2) # {\u0026#39;a\u0026#39;: \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;: \u0026#39;d\u0026#39;} # 딕셔너리 비우기 em.clear() # 딕셔너리에 특정 키가 들어있나 확인 \u0026#39;c\u0026#39; in em # 모든 키 가져오기 em.keys() # 모든 값 가져오기 em.values() # 모든 키, 값 가져오기 # (\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;), (\u0026#39;c\u0026#39;, \u0026#39;d\u0026#39;) 처럼 튜플로 반환한다 em.items() 셋 어떤 것이 존재하는지 여부만 판단하기 위해서 셋을 사용한다. 중복을 허용하지 않는다. 셋은 수학시간에 배웠던 집합과 아주 유사하다.\n셋 예시\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 # 생성 # 그냥 {} 는 딕셔너리 생성자에 선점되었다. empty_set = set() empty_set2 = {1, 2, 3, 4} # 각종 집합 # 교집합 \u0026amp;, intersection() if contents \u0026amp; {\u0026#39;ice\u0026#39;, \u0026#39;cream\u0026#39;} # ice 와 cream 모두 들어있어야 참 a = {1, 2} b = {3, 4} a \u0026amp; b = 2 # 합집합 |, union() a | b = {1, 2, 3} # 차집합 -, difference() a - b = {1} # 대칭 차집합 ^, symmetric_difference # 각 항목에 별개로 가지고 있는 값을 구한다. a ^ b = {1, 3} # 부분집합 \u0026lt;=, issubset() a.issubset(b) # False a.issubset(a) # True a.issubset((1, 2, 3)) # 슈퍼셋 \u0026gt;=, issuperset() a.issuperset((1)) # True ((1, 2, 3)).issuperset(a) # True a.issuperset(a) # True 파이 크러스트: 코드 구조 컴프리헨션 내가 가끔 검색해보고는 하는 한 줄 로 for 문 돌리기와 유사하다.\n하나 이상의 이터레이터로부터 파이썬 자료구조를 만드는 방법이다. 더 파이써닉한 용법이라니는데 간단한 할당문 말고는 컴프리헨션을 사용하면 더 헷갈릴 것 같다.\n한 줄 for 문 예시\n1 num = [i for i in range(1, 6)] 인자 다른 언어들과 마찬가지로, 값을 순서대로 상응하는 매개변수에 복사하는 것이 위치인자이다. 키워드인자는 위치인자의 혼동을 피하기 위해 상응하는 이름을 인자 안에 지정한 것이다.\n인자 예시\n1 2 3 4 5 6 7 8 9 10 11 # 위치 인자 def menu(wine, entree, dessert): pass # 키워드 인자 def menu(wine=wine, entree=entreee, dessert=dessert): pass # 인자의 기본 값 지정 def menu(wine, entree, dessert=\u0026#39;pie\u0026#39;): pass 인자 모으기 예시\n1 2 3 4 5 6 7 8 9 10 11 12 # 위치 인자 모으기 * def print_args(one, two, three, *args): pass # 실제로 호출 시 three까지 위치에 따라 값이 들어가고 나머지는 *args가 인자를 취하게 해준다. print_args(1, 2, 3, 4, 5, 6, 7, 8) # 키워드 인자 모으기 ** def print_keyword(**kwargs) # 실제 호출 시 위치인자와 마찬가지로, 함수에 따로 정의가 안 된 위치인자를 취한다. print_keyword(one=1, two=2, three=3, four=4, five=5) 여러가지 종류의 인자들을 섞어서 사용하려면 함수를 정의할 때 위치인자, 키워드 인자, *args, **kwargs 순으로 정의를 해줘야한다.\ndocstring 파이썬 문서화에 관련된 부분. 일반 주석은 #을 사용하지만, 모듈과 클래스와 메소드에 사용하는 주석의 형태는 따로 있다. 이것을 doctstring 이라고 한다.\ndocstring 예시\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 \u0026#34;\u0026#34;\u0026#34; 모듈 (파이썬 파일) 최상단에 이런 형식으로 주석을 달아주세요. Usage: python my_test.py \u0026lt;param\u0026gt; \u0026#34;\u0026#34;\u0026#34; class TestClass: \u0026#34;\u0026#34;\u0026#34; 클래스 아래에 이런 형식으로 주석을 달아주세요. \u0026#34;\u0026#34;\u0026#34; def test_method(): \u0026#34;\u0026#34;\u0026#34; 함수 아래에 이런 형식으로 주석을 달아주세요.\t\u0026#34;\u0026#34;\u0026#34; pass docstring을 이용해서 주석을 달아두면 코드에서 help 함수를 사용해 접근을 할 수 있다.\n내가 지금 사용하는 클래스가 뭘 하는 애인지 해당 클래스 파일을 읽지 않아도 콘솔에 입력만 하면 볼 수 있다는 장점이 있다.\nhelp 함수로 docstring 접근 예시\n1 2 help(TestClass) TestClass.__doc__ 라인 유지하기 PEP 에 따르면 파이썬은 한 줄에 80글자를 넘으면 안된다. 가독성이 제일 중요한 언어에서 가독성이 떨어지기 때문이다. 그래서 긴 문장을 사용해야 할 때에는 백슬래시(\\)로 라인을 끊어준다.\n백슬래시 예시\n1 2 3 4 5 6 7 8 test = \u0026#34;this\u0026#34; + \\ \u0026#34;is very very\u0026#34; + \\ \u0026#34;long long line\u0026#34; # 추천하지 않는 라인 끊는 방법 test = \u0026#34;\u0026#34; test += \u0026#34;is very very\u0026#34; test += \u0026#34;long long line\u0026#34; 일등 시민 : 함수 함수는 뷸변하기 때문에 딕셔너리의 키로 사용할 수 있다.\n함수를 변수에 할당할 수 있고, 다른 함수에서 이를 인자로 쓸 수 있으며, 함수에서 함수를 반환할 수 있다. 파이썬에서 괄호 ()는 함수를 호출 한다는 의미로 사용되고, 괄호가 없으면 함수를 객체 처럼 간주한다.\n함수에서 함수를 반환하는 예시\n1 2 3 4 5 6 7 8 def run_something_with_args(func, arg1, arg2): func(arg1, arg2) def add_args(arg1, arg2): return arg1 + agr2 \u0026gt;\u0026gt; run_something(add_agrs, 5, 8 ) 14 예제에서 run_something_with_args로 전달된 add_args 는 괄호 없이 객체처럼 취급되어 func 매개변수로 할당된다. 뒤에 괄호 () 가 붙은 func 는 전달 받은 arg1, arg2를 매개변수로 해 함수를 호출한다. 내부 함수 먼저 읽으면 좋을 자료 [Real Python: adding behavior with inner functions decorators] 함수 안에 또 다른 함수를 정의한다. 함수를 global scope 으로부터 완전히 숨겨 encapsulation을 하거나, 복잡한 작업을 하기 위해 Helper 함수를 만들어야 할 때 내부함수를 쓴다.\n내부 함수 예시\n1 2 3 4 5 6 7 def increment(number): def inner_increment(): return number + 1 return inner_increment() \u0026gt;\u0026gt; increment(10) 11 위처럼 작성을 하면 inner_increment 함수를 어디에서도 호출을 할 수 없다.\n내부 함수를 이용해 Helper를 만든 예시\n1 2 3 4 5 6 7 8 9 10 11 12 def factorial(number): if not isinstance(number, int): raise TypeError(\u0026#34;Sorry. \u0026#39;number\u0026#39; must be an integer.\u0026#34;) if number \u0026lt; 0: raise ValueError(\u0026#34;Sorry. \u0026#39;number\u0026#39; must be zero or positive.\u0026#34;) def inner_factorial(number): if number \u0026lt;= 1: return 1 return number * inner_factorial(number - 1) return inner_factorial(number) 그런데 내부함수로 Helper 함수를 만들기보다는 private 으로 Helper 함수를 만드는 것을 권장한다. private helper 함수가 훨씬 더 코드 읽기가 편하고 같은 모듈이나 클래스에서만 재사용이 가능하기 때문이다.\nprivate을 이용해 Helper를 만든 예시\n1 2 3 4 5 6 7 8 9 10 11 12 def factorial(number): if not isinstance(number, int): raise TypeError(\u0026#34;Sorry. \u0026#39;number\u0026#39; must be an integer.\u0026#34;) if number \u0026lt; 0: raise ValueError(\u0026#34;Sorry. \u0026#39;number\u0026#39; must be zero or positive.\u0026#34;) return _factorial(number) def _factorial(number): if number \u0026lt;= 1: return 1 return number * inner_factorial(number - 1) 클로저 클로저는 바깥 함수로부터 전달된 변수값을 저장하고, 변경을 할 수 있는 함수이다. 파이썬에서 함수를 변수에 할당할 수 있는 이유도 클로저 기능을 지원하기 때문이다.\n클로저 예시 generate_power\n1 2 3 4 5 6 7 8 9 10 11 12 # closure factory function def generate_power(exponent): def power(base): return base ** exponent return power raise_two = generate_power(2) \u0026gt;\u0026gt; raise_two(4) 16 \u0026gt;\u0026gt; raise_two(5) 25 클로저 호출 과정 상세 보기 클로저의 개념이 처음이다보니 제대로 이해가 가지 않아 print로 디버깅을 해가며 이해를 진행했다.\n클로저 예시 generate_power_with_debug\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 def generate_power_with_debug(exponent): print(f\u0026#39;closure generated, passed exponent {exponent}\u0026#39;) def power(base): print(f\u0026#39;inner function in closure. passed base {base}\u0026#39;) return base ** exponent return power # closure 생성 raise_two = generate_power_with_debug(2) # closure 호출 print(f\u0026#39;result of closure : {raise_two(4)}\u0026#39;) # 콘솔에 출력된 결과 closure generated, passed exponent 2 inner function in closure. passed base 4 result of closure : 16 generate_power 호출 과정 (1)\n1 raise_two = generate_power_with_debug(2)) generate_power_with_debug 로 변수 exponent에 2를 넣어 클로저 생성한다. 클로저는 매번 호출될 때마다 새로운 클로저를 생성한다. 내부 함수 power은 호출이 되지 않고, 새로운 power 인스턴스를 생성해 리턴이 된다. 리턴값이 함수라는 얘기다. power를 리턴할 때 power의 surrounding state 를 스냅셧으로 남긴다. 여기에는 exponent 변수가 포함되어있다. generate_power 호출 과정 (2)\n1 print(f\u0026#39;result of closure : {raise_twon(4)}\u0026#39;) generate_power_with_debug 클로저를 호출한다. 클로저를 호출함에 따라 변수 base에 4를 넣어 내부함수 power가 호출한다. power는 클로저가 리턴되었을 때 함께 넘어왔던 surrounding state의 스냅샷에 저장이 된 exponent를 이용한다. power 결과를 리턴한다. 클로저 호출 시나리오 총정리\nQ: 어떻게 내부함수를 호출할 때 외부함수의 값에 접근을 할까? A: 클로저를 생성할 때 내부함수를 리턴하는데, 이때 외부함수의 상태 스냅샷을 함께 리턴해주기 때문이다. 클로저를 구분할 수 있는 부분은 내부함수를 괄호() 로 호출하지 않다는 것이다. 예제에서 power를 리턴하기만 하는데, 이렇게 리턴을 하면 exponent 값을 저장한 power 함수의 복사본을 주게 된다. 복사본을 할당 받은 변수 raise_two를 실제로 매개변수를 넣고 호출한다. 매개변수는 내부함수인 power 의 base 와 맵핑이 된다. 클로저로 권한 확인 함수 구현 클로저로 구현한 권한 확인 함수\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 def has_permission(page): def permission(username): if username.lower() == \u0026#34;admin\u0026#34;: return f\u0026#34;\u0026#39;{username}\u0026#39; has access to {page}.\u0026#34; else: return f\u0026#34;\u0026#39;{username}\u0026#39; doesn\u0026#39;t have access to {page}.\u0026#34; return permission # 선언 check_admin_page_permision = has_permission(\u0026#34;Admin Page\u0026#34;) \u0026gt;\u0026gt;\u0026gt; check_admin_page_permision(\u0026#34;admin\u0026#34;) \u0026#34;\u0026#39;admin\u0026#39; has access to Admin Page.\u0026#34; \u0026gt;\u0026gt;\u0026gt; check_admin_page_permision(\u0026#34;john\u0026#34;) \u0026#34;\u0026#39;john\u0026#39; doesn\u0026#39;t have access to Admin Page.\u0026#34; 데코레이터 데코레이터는 callable(함수, 메소드, 클래스)를 인자로 받고, 다른 callable을 리턴한다(내부함수).\n생김새와 위치는 자바의 어노테이션과 동일하다. 데코레이션을 사용하면 이미 존재하고 있던 함수에 별도의 수정사항없이 액션을 추가 할 수 있다.\n데코레이터 사용 시나리오\n함수를 인자로 받는 callable을 선언한다. 인자로 받은 함수를 호출한다. 추가 액션이 있는 다른 함수를 리턴한다. 데코레이터 예시\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def example_decorator(func): def _add_messages(): print(\u0026#39;This is my first decorator\u0026#39;) func() print(\u0026#39;bye\u0026#39;) # 데코레이터도 클로저처럼 내부함수를 괄호()로 호출하지 않는다. return _add_messages # greet = example_decorator(greet) 과 동일하다. 클로저 생성 형태와 동일하다. @example_decorator def greet(): print(\u0026#39;Hello World\u0026#39;) \u0026gt;\u0026gt;\u0026gt; greet() This is my first decorator Hello World bye 이렇게 추가로 액션을 행할 수 있게 해주는 데코레이터는 디버깅, 캐싱, 로깅, 시간측정(timing)에 많이 쓰인다.\n데코레이터 디버깅 예시\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 def debug(func): def _debug(*args, **kwargs): result = func(*args, **kwargs) print( f\u0026#34;{func.__name__}(args: {args}, kwargs: {kwargs}) -\u0026gt; {result}\u0026#34; ) return result return _debug @debug def add(a, b): return a + b \u0026gt;\u0026gt;\u0026gt; add(5, 6) add(args: (5, 6), kwargs: {}) -\u0026gt; 11 11 데코레이터로 구현한 generate_power\n아까 위에서는 클로저로 generate_power를 구현했는데 이번에는 데코레이터로 구현을 했다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 def generate_power(exponent): def power(func): def inner_power(*args): base = func(*args) return base ** exponent return inner_power return power @generate_power(2) def raise_two(n): return n \u0026gt;\u0026gt;\u0026gt; raise_two(7) 49 @generate_power(3) def raise_three(n): return n \u0026gt;\u0026gt;\u0026gt; raise_three(5) 125 데코레이터 호출 과정 상세 보기 데코레이터로 구현한 generate_power_with_debug\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 def generate_power_with_debug(exponent): print(f\u0026#39;closure is generated, passed exponent : {exponent}\u0026#39;) def power(func): print(f\u0026#39;inner function in closure. passed func : {func}\u0026#39;) def inner_power(*args): print(f\u0026#39;inner function in power. passed args : {args}\u0026#39;) base = func(*args) return base ** exponent return inner_power return power # closure 생성 # raise_two = generate_power_with_debug(2) 와 동일 하다. @generate_power_with_debug(2) # power()를 리턴 def raise_two(n): # power()를 호출, inner_power()를 리턴 return n print(f\u0026#39;result of closure : {raise_two(7)}\u0026#39;) # 콘솔에 출력된 결과 closure is generated, passed exponent : 2 inner function in closure. passed func : \u0026lt;function raise_two at 0x100e2dee0\u0026gt; inner function in power. passed args : (7,) result of closure : 49 generate_power 호출 과정 (1)\n1 2 3 @generate_power_with_debug(2) def raise_two(n): return n raise_two = generate_power_with_debug(2) 와 동일하다. @generate_power_with_debug 데코레이터는 exponent 값을 포함한 내부함수 power를 리턴한다. raise_two가 선언되면서 power도 호출이 된다. power는 func를 포함한 내부험수 inner_function을 리턴한다. 여기에서도 inner_function은 호출되지 않고, 새로운 인스턴스를 생성해 리턴이 된다. generate_power 호출 과정 (2)\n1 print(f\u0026#39;result of closure : {raise_two(7)}\u0026#39;) raise_two를 호출하면서 클로저를 호출한다. 클로저 호출함에 따라 변수 *args에는 raise_two 함수에 전달된 인자 7이 전달된다. 이또한 스냅샷으로 외부 state를 저장했기 때문이다. 2-1. *args는 함수에 전달되는 모든 인자들을 뜻하고, **kwargs는 위치 지정된 모든 인자들을 뜻한다. inner_power 결과를 리턴한다. 클로저 VS 데코레이터 데코레이터는 클로저를 반환한다. 클로저는 데코레이터에 의해 반환된다. 이름에 _와 __사용 먼저 읽으면 좋을 자료 What’s the Meaning of Single and Double Underscores In Python? _ 와 __ 사용 예시\n1 2 3 4 5 6 7 _foo # single leading underscore foo_ # single trailing underscore _ # single underscore __foo__ # double leading and trailing underscore __foo # double leading underscore 상세 사용 예시 name e.g. usage single leading underscore _foo - private(internally) 하게 사용이 됨을 나타낸다. - 여전히 외부에서 접근이 가능하기 때문에 문맥적 힌트에 가깝다. single trailing underscore foo_ - 파이썬에서 이미 선점한 키워드를 사용할 때 혼선을 피하기 위한 방법이다.\ne.g. type_, from_ single underscore _ - 사용하지 않은 변수들을 담아두는 용도로 쓴다. e.g. _ = return_something(), - 숫자가 길어질 때 혼선을 방지하기 위해 쓴다. e.g. 1000 → 1_000 double leading and trailing underscore __foo__ - dunder method 라고 한다. - 파이썬에서 이미 선점한 특수 목적 전역 클레스 메소드다. double leading underscore __foo - 부모-자식 필드 이름을 구분하기 위해 사용되는 것으로 파악했다. - 실 사용이 거의 없을 거 같다. 파이 포장하기: 모듈, 패키지, 프로그램 모듈 파이썬을 사용하다보면 모듈이라는 단어가 자주 나오는데 여기서 모듈이란 단순히 파이썬 코드가 들어가있는 파일을 뜻한다.\n패키지 파이썬을 좀 더 확장 가능한 어플리케이션으로 만들기 위해서는 모듈을 패키지라는 파일 계층구조로 구성해야 한다. __init__.py 는 파일 내용을 비워놔도 되지만, 파이썬은 이 파일을 포함하는 디렉터리를 패키지로 간주 하기 때문에 패키지로 사용하고 싶다면 꼭 만들어둬야 한다.\n파이썬에서 batteries included 철학은 유용한 작업을 처리하는 많은 표준 라이브러리 모듈들이 내장이 되어있다는 뜻이다.\nDeque = Stack + Queue 파이썬 list는 left end의 pop()과 append()가 빠르지가 않기 때문에 left-end와 right-end 모두 빠르고 메모리를 효과적으로 사용하기 위해 데크를 제공한다.\nlist의 right-end 연산 속도는 O(1)이지만, left-end 연산 속도는 O(n)이다.\nDeque 구현체 Deque 는 Stack과 Queue의 기능을 가졌다. 출입구가 양 끝에 있는 Queue다.(double-ended queue의 구현체이다) Deque는 양 끝으로부터 항목을 추가하거나 삭제할 때 유용하게 쓰인다. popleft()는 left-end를 제거해서 반환하고, pop()은 right-end를 제거해서 반환한다. Deque 예시\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from collections import deque numbers = deque([1,2,3,4]) \u0026gt;\u0026gt;\u0026gt; numbers.popleft() 1 \u0026gt;\u0026gt;\u0026gt; numbers.popleft() 2 \u0026gt;\u0026gt;\u0026gt; numbers deque([3,4]) \u0026gt;\u0026gt;\u0026gt; numbers.appendleft(2) \u0026gt;\u0026gt;\u0026gt; numbers.appendleft(1) \u0026gt;\u0026gt;\u0026gt; numbers deque([1,2,3,4]) Deque의 흥미로운 점 최대 길이 (maximum lenght)를 지정할 수 있다. 한 쪽에서 데이터를 넣어 큐가 꽉 차게 되면 자동으로 다른 쪽에 있는 아이템을 버린다. 이러한 기능으로 인해 이전 0회의 기록을 남기기 와 같은 요구사항이 있을 때 활용하기가 용이하다. [더 많은 Deque 사용법] 에서 더 많은 용도를 확인할 수 있다. 히스토리 남기기 예시\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 from collections import deque sites = ( \u0026#34;google.com\u0026#34;, \u0026#34;yahoo.com\u0026#34;, \u0026#34;bing.com\u0026#34; ) pages = deque(maxlen=3) pages.maxlen for site in sites: pages.appendleft(site) \u0026gt;\u0026gt;\u0026gt; pages deque([\u0026#39;bing.com\u0026#39;, \u0026#39;yahoo.com\u0026#39;, \u0026#39;google.com\u0026#39;], maxlen=3) pages.appendleft(\u0026#34;facebook.com\u0026#34;) \u0026gt;\u0026gt;\u0026gt; pages deque([\u0026#39;facebook.com\u0026#39;, \u0026#39;bing.com\u0026#39;, \u0026#39;yahoo.com\u0026#39;], maxlen=3) pages.appendleft(\u0026#34;twitter.com\u0026#34;) \u0026gt;\u0026gt;\u0026gt; pages deque([\u0026#39;twitter.com\u0026#39;, \u0026#39;facebook.com\u0026#39;, \u0026#39;bing.com\u0026#39;], maxlen=3) Linux의 tail 모방 예시\n1 2 3 4 5 6 7 8 from collections import deque def tail(filename, lines=10): try: with open(filename) as file: return deque(file, lines) except OSError as error: print(f\u0026#39;Opening file \u0026#34;{filename}\u0026#34; failed with error: {error}\u0026#39;) thread-safe CPython에서 deque의 append(), appendleft(), pop(), popleft(), len()은 thread-safe 하게 만들어졌기 때문에 멀티쓰레드 환경에서 deque를 사용하기 좋다. CPyton은 C로 구현한 파이썬으로, 가장 많이 사용되고 있는 파이썬 구현체다. 오픈소스로 관리가 되고 있다. [깃허브] ","date":"2022-03-08","permalink":"https://leeleelee3264.github.io/post/2022-03-08-introducing-python-part-one/","tags":["Book"],"title":"[Book] [Introducing Python] 노트 정리 (1/2)"},{"content":"\n이 포스트에서는 SSH key, ssh config 와 ssh-keygen를 사용하여 여러 개의 Github 계정을 사용하는 방법을 알아본다.\nIndex\n깃허브 계정 여러 개 세팅하기 매번 해줘야 하는 작업들 레퍼런스 깃허브 게정 여러 개 세팅하기 디렉터리 세팅하기 복수의 깃허브 계정을 사용 할 때, 각 계정들의 root source directory를 나누어두면 관리적 측면에서도, git config 설정을 할 때에도 더 편리하다.\n디렉터리 예시\n. └── home ├── office └── personal ssh 키 발급 Step 1 사용할 계정들의 키 발급 1 2 ssh-keygen -t rsa -b 4096 -C \u0026#34;leelee@office.com\u0026#34; ssh-keygen -t rsa -b 4096 -C \u0026#34;leelee@personal.com\u0026#34; Step 2 로컬의 ssh-agent 에 발급받은 키 연결: 1 2 ssh-add -K ~/.ssh/id_rsa_personal ssh-add -K ~/.ssh/id_rsa_office 위에서 연결한 ssh 키 정보들은 ssh-add -l 로 확인이 가능하다.\nStep 3 ssh config 작성 발급받은 키들과 깃허브 계정 정보를 로컬 단에서 연결을 하기 위해 .ssh/config 에 관련 정보들을 작성을 한다.\n.ssh/config\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # Personal GitHub account Host github.com-personal HostName github.com User git AddKeysToAgent yes UseKeychain yes IdentityFile ~/.ssh/id_rsa # Office Github account Host github.com-office HostName github.com User git AddKeysToAgent yes UseKeychain yes IdentityFile ~/.ssh/id_rsa_office ssh 키 깃허브에 등록 ssh-keygen 결과물\nid_rsa.pub 외부로 공개되는 공개키이다. github에 등록되는 키다. 파일 끝을 보면 이메일을 확인할 수 있다. id_rsa 외부로 공개되면 안되는 비밀키이다. id_rsa.pub 에시\n1 2 3 4 5 6 7 8 9 10 11 ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQCruMY405yFL/6fvDvVFUTlxgXVO XRhdXlWDGsX5Kcn7yvEiGwBhVngvL8WWfA+hlelodoIAvlgnN9sJmVDDHF8XkK6r/ INdvFBAQ28+2GlOM8l038HDiCOTg/GzhEQK0hVzE0Cgsfrw2YMSxDJ9Gr9eSDSN0ia0LM LHMXdn5I5aeePGlt0boMIJgohzLVb4HT5KipBxbHETe05a8oOvmc9nS8r47ibSWpecuqDMJ7 7YrBa82X0d+5nAdZ1QiJg63k7ifJdPC4/CJHVvmglsHBzTkWEdn89R6q4OwyrUBRnrcITrF8 aCQMax2A5f7SaLcJ9xXQx47LT0ApfJ5UhHmLdK2vKzWEeEXhMfT3d0wIlppsEs5FuZRLqSAyZ QCn1IxZvV+KBAe5O3B9sidhULMnTzGqLCe3lLv3K0uI2MrP694LHqjW0duppbRbZZSNGdc0AM PtOqprI+lvBAugi6mN20sWRBMsHz1m1HdUI4yM85VAhYLNLgqs5n13ZfwUYEEh9EyqtGESToy 8DCSRqPqHNINB0skGBh9DF3ChjhdKvyn40AmpHdAzFlWgKGXbvx1DKzVhkubGNkISicwT7U+9 /18UuHJL2OsoFd9YcQ0qJqcrXWY2RxVkqAxzndxaPNeT5uXhZt0yNukm3UXd9khEd/Qn8F1n IqgHGiVCntP9wmQ== leelee@personal.com github setting 페이지로 이동 [Picture 1] Github Setting 메뉴바 SSH and GPG Keys 항목에서 New SSH key 등록 office를 위한 id_rsa_personal.pub 와 id_rsa_office.pud 를 각각 등록해준다.\n[Picture 2] New SSH key 등록 계정 정보 명시 Github에서 사용하는 name과 email을 별도로 설정해주지 않으면 개인 리포지토리에 올릴 커밋의 작성자가 회사 계정으로 되어있거나, 권한이 없다며 push를 할 수 없다. 떄문에 각각 계정별로 정보를 명시해줘야 한다.\n계정 정보 명시 시나리오 default: personal\nhome과 office 디렉터리에 각각 .gitconfig 파일을 하나씩 만들어준다. home의 .gitconfig에 personal 계정의 name과 email을 입력해준다. office 의 .gitconfig를 불러오는 설정을 추가해준다. office의 .gitconfig에 회사 계정의 name과 email을 입력해준다. home .gitconfig\n1 2 3 4 5 [user] name = leelee-personal email = leelee@personal.com [includeIf \u0026#34;gitdir:~/office/\u0026#34;] path = ~/office/.gitconfig office .gitconfig\n1 2 3 [user] name = leelee-office email = leelee@office.com 완성된 디렉터리 에시\n. └── home ├── .gitconfig ├── office │ └── .gitconfig └── personal 매번 해줘야 하는 작업들 레포지토리 주소 수정 .ssh/config 에서 연결할 Host를 계정별로 분기해서 각각 github.com-personal 과 github.com-office 로 구분을 했다. 때문에 매번 레포지토리를 만들거나, 클론할 때 구분하는 작업을 해줘야한다. git remote set-url 로 레포지토리를 연결할 떄도 똑같은 형식으로 해줘야 한다.\n기존 url\n1 git clone git@github.com:(Repo path).git 수정 url\n1 git clone git@github.com-office:(Repo path).git set-url 예시\n1 git remote set-url origin git@github.com-office:(Repo path).git Reference [CodeWords: A Mobile Application Blog by Heady]\n","date":"2022-01-12","permalink":"https://leeleelee3264.github.io/post/2022-01-12-git-multi-account/","tags":["General"],"title":"[General] 터미널에서 여러 개의 Github 계정 사용하는 방법"},{"content":"\n2021년 백엔드 개발자 면접 질문 중 Python과 기본지식과 관련된 질문을 정리한다.\nIndex\nPython DevOps 개발 전반 상식 Python 파이썬 쓰레드 파이썬 쓰레드에 대해 아는 점은 파이썬 자체에서 쓰레드를 지원하는 것은 아니고, 운영체제에서 제공하는 쓰레드를 사용한다는 것과, 쓰레드가 있다고 해도 한 타임에 한 쓰레드만 돌아간 다는 사실이었다. 이러한 특성의 원인은 파이썬의 디폴트 구현채인 CPython에 있었다.\nCpython 쓰레드 Cpython은 OS 쓰레드를 사용한다. 파이썬 쓰레드란, OS 쓰레드를 파이썬이 런타임에 관리를 하는 것 뿐이다. Cpython 인터프리터는 전역 인터프리터 록 (global interpreter lock) 메커니즘을 사용하여 한 번에 오직 하나의 스레드가 파이썬 바이트 코드를 실행하도록 보장한다.\n이런 인터프리터 전체를 잠구는 특성은 인터프리터를 다중스레드화 하기 쉽게 만든다, 하지만 한 번에 단 하나의 쓰레드를 실행시켜 쓰레드의 특징인 병렬성 잃어버리게 한다. (이는 multiprocessing 으로 보완)\n파이썬의 동시성과 병렬성 파이썬 쓰레드에 대해 너무 간략하게 알고 있었기 때문에 추가로 더 찾아보게 되었는데, 그때 프로그래밍에서 흔히 사용되는 개념인 동시성과 병렬성에 대해서, 또 파이썬은 이 둘을 어떻게 지원하는지 학습했다.\n원래는 동시성과 병렬성이 같은 뜻인줄 알았다. 알고보니 동시성은 짧은 시간에 한 가지 일을 처리 하고 금방 바꿔서 또 다른 일을 처리하는 걸 말한다. 결국 한 순간에는 한가지 일만을 처리하고 있다. 그런데 병렬성은 한 순간에 여러가지 일을 처리하고 있다.\n[Picture 1] 병렬성과 동시성 그런데 병렬성이 빛을 보기 위해서는 정말로 CPU 여러 개가 있어야 한다. 여러 가지 CPU들에 일을 하나씩 할당해 마치도록 하기 떄문이다. 여러가지 일을 한 번에 처리하는 병렬성이 좋아보이지만, 어떤 일을 처리하냐에 따라서 동시성과 병렬성을 선택해야 한다. 대기하는 일이 대부분인 입출력 I/O 작업이라면 병렬을 해서 CPU를 놀게 하는 것보다는 동시성을 사용하는 게 더 효과적이다. 이제 실제 파이썬 구현 부분을 봐보도록 하자.\n동시성 장점 편리하고, 잘 이해되는 방법으로 다른 리소스들을 기다리는 테스크를 실행한다. 여기서의 리소스들은 네트워크, 입출력, 하드웨어 디바이스의 신호 등이 될 수 있다.\n단점 General한 쓰레드의 단점들처럼, 객체 엑세스를 잘 관리해줘야 해서 CPU에 민감한 작업은 할 수 없다. 또한 실행되고 있던 A 쓰레드가 B 쓰레드로 스위치 되면서 A 쓰레드는 멈춰버리기 때문에 쓰레드를 사용하는 퍼포먼스적 이점이 없다.\n동시성의 대표 예시: 코루틴 Python 코루틴 and async 예시\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 import aiohttp import asyncio urls = [ \u0026#34;https://imdb.com\u0026#34;, \u0026#34;https://python.org\u0026#34;, \u0026#34;https://docs.python.org\u0026#34;, \u0026#34;https://wikipedia.org\u0026#34;, ] async def get_from(session, url): async with session.get(url) as r: return await r.text() async def main(): async with aiohttp.ClientSession() as session: datas = await asyncio.gather(*[get_from(session, u) for u in urls]) print ([_[:200] for _ in datas]) if __name__ == \u0026#34;__main__\u0026#34;: loop = asyncio.get_event_loop() loop.run_until_complete(main()) 코루틴 실행 시나리오\nget_from 함수가 코루틴이다. asyncio.gather 는 여러개의 코루틴 (다른 url들을 가진 다수의 get_from 함수 인스턴스) 를 생성해낸다. asyncio.gather 는 모든 코루틴이 실행 완료되기를 기다렸다가, 결과를 취합해서 리턴한다. 코루틴 장점 어떤 게 코루틴인지 문맥적으로 확실하게 구분을 할 수 있다. 쓰레드를 사용하면 어떤 함수도 쓰레드로 돌아갈 수 있어 혼동이 오는 데 코루틴은 그 점을 방지한다. 꼭 쓰레드를 필요로 하지 않는다. 코루틴은 파이썬 런타임이 직접 관리를 할 수 있다. 때문에 스위칭이 될 때에도 쓰레드보다 오버헤드도 작고, 메모리도 더 적게 필요로 한다. 코루틴 단점 async await 문법을 잘 지켜야 한다. 또한 쓰레드가 그러하듯, CPU에 민감한 작업은 할 수 없다.\n병렬성 말 그대로 프로세스를 여러 개 만드는 것이다. 각각의 CPU에서 돌아가기 때문에 멀티코어여야 한다. 프로세스를 여러개 만든 다는 말은 파이썬 인터프리터 인스턴스를 여러개 만든 다는 뜻이다!\n장점 쓰레드와 코루틴과는 다르게 객체의 다중 엑세스에 대한 관리가 조금 더 쉽다. 그리고 쓰레드와 코루틴은 다중 엑세스 때문에 모든 오퍼레이션이 순차적으로 돌아가게 하지만 이는 그럴 필요가 없다.\n단점 프로세스를 여러개 만드는 것 자체에 오버해드가 든다. 그리고 서브 프로세스들은 메인 프로세스에서 온 데이터 복사본이 있어야 하는데 이렇게 프로세스 사이에서 데이터가 왔다갔다 하기 위해서는 직렬화를 해야 한다. 이때 pickle 파이썬 라이브러리를 쓰는데, 보통의 객체들은 지원하지만 특이한 객체들은 지원을 하지 않는다.\n[Python concurrency and parallelism explained] 을 보면서 파이썬이 지원하는 동시성, 병렬성 구현을 살펴봤다.\n파이썬을 백엔드로 쓰면서 한꺼번에 몰린 요청을 처리하는 방법 이건 면접에서 나온 질문은 아니었고, 내가 궁금해서 물어본 부분이었다.\n파이썬은 인터프리터 언어다보니 백엔드로 사용을 하다보면 한계가 온다고 한다. 그래서 결국 컴파일 언어인 자바로 다시 만드는 경우가 왕왕있다고 하는데 한꺼번에 요청이 몰리면 어떻게 파이썬으로 처리를 하냐고 물어봤다.\n처리 방법 백엔드 코드를 쿠버네티스에 올려서 운용을 한다고 말씀해주셨는데, 그래서 요청이 몰릴때에도 이 쿠버네티스에 올린 파드를 증설한다고 한다. 즉, 컨테이너를 몇 개 더 만들어 요청을 분산처리 한다. 역시 코드 자체로 성능을 개선하기 보다는 돈을 써서 서버를 증설하는 방법이 회사에서 제일 많이 사용되는 방법이 아닐까.\nDjango 모듈 구조와 흐름 장고의 흐름은 [Picture 2] 같다. 굉장히 간단한 질문이었는데 장고를 거의 사용하고 있지 않아서 아주 뜨문뜨문 대답을 했다.. 한창 스프링을 쓰고 있었기 때문에 스프링에 빗대어서 대답을 했다.\n[Picture 2] Django 모듈 구조 View Controller Model 데이터가 들어있는 엔티티 장고의 모델의 필드들은 db 테이블의 컬럼들과 매핑이 된다. db와 바로 연결이 되어있기 때문에 모델을 변경하면 자동으로 db에 migrate 될 수 있다. Template 화면이다. DRF라면 거의 쓸 일이 없다. Django wsgi 항상 프로젝트 안의 wsgi가 뭔가 했는데 웹 서버 및 어플리케이션을 위한 파이썬 표준이라고 한다.\n[Picture 3] wsgi 흐름 클라이언트가 정적인 웹페이지를 요청했을 경우 웹서버에서 쳐 낼 수 있지만, 동적인 페이지를 요청했을 때는 웹서버에서 처리를 할 수 없고, 장고 서버에 리퀘스트를 넘겨줘야한다. 그런데 웹서버는 파이썬을 모른다. 그렇기 때문에 가운데에 wsgi가 인터페이스 역할을 해서 웹서버와 장고를 연결해준다.\n그럼 당연히 장고에서 제일 먼저 호출되는 부분은 wsgi가 될 수 밖에 없다. 얘가 리퀘스트를 물어다주니까. url 디스패처와 미들웨어 등등도 일단은 리퀘스트를 받아와야 해서 그 이후에 호출이 된다.\nDjango ORM objects.all objects.all 은 전체 자료를 불러온다. 모든 데이터가 필요하지 않은 이상 제일 효율이 좋지 않은 쿼리라 할 수 있다.\nselected_related VS prefetch_related Django ORM에서 쿼리 성능 향상으로 많이 쓰이는 기능은 select_related 와 prefetch_related인데 둘 다 즉시 로딩 (Eager-loading)과 한 번 DB에서 로딩을 한 이후부터는 캐싱이 되어서 쓰이는 등의 특징은 동일하지만 데이터를 가져오는 방식과 사용하는 상황이 조금 다르다.\n대게는 selected_related 사용을 추천하고 있다.\nselected_related one-to-one 같은 single-valued relationship 에서만 사용이 가능하다. DB에서 join을 해서 데이터를 가져온다. prefetch_related one-to-one 같은 single-valued relationship 에서 사용이 가능하다. many-to-many, many-to-one에서도 사용이 가능하다. DB에서 데이터를 가져올 때 2개의 단계를 거친다. 관계별로 개별 쿼리를 실행 (연결 테이블이 3개라면 3개의 쿼리가 따로 실행) 각각 가져온 데이터들을 파이썬에서 합쳐주기 참고하면 좋을 자료들 [select_related와 prefetch_related]\nselect_related와 prefetch_related에 대한 차이를 잘 설명해주고 있다. select_related와 prefetch_related를 함께 사용하는 방법도 있다. [당신이 몰랐던 Django Prefetch]\nDjango Prefetch 효율적 사용으로 성능개선하기 DevOps Nginx load balancing 스케쥴링 메서드 Round Robin 라운드 로빈은 운영체제 수업에서 배웠던 라운드 로빈과 동일한 알고리즘으로, 아무런 설정을 하지 않았다면 기본적으로 라운드 로빈 방식으로 스케쥴링이 된다. Least Connections 가장 적은 수의 active connection을 가지고 있는 서버에게 요청을 할당한다. IP Hash 똑같은 아이피에서 온 요청들을 똑같은 서버에서 처리할 수 있도록 보장한다. 동일 아이피의 기준은 IPv4일때는 앞의 3 옥텟이 동일해야 하고, IPv6에서는 모든 자리가 동일해야 한다. Generic Hash 유저가 정의한 키(hashed key value) 에 맞춰서 요청을 할당한다고 한다. 쿠버네티스 파드 쿠버네티스에서 생성하고 관리할 수 있는 배포 가능한 가장 작은 컴퓨팅 단위. 하나 이상의 컨테이너 그룹이다. 이 그룹은 스토리지 및 네트워크를 공유한다. 도커 개념 측면에서 파드는 공유 네임스페이스와 공유 파일시스템 볼륨이 있는 도커 컨테이너 그룹과 비슷하다.\nDocker Compose 도커를 실행하기 위해서는 도커 커맨드를 사용해야 하는데 간단한 커맨드는 상관이 없지만 볼륨을 연결하는 등의 추가 설정을 하다 보면 커맨드가 굉장히 길어진다. 이렇게 길어진 도커 커맨드를 쉘 스크립트로 짜도 되지만 이런 불편을 도커 컴포즈를 이용해 해결 할 수 있다.\n도커 컴포즈는 일종의 툴인데 도커 컨테이너 실행 환경을 YAML 파일로 관리할 수 있다.\n도커 컴포즈 예시\n1 2 3 4 5 6 7 8 9 10 11 version: \u0026#34;3.3\u0026#34; services: # db config web: build: . ports: - \u0026#34;4012:4012\u0026#34; environment: TZ: \u0026#34;Asia/Seoul\u0026#34; container_name: gov-prod-web_1 개발 전반 상식 DDD 디자인 패턴중 하나이다. Domain Driven Design의 약자이다. 보통 클린 아키텍처랑 함께 사용된다. 도메인 코드가 어플리케이션과 인프라 코드와 분리 되는 것에 집중을 한다는 점이 같기 때문이다. 도메인이란 프로젝트 안에서 개발되어야 하는 주제를 뜻하며, 비즈니스 로직이라 할 수 있다. 모든 소프트웨어 디자인 패턴의 목적처럼 시스템을 만들고 유지보수하는데 투입되는 인력 최소화에 있다. DDD 를 보다보면 유비쿼터스라는 단어가 많이 나온다. 개발자, 디자이너, 기획자가 모두 동일한 의미로 이해하는 단어라는 뜻이다. DDD 패턴은 비즈니스 로직인 도메인을 인프라와 어플리케이션(서비스) 와 분리하기 때문에 비즈니스 자체에 집중을 할 수 있다. 그래서 개발자가 다른 부서와 협업을 할 때 의사소통이 더 원활하게 이뤄질 수 있다. (db, network 등등의 로우 레벨 을 얘기하지 않기 때문)\n[Picture 4] Layer of the Clean Architecture 도메인 개념에 집중해서 아키텍처를 만들면 [Picture 4] 와 같다. 도메인 속에 엔타티가 들어있는데 이게 제일 중요한 개념이라고 생각한다. 엔티티에 최대한 많은 비즈니스 룰을 담아서 응집성을 높이고, 중복코드를 줄이는게 ddd의 목표이다. 단! 엔티티 밖의 메소드 (예를 들어 어플리케이션 레이어) 가 엔티티의 값을 변경하는 일은 절대 없어야 한다. 이는 객체지향에서 추구하는 캡슐화와 유사하다고 볼 수 있다.\n엔티티의 일은 엔티티 안에서 끝이 나야 한다! validation 조차도 UseCase에서 호출하면 안되고, 비즈니스를 제일 잘 아는 Entity에서 검증해야 한다.\n[Picture 5] Entity Anti Pattern MVC 패턴에서도 DDD에서도 서비스의 역할을 정하는 게 가장 어렵다. 조금만 잘못해도 코드가 서비스에 치중을 해 결국 서비스에서 엔티티의 값도 바꾸고, DB에 저장 요청도 하는 거대한 서비스 중심이 될 수 있기 때문이다. DDD 에서 서비스 (또는 use case) 의 역할은 여러가지 엔티티가 함께 다뤄져야 할 때이다. (여러가지 엔티티가 함께 다뤄지지 않아도 Controller 영역에서 바로 엔티티를 호출해서는 안된다)\nDAO와 DTO, ENTITY DAO DAO는 Data Access Object 로, 데이터에 엑세스하기 위한 객체다. 정말 데이터를 얻어오기 위해 접근을 하는 객체라 다른 말을 할 것이 없다. DAO 볼때마다 드는 생각이 그럼 Repository 는 뭐지? 둘의 차이는 [Picture 6] 와 같다.\nDAO VS Repository 레포지토리가 더 상위계층이고 DAO가 하위계층이다.\n레포지토리는 도메인과 데이터 매핑 레이어 가운데에 있는 존재이다. DAO 는 말 그대로 못생긴 쿼리를 한 번 숨기는 역할을 하고, 레포지토리는 여러가지 DAO를 활용해서 데이터를 상위 계층으로 전달을 할 수 있다.\n[Picture 6] DAO and Entity DTO DTO는 Data Transfer Object로, 데이터를 여러 계층 사이로 전송하기 위한 객체다. 스프링관점으로 생각을 해보자면 DAO가 DB에서 가져온 정보를 서비스 계층으로 넘길 때 DTO 에 넣어서 전송을 하고, 서비스 계층이 컨트롤러 계층으로 넘길 때 DTO를 사용한다 .\nEntity Entity 는 DDD의 관점에서 보면 비즈니스 로직이고, JPA로 보면 DB테이블 그 자체이다. 이렇게 중요한 부분 그 자체이기 때문에 변경도 엔티티 안에서 일어나야 하고, 다른 계층으로 넘길때도 그냥 넘겨서는 절대 안된다. 꼭 DTO에 담아서 전달을 해 줘야 한다.\nRequest Method GET 데이터를 조회하기 위한 요청이다. 몇번을 반복적으로 요청해도 받아보는 데이터에 변화가 없다는 게 중요한 점이다. 이런 GET의 성격 때문에 무엇인가를 바꾸려는 작업을 GET으로 만들지 말아야 한다. POST POST는 무엇인가를 바꾸기 (update/insert/delete) 위한 요청이다. 같은 요청을 여러번 날리다보면 사이드 이펙트가 발생할 수 있다. GET VS POST POST가 GET보다 보안상 안전하다. GET은 URL안에 모든 정보를 다 포함하고 있기 때문에 웹서버 등지에서 엑세스 로그를 남길 경우 GET에 있던 정보들이 그대로 남게 된다. 하지만 POST 또한 데이터를 URL에 넣지 않고 body에 넣었단 차이만 있을 뿐이고, 이 body도 까서 볼 수 있기 때문에 마냥 안전하다고 볼 수 없다. 민감한 데이터는 어지간하면 네트워크를 타고 움직이지 않는 것이 제일 좋다. 멱등성 이렇게 GET처럼 똑같은 요청(연산)을 여러번 하더라도 결과가 달라지지 않는 성질을 멱등성(Idempotent)라고 한다. Rest api에서 제일 많이 사용되는 메서드들 중에서 POST를 제외한 GET, PUT, DELETE은 멱등성을 지켜야 함을 명심하면서 서버 구현을 하도록 해야 한다.\n[Picture 7] HTTP Method POST VS PUT 처음에 HTTP 메서드들을 배웠을 때 POST와 PUT 구분이 어려웠다. 결국 POST나 PUT 둘 다 CREATE이 가능하기 때문에. 그런데 찾아보니 아래와 같은 사항으로 구분을 할 수 있었고, 다시 고려해보면 PUT은 CREATE 보다는 UPDATE에 더 적합해 보인다.\n시나리오 1: 멱등성을 유지해야 하는가?\nPOST = (a++) ⇒ 계속 1이 증가하여 결과 값이 매번 달라진다. PUT = (a=4) ⇒ 계속 a는 4로 업데이트 되기 때문에 결과값이 매번 같다. 시나리오 2: 리소스 결정권이 있는가?\n리소스 결정권은 클라이언트가 이 리소스의 위치 (리소스 id)를 정확히 아는지 모르는지의 차이다.\nPOST /articles PUT /articles/11120 PUT 클라이언트가 리소스가 어디에 저장이되는지 정확히 알 수 있다. 내가 많이 쓰던 POST의 path variable 이 들어간 URL이 사실은 PUT에 맞는 컨벤션이었다. POST POST는 해당 URL을 요청하기만 하면 원하는 리소스를 만들어주겠다며 factory만 제공해주고 리소스의 정확한 위치는 제공하지 않고 있다. PUT VS PATCH 둘 다 리소스를 업데이트 한다는 느낌이 강하지만 작은 차이가 있다.\nPUT은 리소스 전체 또는 다수를 업데이트 하는 느낌이 강하다. PATCH는 리소스의 일부 또는 단 하나를 업데이트 하는 느낌이 강하다. Cross Origin Resource Sharing CORS의 정확한 개념 줄여서 CORS라고 한다. 위키사전에서 찾아봤을 때 교차 출처 리소스 공유라고 직역을 하고 있다.\n원래 내가 알고 있던 부분은 같은 출처가 아닌, 다른 출처에서 리소스 요청을 했을 때 보안상 요청을 처리하지 못하고 error를 보낸다 였다. 그래서 이 문제를 해결하기 위한 정책이 CORS이고. 그런데 내가 심각하게 오해하고 있던 부분은, 여기서의 Origin이 서버를 의미하는 게 아니라 요청을 보낸 클라이언트를 의미하는 것 이었다.\n브라우저에 치중되어 있는 CORS RFC에 존재하는 리소스 요청 정책은 2가지 이다. 하나는 SOP (Same-Origin Policy) 이고 다른 하나는 CORS이다. 하지만 이 정책들은 브라우저에민 구현되어있는 스펙이다.\n서버와 서버가 리소스를 주고 받을 때는 CORS 문제가 발생하지 않는다. 서버에서 응답을 주더라도 브라우저 단에서 응답을 막고 error를 내려준다. 때문에 서버 로그에서는 정상적으로 응답이 내려갔다고 보이며 디버깅이 상당히 어려워진다. 때문에 서버에서도 CORS의 존재에 대해 염두해 두고 있어야 한다!\n[Picture 8] CORS [Picture 8] 의 설명이 제일 명확했다. 브라우저는 스크립트가 요청을 보낼 때 지금 스크립트가 서비스 되고 있는 url과 요청을 보내는 url을 비교한다. 여기서 동일한 url로 보여지는 조건은 프로토콜/스키마 (https, http) 와 호스트 (www.google.com) 포트번호가 모두 같을 때 동일한 url로 인정한다.\nCORS 발생 시나리오 Preflight Simple Request Credentialed Request 어떻게, 어떤 조건에서 헤더를 채워서 보내는지에 따라 브라우저가 임의로 하나의 시나리오를 보내 요청을 보낸다. 어떤 조건속에서 어떤 CORS 시나리오가 만들어지는지 잘 유념하자.\nCORS 해결 방법 클라이언트에서 발생하는 문제이지만 이걸 해결하기 위해서는 서버가 작업을 해야 한다. 해당 리소스에 대해 어떤 origin들이 요청을 할 수 있는지 응답 헤더중 하나인 Access-Control-Allow-Origin에 기입을 해주는 것이다. 여기에 와일드 카드를 넣어버릴 수 있지만 이럼 언제나 보안 문제가 일어날 수 있음을 명심하자. Access-Control-Allow-Origin에 들어있는 Origin들과 요청을 보냈던 클라이언트의 Origin을 보고 브라우저가 유효한 응답임을 판단한다.\nOptions Method Options 메소드가 언제 쓰이는지 궁금했었는데 이렇게 Access-Control-Allow-Origin 을 알기 위해 선행 요청을 보낼 때도 사용이 된다고 한다.\n[Picture 9] Options Method 참고하면 좋을 자료들 [Cross Origin Resource Sharing - CORS] [CORS는 왜 이렇게 우리를 힘들게 하는걸까?] Authentication 과 Authorization의 차이 Oauth2에 대해 대답을 하다가 나온 문제였다. 둘이 어떤 개념인지는 알지만 용어적으로 낯설어 제대로 대답을 하지 못 해 이번에 정리를 하면서 찾아봤다. Authentication과 Authorization을 쉽게 비교하기 위해 표를 작성했다.\n/ Authentication Authorization 역할 사용자 신원 확인 리소스, 기능 에 대한 엑세스 권한 확인 목적 사용자가 누구인지 확인/판별 사용자가 해당 리소스/기능에 대해 사용 권한이 있는지 확인/판별 방법 비밀번호 인증, 2FA, Oauth RBAC, ABAC, MAC, DAC 절차는 Authentication ➡️ Authorization 이 이루어진다고 보면 된다.\nAuthentication이 되었다고 해도 보내는 리퀘스트마다 이 사용자가 정말로 자격이 있는지 매번 확인을 하는 절차가 Authorization이라고 생각한다.\nAuthorization 방법 RBAC\nRole-based access control 조직에서 사용자에게 부여한 역할을 근간으로 access 권한을 부여한다. 예시: 매니저에게 스케줄 접근 권한을 부여한다. ABAC보다 더 구현하기가 쉽다. ABAC\nAttribute-based access control 직함, 부서와 같은 사용자의 특성에 따라서 권한을 부여한다. 예시: 백엔드 개발팀에게 DB 접근 권한을 부여한다. RBAC보다 더 유연하고, 복잡한 access control을 수용할 수 있다. MAC\nMandatory access control 미리 정의된 역할, 정책, 규정 등에 따라서 권한을 부여한다. DAC\nDiscretionary access control 리소스의 소유자가 임의로 접근 권한을 부여한다. 스키마를 짤 때 하는 고민 이건 정말 광범위한 범위가 아닐까? 아직도 어떤 방식으로 스키마를 짜는 게 최적인지 잘 모르겠다. 정말 서비스마다 다른 것 같다. 내가 스키마를 짜면서 확실하게 느꼈던 부분 정도만 정리를 했다.\n높은 수준의 정규화를 했을 때는 조회를 할 때 JOIN을 많이 해줘야 하지만 데이터 수정과 삭제가 용이하다. 낮은 수준의 정규화를 했을 때는 조회할 때 JOIN을 많이 안 해줘도 된다. 그러나 중복된 데이터가 많아진다. 옛날에는 하드가 비싸서 정규화에 신경을 많이 썼는데 요즘은 하드가 비싸지 않아 정규화를 많이 할 필요가 없다고 한다. 히스토리 처럼 아래로 쌓이는 데이터의 경우 높은 수준의 정규화를 하지 않아도 된다. UPDATE가 자주 일어나는 데이터인지 INSERT가 많이 일어나는 데이터인지에 따라서 스키마가 달라질 필요가 있다. 고수준 인터페이스와 저수준 인터페이스 고수준 인터페이스와 저수준 인터페이스라는 단어보다는 고수준 모듈, 저수준 모듈 이라는 말을 더 많이 사용한다.\n고수준 모듈 추상화가 되어있는 기능을 제공한다. 1 2 3 public interface Animal { void eat(); } 저수준 모듈 고수준에서 제공하는 기능을 실제로 구현한다. 1 2 3 4 5 6 public People implements Animal { @Override void eat() { Systems.out.println(\u0026#34;People eat many things\u0026#34;); } 여태 고수준과 저수준을 반대로 생각하고 있었다. 고수준이 실제로 구현이 된 부분이고 저수준이 추상화의 부분인줄 알았으나 실제는 반대였다.\n","date":"2022-01-01","permalink":"https://leeleelee3264.github.io/post/2022-01-01-interview-python/","tags":["General"],"title":"[General] 2021년 백엔드 개발자 면접 질문 (2/2) - Python, 기본 지식"},{"content":"\n2021년 백엔드 개발자 면접 질문 중 Java와 관련된 질문을 정리한다.\nIndex\nIntro 기술면접 회고 Java Intro 10월 달 부터 이직을 준비하기 시작하며 Resume 와 Cover Letter 를 썼고, 11월 달에는 면접을 보러 다녔다. 대부분의 면접들이 몇 단계로 이루어져있었는데 기술 면접에서 면접관들이 물어봤던 질문들을 기록해두고 공유하면 좋을 것 같아 포스팅을 하기로 했다.\n지원분야가 Backend이기 때문에 대부분의 질문들이 Backend 와 관련이 되어있지만, 직전에 근무하고 있던 회사에서 DevOps의 경험도 있다고 이력서에 적어서 DevOps와 관련된 질문들도 약간 있었다. 깃허브에 [DevOps 커리큘럼]이 있는데 면접 때 보고 가면 도움이 될 거 같다.\n기술면접 회고 커리어를 시작하고 이렇다 할 면접들을 보러 다닌적이 없었는데 기술면접을 보고 나니 왜 기술 공부를 더 열심히 해야 하는지 깨달았다. 이론적인 측면들은 지루해서 공부를 피하기 마련이었는데 기술면접에서 다 물어보는 것들이었고, 결국은 이 이론적인 측면들을 잘 알아야지만 더 좋은 코드를 만들 수 있다.\n깊이는 없는 새로운 기술에 대한 욕심 파이썬을 그냥 써보기만 했고 깊이가 없었다. 다른 개념들도 마찬가지였다. 막 커리어를 시작했을 때는 이것저것 조금씩 공부하는 게 좋았는데 이제는 깊이가 있는 공부를 해야하는 때가 아닌가 싶다.\n퇴사를 준비하면서 이것저것 많이 여쭤봤던 팀장님께 인사를 드렸는데 기술에 대한 욕심을 조금 버리는 게 좋다고 조언해주셨다. 그도 그럴게 2년 동안 정말 신기술에 집착을 많이 했던 것 같다\u0026hellip; 그 분이 항상 해주시던 말씀이 프레임워크를 공부하기보다는 언어를 더 공부하라 였는데 결국 기본기가 제일 중요한 게 아닐까? 디자인 패턴과 정규식은 어디에서도 쓰이는 것처럼.\n빨리 사둔 디자인 패턴 책도 읽고 이팩티브 자바도 다시 읽어봐야겠다. 자바 빨리빨리 공부하고 파이썬으로 진짜 넘어가야지!\n면접 질문 추이 이력서에 주로 사용하던 언어가 자바라고 썼기 때문에 자바 질문이 들어왔고 면접을 본 회사들은 대부분 파이썬을 사용하고 있었기 때문에 파이썬 질문도 많았다. 질문의 구성은 크게 아래와 같았다.\n자바 질문 파이썬 질문 개발 전반 상식 이번 포스팅에서는 자바 질문과 답변을 다룬다.\nJava 실행중인 Spring Boot에서 변경된 properties 로드하기 맨 처음에는 Spring boot에서 properties 를 적용하는 방법에 대한 질문인 줄 알고 Spring boot externalized properties 우선순위에 대해 답변했는데 아니었다. 이미 러닝중인 서버에 수정된 propreties를 재시작없이 어떻게 반영하냐에 대한 질문이었다.\nSpring Boot Actuator Spring Boot Actuator 를 이용하면 된다. 이렇게 config 를 러닝 타임에 업데이트 하는 상황은 서버가 하나 떠있을 때 보다는 서버를 여러 개 띄워두는 Spring Cloud 환경에서 많이 사용하는 것으로 보인다.\n[Picture 1] Spring Cloud Config Spring Cloud Config는 Spring Cloud Config 서버와 클라이언트 어플리케이션 ([Picture 1] 에서 Microservice #1 #2 #3 로 표기된 서버들) 로 구성이 되어있는데 config 서버 설정에 변경이 생겼을 때 클라이언트 어플리케이션도 변경을 반영해줘야 한다. 이때 다시 시작하지 않고 actuator 를 이용해서 refresh 하면 된다.\n여기서 actuator 는 실행중인 스프링 어플리케이션 내부 정보를 REST 엔드포인트와 JMX MBeans(Java Management Extension. 모니터링용 객체) 로 노출시키는 스프링 부트의 확장모듈이다. 실행중인 스프링 어플리케이션을 뜯어 볼 수 있다는 게 중요하다!\nSpring Boot Actuator 사용법 클라이언트 어플리케이션에 actuator 라이브러리를 implement 한다. 클라이언트 어플리케이션 application.properties또는 application.yml 에 actuator 사용을 위한 설정을 추가 한다. Config 서버에서 가져온 설정을 사용하는 코드 부분에 @RefreshScope 추가 한다. http://클라이언트서버/actuator/refresh POST 호출로 변경사항 적용한다. 참고하면 좋을 자료들 [Spring cloud config 리프래시 하기 (Use RefreshScope)] [Spring Cloud Config 2] JDBC Java Database Connectivity의 약자이다.\n데이터베이스 연결을 관리하는 자바 API로, 쿼리와 커맨드를 발행하고 데이터베이스에서 건내주는 결과 셋을 처리한다. JDBC는 자바 어플리케이션이 데이터베이스 또는 RDBMS와 소통하기 위한 프로그래밍 레벨 인터페이스를 제공한다.\n[Picture 2] JDBC 상세 JDBC는 결과적으로 자바 코드로 데이터베이스를 관리할 수 있게 만들어준다.\nJDBC API는 자바 어플리케이션과 JDBC Manager 사이의 커뮤니케이션을 지원한다. JDBC Driver는 데이터베이스와 JDBC Manager 사이의 커뮤니케이션을 지원한다. 직렬화 직렬화는 객체를 바이트 스트림으로 바꾸는 것이다. 이와 반대로 바이트 스트림을 객체로 바꾸는 것은 역직렬화라고 한다.\n객체는 플랫폼에서 독립적이지 못하다. 그래서 [Picture 3] 처럼 파일, 메모리, 데이터베이스 처럼 다른 시스템으로 보내려고 할 때 플랫폼에서 독립적인 바이트 스트림으로 변환을 한다.\n[Picture 3] 직렬화란? 자바 직렬화 자바 직렬화도 마찬가지로 JVM 메모리에 올라가있는 객체를 byte 스트림 (byte 형태의 데이터) 바꾸는 것이다.\n[Picture 4] 자바 직렬화 상세 그런데 요즘의 API들을 생각해보면 데이터를 다 JSON으로 직렬화 해서 내보내고 있다. JSON 처럼 문자열로 변환하는 형태가 아니라 이진 표현으로 변환해서 내보낼때는 Protocol Buffer를 사용한다고 한다. 그럼 손쉬운 JSON과 Protocol Buffer가 아니라 번거로운 자바 직렬화를 사용할 때의 장점은 무엇일까?\n자바 직렬화 장점 자바 직렬화의 장점은 자바에 최적화 되었다는 점이다. 자바 시스템에서 또 다른 자바 시스템으로 데이터를 보낼 때 손쉽게 직렬화-역직렬화를 할 수 있다. 서블릿 세션이 대표적인 사용처라고 하는데 홈페이지를 만들때도 유저의 로그인 세션을 직렬화해서 관리했던 게 생각이 난다.\n자바 직렬화 문제점 외부에 나가서 장기 보관될 정보는 자바 직렬화를 사용하지 않는다. 추후에 변경이 있으면 오류가 나기 때문이다. 자바 직렬화에 사용하는 시리얼 ID도 개발시에 따로 관리를 해줘야 한다. 직렬화-역직렬화를 할 때 타입과 필드의 변경에 엄격해 오류가 잘 발생할 수 있다. 때문에 자주 변경되는 클래스는 자바 직렬화를 사용하지 않는 것이 좋다. 자바 직렬화된 데이터는 JSON 보다 훨씬 크기가 크다. 떄문에 직렬화된 데이터를 캐시등의 이유로 존재하는 Redis 와 같은 메모리 서버에 저장을 하면 트래픽에 따라 비용이 급증할 수 있다. 위와 같은 이유들로 최대한 JSON 포맷으로 변경을 고려해야 한다. 참고하면 좋을 자료들 [자바 직렬화, 그것이 알고싶다. 훑어보기편] 저번에 FCM으로 보낼 푸시 메세지들을 직렬화해서 Redis 메모리에 넣어두어 어플리케이션 서버에서 큐 구조로 순차적으로 꺼내갈 수 있게 해뒀는데 이번에 직렬화를 찾아보니 JSON으로 바꾸는 방향으로 해야겠다..\nInterface와 Abstract class의 차이 추상 클래스와 인터페이스 모두 선언만 가능하고, new 를 사용해서 인스턴스를 만드는 게 불가능하다는 공통점이 있지만, 차이점이 더 많다.\nInterface 인터페이스는 A is able to B 를 만족시킨다. 인터페이스는 자바8, 9에 들어오면서 원래의 인터페이스에서 많은 변화가 생겼다.\nInterface의 변화 추상메서드만 만들 수 있었으나 JAVA 8에 들어오면서 default를 사용해 메서드 구현이 가능해졌다. 필드를 가질 수 없었으나 JAVA 8에 들어오면서 final 과 static 필드를 가질 수 있게 되었다. 접근 지정자가 default로 private 이었으나 JAVA 9에 들어오면서 private 사용도 가능해졌다. Abstract class 추상클래스는 A is B를 만족시킨다. 추상메서드를 만들 수 있고, 구현이 된 메서드를 만들 수 도 있다. 일반 클래스와 마찬가지로 필드도 가질 수 있다.\nAbstract class 사용시 주의 점 실무에서 추상클래스를 사용했는데 상속관계를 제대로 고려하지 않았었기 때문에 모두 리팩토링을 해야 했다. 추상클래스를 사용을 할 때에는 super 클래스와 하위클래스의 관계를 잘 생각해서 구현을 해야 하고, 추상클래스 보다는 인터페이스 사용을 권장한다.\nDI, IOC Dependency Injection과 Inversion Of Control. Spring에서 처음 접한 개념인데, Spring 뿐만 아니라 다른 언어와 프레임워크에서도 널리 사용되는 개념이다.\nIoC는 설계 원칙이고 DI 는 IoC 원칙을 지키기 위한 디자인 패턴이다. 실제로 DI 말고도 IoC를 위한 다양한 패턴이 존재한다.\n[Picture 4] IoC Pattern 구현 방법론 IoC IoC는 객체 사이의 결합도를 줄이기 위해 제어를 역전시킨다. 가장 흔한 제어의 역전의 예시로는 객체 생성이 있다. 제어 역전이 일어나면 객체를 직접 생성하지 않고, 이미 생성되어 있는 코드를 사용하기만 한다.\nSpring으로 예를 들어보면 Bean으로 선언된 객체들은 Spring에서 관리를 한다. Bean 객체들은 IoC Container 안에 생성이 되고, 관리가 된다. 우리는 그 객체들이 어떻게 관리가 되고 있는지 알 필요 없이 해당 객체를 사용해야 할 때 의존성을 주입받아 (DI) 사용을 하면 된다.\nDI DI는 의존성 있는 객체의 생성을 class 외부에서 수행한 후, 다양한 방법으로 해당 객체를 class 에게 제공한다.\nDI 예시\n1 2 3 4 5 6 7 8 9 10 @Controller public class TestController { private final TestService testService; public TestController(TestService testService) { this.testService = testService; } } DI을 이용해 객체 결합도를 느슨하게 하기 위해서는 Client, Service, Injector 클래스가 필요하다. Injector 클래스가 Service 객체를 만들어 Client 클래스에 제공하는 형태인데 코딩을 하면서 무수히 많은 객체들의 의존성을 매번 이렇게 만들어 줄 수는 없다.\n때문에 IoC Container와 같은 기능을 제공하는 프레임워크를 사용해 위와 같은 일을 위임한다. 프레임워크를 사용하면 객체의 의존도를 고려하면서 객체의 생성. 소멸을 신경쓰지 않아도 되고 비즈니스 코드에 더 집중을 할 수 있고 결과적으로 변경에 유연한 코드를 만들 수 있다.\n참고하면 좋을 자료들 [Dependency Injection, IoC, DIP, IoC container정리] Static static 키워드를 사용한 메서드와 변수는 해당 클래스의 객체를 생성하지 않아도 해당 메서드와 변수를 사용할 수 있다. 매번 객체를 생성하지 않아도 되기 때문에 손쉽게 사용을 할 수 있어 내 경우는 Utill 성 메서드들을 static으로 만들었다.\nstatic 의 특징 static은 메모리에 딱 한 번 할당이 된다. 일반적인 객체들이 Heap 영역에 할당이 되는 것과는 다르게 stack 영역에 만들어진다. 때문에 Heap 처럼 Garbage Collection을 걱정하지 않아도 된다. stack 은 모든 객체가 공유하는 메모리 공간이기 때문에 객체 생성이 없이도 static 메서드와 변수를 사용할 수 있다. Java8에서의 static Java 8이전에 static 변수와 메서드는 JVM 메모리에서 PermGen에 저장이 되었으나 Java 8에서는 PermGen 가 사라지고 MetaSpace가 그 역할을 대신한다. 변경된 Java 8 JVM 구조는 [Picture 5] 와 같다.\n[Picture 5] IoC Pattern 구현 방법론 static 핵심 구현 사항 인스턴스 변수 (non-static) 를 사용하는 메서드는 인스턴스 메서드를 사용하고 클래스 변수 (static) 을 사용하는 메서드는 static 메서드를 사용한다.\n클래스를 설계할 때 인스턴스에 공통적으로 사용해야 하는 맴버변수에 static을 사용한다. static 메서드에서는 static이 아닌 맴버변수는 사용할 수 없다. static 이 아닌 메서드에서는 static인 맴버변수를 사용할 수 있다. 메서드 안에서 인스턴스 변수를 사용하지 않는다면 static을 사용을 고려한다. 1과 같은 특성으로 한 인스턴스에서 static 변수의 값을 바꿨을 때 모든 인스턴스에 변경이 적용된다. 이와 같은 모두 변경 불상사를 피하기 위해서 static 변수를 사용할 때 final 키위드를 함께 사용해 변경을 불가하게 만든다.\n2, 3과 같은 일이 일어나는 이유는 인스턴스 변수가 static 변수 또는 메서드를 사용하는 시점에서는 static 변수와 메서드가 이미 생성이 되어있지만 반대의 경우에는 사용 시점에 인스턴스가 만들어졌는지 알 수 없기 때문이다.\n","date":"2021-12-02","permalink":"https://leeleelee3264.github.io/post/2021-12-02-interview-java/","tags":["General"],"title":"[General] 2021년 백엔드 개발자 면접 질문 (1/2) - Java"},{"content":"\nShell script로 MySQL DB full backup을 구현하는 방법에 대해 알아본다.\nIndex\n백업 진행 순서 백업에 사용된 스크립트 \u0026amp; 세부 사항 개선해야 할 점과 참고자료 백업 진행 순서 Intro: 백업의 종류 SQL 백업에는 전체백업, 증분백업, 차등백업이 있다. 백업 종류에 대한 더 자세한 정보는 이전의 포스팅에서 확인이 가능하다. [SQL 백업 종류]\n백업 환경 상용에서 사용하는 디비의 데이터를 일주일에 한 번 씩 로컬 서버로 백업을 해두기로 했다. 빈번하게 진행되는 백업이 아니라서 시간이 오래 걸리지만 간단한 전체백업 을 하기로 했다. 대신 서비스에 장애가 가지 않도록 사용량이 적은 월요일 오전 3시에 백업이 되도록 crontab으로 스케쥴링을 걸어두었다.\n우분투 환경의 쉘 스크립트를 작성해 Mysql db 백업을 진행했다.\n백업 과정 현재 상태 상용에서 사용하는 디비 이름은 mydb 이다. 과거의 상용 데이터를 가진 로컬 디비 이름은 prev_mydb이다. 백업을 진행하는 일시는 2021년 09월 06일이다. 로컬 디비에는 저번주인 08월 30일의 snapshot 인 mydb와 2주전인 08월 23일의 snapshot인 prev_mydb가 있다. 백업 목표 로컬디비의 mydb를 prev_mydb 라는 이름으로 변경한다. 상용디비의 mydb를 통째로 로컬디비로 옮겨온다. (전체백업) 로컬 디비에 남는 디비는 prev_mydb와 mydb가 된다. 상세 작업 순서 mysql은 데이터베이스 자체를 RENAME 할 수 없어서 prev_mydb를 지우고 다시 만들어야 했다. 백업과정을 [Picture 1] 에서 모식도로 나타냈다.\n[Picture 1] DB 백업 상세 작업 상용 서버에서의 action 로컬 서버가 상용 db로 접속 mysqldump를 실행해서 상용 디비의 mydb의 전체 데이터를 로컬 서버로 복사 (prod mydb data 생성) 로컬 서버에서의 action 서버에는 지금 막 덤프한 prod mydb data와 mydb, prev_mydb 3개가 존재한다. prev_mydb 를 삭제하고, mydb를 복사한다. (local mydb data 생성) local mydb data 로 prev_mydb 를 생성한다. mydb를 삭제한다. prod mydb data 로 mydb 를 생성한다. 작업이 완료되면 로컬 디비에는 새로운 mydb와 prev_mydb 가 남는다. prod mydb data 는 압축을 해 gz 파일 형식으로 로컬 서버에 남겨둔다. 백업에 사용된 스크립트 \u0026amp; 세부사항 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 #!/bin/bash # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ # Title: bk_prod.sh # Author: Seungmin Lee # Date: 2021-08-16 # Schedule: Every Sunday AM 03:00 # # This script is for backup rds (product) database. Backup db will be used in statistic part cd ~/bk_prod DATE=`date +\u0026#39;%Y%m%d\u0026#39;` # dest db info HOST=rds.examplehost.com READ_ID=rds_id # src db info LOCAL_ID=local_id # pre setting file. # default pre setting file is ~/.my.cnf LOCAL_CNF_PATH=~/bk_prod/.my.local.cnf LOCAL_BASIC=\u0026#34;--defaults-file=${LOCAL_CNF_PATH} --user=${LOCAL_ID}\u0026#34; DB=mydb PRE_DB=prev_mydb # view will be ignored from db dump VIEW_ONE=${DB}.v_channel_lang VIEW_TWO=${DB}.v_voice_usage # --------------------------------------------------------------------------------------------------------------------------------------------- # # --------------------------------------------------------------------------------------------------------------------------------------------- # # dump rds db first mysqldump --single-transaction --host=${HOST} --user=${READ_ID} --ignore-table=${VIEW_ONE} --ignore-table=${VIEW_TWO} ${DB} \u0026gt; ${DB}${DATE}.sql # local db backup for db rename mysqldump ${LOCAL_BASIC} ${DB} \u0026gt; local_${DB}${DATE}.sql # create and dump previous db in local db # ---------------------------------------------------------------------------------------------------------------------------------------------- # # ---------------------------------------------------------------------------------------------------------------------------------------------- # # check prev_mydb is in local db RESULT=`mysql ${LOCAL_BASIC} --skip-column-names -e \u0026#34;show databases like \u0026#39;${PRE_DB}\u0026#39;\u0026#34;` if [ ${RESULT} == ${PRE_DB} ] then echo \u0026#34;DELETE FORMER ${PRE_DB}\u0026#34; mysql ${LOCAL_BASIC} -e \u0026#34;DROP DATABASE ${PRE_DB}\u0026#34; else : fi mysql ${LOCAL_BASIC} -e \u0026#34;CREATE DATABASE ${PRE_DB}\u0026#34; mysql ${LOCAL_BASIC} pre_${DB} \u0026lt; local_${DB}${DATE}.sql # now delete xouchcare db in local db mysql ${LOCAL_BASIC} -e \u0026#34;DROP DATABASE ${DB}\u0026#34; # make new xouchcare db in local db mysql ${LOCAL_BASIC} -e \u0026#34;CREATE DATABASE ${DB}\u0026#34; mysql ${LOCAL_BASIC} ${DB} \u0026lt; ${DB}${DATE}.sql echo \u0026#34;DB back up fine\u0026#34; # done gzip *.sql mysql command options —defaults-file (line 25) 쉘스크립트에서 DB 접속 문제\nmysqldump와 같은 mysql에서 사용하는 커맨드는 실행을 위해서는 db 접속 호스트 정보와 id, pw를 함께 입력해야 한다. 터미널에 커맨드를 실행할 때는 상관이 없지만 쉘스크립트로 실행을 할 때는 따로 터미널로 계정 정보를 기입해 줄 수 없다. 이를 해결하기 위해서는 계정 정보를 넣어둔 my.cnf 를 미리 만들어주면 된다.\n실행하려는 쉘스크립트에서 접속을 해야 하는 디비가 상용과 local 두가지라서 각각의 계정 정보를 넣어둔 my.cnf 와 my.local.cnf 파일을 만들었다. my.local.cnf 에서 보는 것처럼 실행을 해야하는 커맨드별로 계정정보를 넣어두면 쉘에서 해당 mysql 커맨드를 실행할 때 id, pw 를 입력하지 않아도 된다.\ncnf 파일 경로 설정\n기본 my.cnf 위치는 ~/my.cnf 로 홈디렉토리에 생성이 되거나, 홈디렉토리에 직접 만들어주면 된다. 쉘에서도 따로 경로를 표기하지 않아도 알아서 my.cnf의 위치로 가서 파일을 읽어온다. my.local.cnf 처럼 추가로 만들때면 경로를 따로 표기해줘야 한다. 실제 커맨드 라인에서 사용을 할 때는 —defaults-file=path_of_cnf_file 을 써주면 된다.\n상용 디비 계정 정보를 넣어둔 my.cnf 파일\n1 2 3 [mysqldump] user=rds_id password=rds_pw Local 디비 계정 정보를 넣어둔 my.local.cnf 파일\n1 2 3 4 5 6 7 [mysqldump] user=local_id password=local_pw [mysql] user=local_id password=local_pw —ignore-table (in line 37) mysqldump 를 실행할 때 덤프를 하지 말아야 하는 테이블을 명시해주면 덤프에서 제외가 된다. 제외를 할 테이블들이 있으면 —ignore-table=table_name 을 하나씩 적어주면 된다. 아쉽게도 하나의 옵션에 여러개의 테이블을 쓸 수는 없었다.\n내 경우는 로컬 디비에 접속을 하려는 계정이 view에 대한 권한이 없었기 때문에 아예 상용 디비를 덤프할 때 제외를 시켜놨다.\n—single-transaction (in line 37) innodb는 트랜잭션을 지원하기 때문에 해당 옵션을 사용할 수 있다. 덤프를 할 때 하나의 새로운 트랜잭션을 열어서 진행을 한다. 그럼 덤프를 실행하는 딱 그 순간의 디비가 스냅샷처럼 덤프가 떠진다. 덤프를 하고 있는 중에 db에 실행된 delete, insert, update과 같은 DML 쿼리의 결과는 반영하지 않는다.\n아예 트랜잭션을 걸기 때문에 덤프를 하면서 작업중인 테이블에 락을 하나하나 걸지 않아도 된다. innodb가 아니면 lock-tables 옵션을 걸어줘야 한다.\nsingle-transaction의 한계\n단, 데이터를 바꾸는 DML은 트랜잭션이 막을 수 있지만 스키마를 바꾸는 DDL은 트랜잭션이 막을 수 없다고 한다. 그래서 성공적으로 덤프를 하기 위해서는 덤프를 하는 동안 DDL 쿼리를 해서는 안된다.\n—skip-column-names (in line 47) 말 그대로 결과에서 컬럼의 이름을 빼고 보여주는 옵션이다. 쉘스크립트에서 mysql의 결과 값을 인자로 받아 실행을 할 때 컬럼 이름이 들어가 있으면 결과를 식별하기가 어려워 —skip-column-names 를 사용한다.\n사용 예시\n1 2 3 4 5 6 # with column +----------------------+ | Database (mydb) | +----------------------+ | mydb | +----------------------+ 1 2 3 4 # without column +----------------------+ | mydb | +----------------------+ -e (in line 47) —execute 과 동일한 기능을 하는 옵션. 해당 옵션을 사용하면 mysql은 따옴표로 감싸준 쿼리(SQL statement)를 실행하고 값을 뱉어낸 다음에 종료한다.\nmysql에 접속해서 콘솔을 사용하면 어떤 쿼리도 실행을 시킬 수 있는 것처럼 -e 를 사용하면 mysql 커맨드 라인에서 원하는 쿼리를 실행할 수 있다.\n-e 사용 예시\n1 2 3 4 5 # 예시 1 RESULT=`mysql ${LOCAL_BASIC} --skip-column-names -e \u0026#34;show databases like \u0026#39;${PRE_DB}\u0026#39;\u0026#34; # 예시 2 mysql -uroot -p1234 -e \u0026#34;select * from test.user from age=20\u0026#34; 개선할 점과 참고자료 개선할 점 전체백업의 장점은 구현이 쉽다. 그런데 디비 속의 모든 데이터를 백업하기 때문에 시간이 오래 걸린다.\n그리고 만약 전체 백업이 제대로 진행이 안된다면 많은 양의 백업 데이터를 유실할 위험도 크다. 그래서 서비스에서는 전체백업 + 차등백업 / 전체백업 + 차등백업 + 증분백업 으로 여러 주기로 백업을 진행한다.\n라이브러리를 사용해서 DB 백업 하기 쉘스크립트로 짤 수 있는 전체백업과는 다르게 차등백업과 증분백업은 구현된 라이브러리를 써서 많이 진행한다.\n검색을 했을 때 많이 보였던 게 XtraBackup 인데 지원 문제가 있어서 요즘은 Mariabackup 을 사용한다. 나중에 좀 더 세밀한 백업을 위해 구조를 바꾸면 좋을 것 같아서 XtraBackup과 Mariabackup에 관련된 문서를 남겨둔다.\n[우아한-장애와 관련된 XtraBackup 적용기] [Mariabackup을 이용한 증분 백업] [XtraBackup에서 Mariabackup로 변경해야 하는 이유] 참고자료 [Mysqldump \u0026ndash;single-transaction option] [4.2.2.1 Using Options on the Command Line] [Storage Engine (InnoDB vs MyISAM)] ","date":"2021-09-09","permalink":"https://leeleelee3264.github.io/post/2021-09-09-linux-shell-db-backup/","tags":["Project"],"title":"[Project] Shell script로 MySQL DB full backup 구현하기"},{"content":"\nMySQL 백업에 대해 알아본다: Full backup, differential backup, incremental backup.\nIndex\n언제 DB 백업을 해야 하나? 백업 개념 MySQL의 백업 커맨드 Linux DB Migration 언제 DB 백업을 해야 하나? 백업의 용도 내가 실무에서 접할 수 있는 디비 백업의 용도는 2가지가 정도이다.\n(보관) 데이터 유실을 막기 위해 서비스에서 사용하는 디비 외의 다른 디비에 데이터를 저장하기 위해 (이전) 원래있던 디비를 버리고 새로운 디비를 사용하기 위해 2의 경위에는 디비가 바뀌니 이에 따라 코드나 명세등 여러가지를 바꿔줘야 하고, 데이터 양이 많으면 이전하는 작업 자체가 오래 걸리기 때문에 자주 일어나지 않는게 제일 좋지만, 서비스 초기와 서비스를 제공하면서 규모와 구조는 언제든지 바뀔 수 있기 때문에 항상 마음의 준비를 해두는 게 좋다.\n실무에서 백업 작업을 한 경험 서버와 DB가 하나의 호스트에 있을 때 문제점 앱/웹서버들과 디비서버를 하나에 몰아두는 것은 좋은 구조가 아니다. 한 곳에 몰아두면 관리는 쉬울 수 있겠지만 하나가 잘못되면 다른 부분까지 영향을 미쳐 결국 서비스를 마비시킬 수 있는 위험부담이 크다. 앱서버에 요청이 몰리거나, 대규모 디비 연산이 수행되면 양쪽이 정상작동 하지 않는 문제가 발행한다.\n대표적인 예가 지금 회사에서 사용하고 있는 개발 서버이다. 상용서버인 API는 서버와 아마존 RDS를 사용하는 디비과 완전히 분리가 되어있기 때문에 구조적 문제가 없다. 그러나 개발서버인 TPI는 디비가 TPI 서버 안에 들어가 있다.\n[Picture 1] 회사 서버 구조 문제 발생 개발서버다 보니 서버안에 여러가지 앱 서버들을 띄워두었는데 초기에 개발서버가 커질 것을 예상하지 못하고 AWS에서 CPU와 메모리 성능이 작은 인스턴스를 선택해서 서버로 띄웠다. 게다가 위에서 말한 것 처럼 개발에서 사용하는 디비또한 해당 서버 안에 띄워두었다. 앱서버의 갯수가 늘수록 서버의 자원은 부족해졌다.\n결국 어느 한 앱 서버에서 지나치게 CPU를 독점해서 연산을 진행하면서 다른 앱서버들과 디비가 멈췄고 심지어 서버 자체에 ssh로 접속하는 것도 불가능했다. 자원이 적은 서버에 이것저것 올려두는 것도 무리였지만 제일 큰 문제는 서버가 터지자 디비까지 작동을 멈췄던 점이다. 도미노처럼 다 멈춘 서버는 결국 재부팅을 해서 살렸다.\n문제 해결 이런 도미노 문제가 발생하다보니 하나에 뭉쳐있는 서버를 작게 쪼개는 수 밖에 없다. 서버를 3대를 띄워서 2대를 앱서버 컨테이너 서버로 사용하고 나머지 하나를 디비 컨테이너 서버로 사용한다. 상용 서버 처럼 RDS 디비를 사용하면 좋겠지만 RDS는 비싸기 때문에 선택하지 못했고 원래 목적인 서버와 디비의 분리는 이루어졌다!\n이렇게 나눠두면 앞의 두 컨테이너 서버에 요청이 심각한 수준까지 몰려서 서버가 다운이 된다고 해도 디비 컨테이너 서버에는 아무런 영향이 없고, 디비를 띄워둔 서버에는 다른 것들을 띄워두지 않고 정말 Mysql 만 띄워두기 때문에 서버에 과부하가 걸릴 확률이 크게 낮아진다.\n더 어려운 백업은? 디비 이전을 위한 백업은 사용하고 있던 디비 속의 데이터를 몽땅 꺼내와서 새로 만든 디비에 넘겨주기만 하면 끝이라서 비교적 쉬운 백업이다. 문제는 데이터 유실을 막기위해, 정말 백업의 용도로 이루어진 디비 백업이다. 디비를 통째로 복사해서 가지고 있을 것인지, 일정한 간격으로 백업을 진행해서 마지막 백업 이후에 이어서 백업을 할 것인지 등등 여러 컨셉의 백업이 있다.\n(plus) 문제 해결 중 느낀 점 이번 문제를 직면하면서 요즘 왜 MSA가 많이 선택을 받는지 이유를 조금이나마 알 것 같았다. MSA를 다룬 글에서는 에자일식으로 빠르게 개발이 가능한게 큰 장점이었는데 이렇게 잘게 나눠두면 서버의 오류가 멀리 퍼지는 것도 최소화 할 수 있는 것도 장점이라고 생각된다. 한군데에 몰려둔 TPI 형태의 아키텍처가 모놀리틱이고 모놀리틱보다는 분리가 되어있는 게 SOA (Service-Orient Architecture) 였다.\nSOA와 MSA의 차이는 여기서 더 자세히 볼 수 있다. [SOA vs MSA]\n[Picture 2] 여러가지 아키텍처 백업 개념 아래의 컨셉은 데이터베이스 뿐만 아니라 컴퓨터 전반에서 이루어지는 백업에서 통용되는 컨셉이다.\n전체 백업 (Full Backup) 차등 백업 (Differential Backup) 증분 백업 (Incremental Backup) 전체 백업 디비 이전을 위해 진행하는 백업은 전체 백업이다. 아무 조건 없이 모든 데이터를 덤프 뜨기 때문에 조건이 없어 백업을 진행하기 쉽지만 시간이 제일 오래 걸리고 파일 크기도 제일 크다.\n차등 백업 [Picture 3] 차등 백업 차등 백업은 전체 백업보다 조금 더 정교한 형태의 백업이다. 전체백업은 큰 규묘의 데이터를 백업하다보니 일주일에 한 번 정도 진행이 된다. 만약 저번주의 전체 백업은 잘 마쳤지만 이번주의 전체 백업이 정상적으로 진행이 안되었다면 많은 데이터를 유실하게 된다.\n이런 전체백업의 문제점을 커버하기 위한 차등 백업은 전체 백업 이후 추가된 데이터들만 백업을 하는 방식으로, 그림처럼 일요일에 전체 백업을 한다고 하면\n월: 월요일에 추가된 데이터\n화: 월요일 + 화요일에 추가된 데이터\n수: 월요일 + 화요일 + 수요일\n\u0026hellip;\n이런 식으로 다음 전체 백업이 있는 일요일 전까지 백업이 진행된다. (마지막 차등백업은 무시된다고 한다). 전체 백업보다는 정교하고, 데이터도 비교적 적기 떄문에 크기도 작고 작업 시간도 단축되었으나 다음 전체 백업 날이 다가올수록 파일 크기가 커진다. 그리고 변경된 사항들을 누적해가면서 저장하기 떄문에 하루에 한 번 이상 진행하기에는 무리가 있다.\n증분 백업 [Picture 4] 증분 백업 증분 백업은 차등백업보다 더 작은 규모의 백업을 진행한다. 차등백업이 전체백업 이후의 변경을 하루하루 누적한다면 증분백업은 딱 그날 바뀐 부분만 백업을 한다. 예를 들어 일요일날 전체백업을 진행했다면\n월: 월요일에 추가된 데이터\n화: 화요일에 추가된 데이터\n수: 수요일에 추가된 데이터\n\u0026hellip;\n이런 식으로 그날의 데이터만 저장을 한다. 때문에 크기가 가장 작고, 백업을 진행하는 속도도 가장 빠르다. 워낙에 작고 빠른 백업이 이루어지기 때문에 다른 백업들과는 다르게 한 시간에 한 번 씩 백업을 해도 무리가 없다!\n이렇게 보면 증분 백업이 제일 좋아보이지만 백업본이 날아가서 복구를 해야 하거나 할 때 시간이 오래 걸린다. 정교한 백업을 진행한 만큼 데이터를 다시 똑같이 쪼개서 백업을 해야 하기 때문에. 반면 전체백업은 똑같이 통으로 가져오기만 하면 되기 때문에 가장 빠르게 복구할 수 있다.\n표로 정리하는 백업의 장단점 [Picture 5] 백업의 장단점 MySQL의 백업 커맨드 백업 커맨드는 DB서버에 직접 들어가서 실행해야 한다. (DML/DDL/DCL 처럼 DB 콘솔창에서 실행 불가)\n백업하기\n1 2 3 4 5 6 7 8 9 \u0026lt;-- 아래에 나오는 # 는 리눅스 프롬프트 입니다. ex) root# --\u0026gt; \u0026lt;-- DB 백업 --\u0026gt; # mysqldump -u계졍 -p패스워드 복사할_DB_이름 \u0026gt; file_name.sql # mysqldump -uroot -p1234 mine \u0026gt; /etc/var/log/temp/my_backup.sql \u0026lt;-- Table 백업 --\u0026gt; # mysqldump -u계정 -p패스워드 DB_이름 Table_이름 \u0026gt; file_name.sql # mysqldump -uroot -p1234 mine mine_table \u0026gt; /etc/var/log/temp/my_table_backup.sql 복원하기\n1 2 3 4 5 6 7 \u0026lt;-- DB 복원 --\u0026gt; # mysql -u계정 -p패스워드 복원할_DB_이름 \u0026lt; file_name.sql # mysql -uroot -p1234 mine \u0026lt; /etc/var/log/temp/my_backup.sql \u0026lt;-- Table 복원 --\u0026gt; # mysql -u계정 -p패스워드 DB_이름 Table_이름 \u0026lt; file_name.sql # mysql -uroot -p1234 mine mine_table \u0026lt; /etc/var/log/temp/my_table_backup.sql Linux Mysql DB Migration 추후에 있을 DB 이전 작업을 위해 리눅스 환경에서 한 서버(old server)에서 다른 서버(new server)로 디비를 옮기는 방법을 추가로 알아봤다.\n선행 작업\nDB 백업 (위 항목의 DB 백업 Command 사용) 백업한 sql 파일을 FTP 등을 이용해 new server로 전달 new server에서 진행하는 작업\n1 2 3 4 5 6 \u0026lt;-- in mysql server --\u0026gt; mysql \u0026gt; create database db_name mysql \u0026gt; quit \u0026lt;-- in mysql server host --\u0026gt; # mysql -u계정 -p패스워드 db_name \u0026lt; file_name.sql ","date":"2021-07-21","permalink":"https://leeleelee3264.github.io/post/2021-07-21-mysql-backup/","tags":["Infra"],"title":"[Infra] MySQL Study (5/5) - Backup"},{"content":"\nMySQL join \u0026amp; join set에 대해서 알아본다: Inner join, left join, right join and full outer join.\nIndex\nHistory of Join Join Set Reference 리뷰 후에 알게된 부분들 비등가 조인. Oracle에서는 지원을 하는데 Mysql이 지원을 하는지 모르겠다. 일반 조인이랑 비등가 조인 쿼리를 직접 짜서 결과를 보고 비교해야 할 것 같다 [비등가 조인] 나중에 쿼리 추가하기. 테이블 조인 방식이 원래 for loop을 돌리는 건 알고있었지만 더 다양한 알고리즘이 있다. [테이블 조인 알고리즘] History of Join Join 테이블과 테이블을 묶어서 보여주는 Join 커맨드는 관계를 핵심으로 하는 RDBMS에서는 빼놓을 수 없는 기능이다. 한 테이블에 모든 정보를 넣어놨다면 Join을 쓸 필요가 없겠지만 중복을 피하기 위해 정규화를 거치고 데이터 유지보수를 위해 테이블을 쪼개다보면 Join을 꼭 써야 한다. 서브 쿼리를 이용해서 조인을 흉내낼 수 있다고 하는데, 서브쿼리 특성상 쿼리가 더 어렵고 지저분해지지 않을까?\nJoin? Inner Join?\nJoin과 Inner Join은 동일한 기능인데, 쿼리를 읽을 때 더 명확한 느낌을 주기 위해 Inner Join이라고 쓴다고 한다. 동일하게 Left Join과 Left Outer Join, Right Join과 Right Outer Join은 동일한 기능이다.\n지금은 이렇게 필수가 되어버린 Join이 사실은 sql이 만들어졌을 때 부터 있던 기능은 아니라고 한다.\nHistory of Join [Picture 1] history of join [Picture 1] 에서 보는 것 처럼 Join은 1992년에 만들어진 SQL-92 standard 에서 만들어졌다. 92는 Join뿐만 아니라 DDL인 ALTER과 DROP도 포함이 되어있고, 학교에서 제일 많이 가르치고 실무에서도 제일 많이 사용되고 있다.\nJoin이 없었을 때의 Join 방법 이처럼 Join이 없을때도 Join을 흉내내는 방법은 있었는데 From에 여러 테이블을 써주면 된다.\n예를 들어 A와 B 테이블을 합치려고 할 때 From 에 A와 B를 써주면 두 테이블을 합칠 때 나올 수 있는 모든 경우의 수들이 다 나오고, ON에서 A.id = B.a_id 하는 것처럼 WHERE 조건에서 A.id = B.a_id를 하면 Inner Join을 한 것과 동일한 결과가 나온다.\nfrom과 join 예시\n1 2 3 4 5 6 7 8 9 10 11 12 13 # Multiple Table In From SELECT * FROM orders O, users U WHERE user_id = U.id # Join SELECT * FROM orders O INNER JOIN users U ON O.user_id = U.id # 테이블 별칭을 사용했을 때 맨날 O.user_id, U.id 이런 식으로 모든 Column에 테이블 명시를 해줬는데 # 알고보니 한 쪽 테이블에만 존재하는 Column이면 따로 명시를 안 해줘도 된다! # 그런데 하나하나 명시를 해주는 쪽이 쿼리를 한 눈에 파악하기 더 용이해보인다. Where 조건을 걸지 않아서 orders와 users를 합쳤을 때 나오는 모든 경우의 수가 다 나왔다. 이걸 Cross Join 또는 Cartesian Join이라고 한다. Join연산은 항상 카티전 Join 과 동일하거나 작은 수의 결과를 리턴한다.\n[Picture 2] Cross Join Cross Join 결과\n더보기 id user_id cost id name number addr active 4 100 33 100 Peter 01012345678 대전광역시 Y 4 100 33 200 Lee 01087654321 경기도 N 4 100 33 300 Jamie 403926999 Calgary Alberta Canda N 5 100 11 100 Peter 01012345678 대전광역시 Y 5 100 11 200 Lee 01087654321 경기도 N 5 100 11 300 Jamie 403926999 Calgary Alberta Canda N 6 200 2 100 Peter 01012345678 대전광역시 Y 6 200 2 200 Lee 01087654321 경기도 N 6 200 2 300 Jamie 403926999 Calgary Alberta Canda N 7 200 100 100 Peter 01012345678 대전광역시 Y 7 200 100 200 Lee 01087654321 경기도 N 7 200 100 300 Jamie 403926999 Calgary Alberta Canda N 8 100 20 100 Peter 01012345678 대전광역시 Y 8 100 20 200 Lee 01087654321 경기도 N 8 100 20 300 Jamie 403926999 Calgary Alberta Canda N 9 400 200 100 Peter 01012345678 대전광역시 Y 9 400 200 200 Lee 01087654321 경기도 N 9 400 200 300 Jamie 403926999 Calgary Alberta Canda N Where 조건을 건 Join과 동일한 결과\nid user_id cost id name number addr active 4 100 33 100 Peter 01012345678 대전광역시 Y 5 100 11 100 Peter 01012345678 대전광역시 Y 6 200 2 200 Lee 01087654321 경기도 N 7 200 100 200 Lee 01087654321 경기도 N 8 100 20 100 Peter 01012345678 대전광역시 Y 그럼 Join을 안 써도 되나? 사실 Join과 Multiple Table In From의 퍼포먼스는 동일하다. 하지만 요즘은 모두가 Join을 쓰는 추세이고, Join을 사용해서 기본적으로 피해지는 오류들이 있으니 가능하면 Join을 사용하도록 하자.\nJoin의 장점 1 Join은 테이블을 합치기 위해서 사용하는 조건과 값을 뽑아내기 위한 조건을 분리할 수 있다.\nJoin의 테이블 결합 조건은 ON에 들어가기 때문에 WHERE에는 정말 값을 뽑아내기 위한 조건만 써주면 되기 때문에 복잡한 WHERE 조건이 들어간다고 해도 헷갈리지 않는다.\n여러 값을 뽑는 예시\n1 2 3 4 5 6 7 8 9 10 11 12 # 2021-07-13의 데이터를 뽑아낸다고 가정 #Join SELECT * FROM orders O INNER JOIN users U ON O.user_id = U.id WHERE O.sold_date = \u0026#39;2021-07-13\u0026#39; # Multiple Table In From SELECT * FROM orders O, users U WHERE O.user_id = U.id AND O.sold_date = \u0026#39;2021-07-13\u0026#39; Join의 장점 2 여러개의 테이블을 합칠 때 Join이 훨씬 용이하고, Multiple Table In From은 재앙이 된다.\n여러 개의 테이블 합치는 예시\n1 2 3 4 5 6 7 8 9 10 11 # Join SELECT * FROM orders O INNER JOIN users U ON O.user_id = U.id INNER JOIN point P ON O.id = P.order_id # Multiple Table In From # 테이블이 합쳐질때마다 무거워지는 WHERE 조건 SELECT * FROM orders O, users U , point P WHERE O.user_id = U.id AND O.id = P.order_id Join의 장점 3, 4 Left Join, Right Join, Full Outer Join의 지원으로 내가 원하는 형태로 테이블을 합치기가 더 좋다. 기본 Join과 마찬가지로 Multiple Table In From 를 사용해서는 원하는 결과를 뽑아내기 힘들거나, 더 복잡한 쿼리를 짜야 한다. Join을 사용하면 자연스럽게 Cross Join 문제를 피할 수 있다. Join Set 위에서 말한 것 처럼 Join은 여러가지 결합 형태를 지원하기 때문에 상황에 맞춰 테이블들을 조합해서 집합을 만들어 낼 수 있다. 검색을 해보니 정말 다양한 집합을 만들 수 있었는데 실무를 하면서 평소에 쓸 일이 제일 많았던 Join 형태 4개를 골라서 정리했다.\n예시로 사용할 데이터\nusers (사용자) id name number addr active 100 Peter 01012345678 대전광역시 Y 200 Lee 01087654321 경기도 N 300 Jamie 403926999 Calgary Alberta Canda N orders (주문) id user_id cost 4 100 33 5 100 11 6 200 2 7 200 100 8 100 20 9 400 200 위의 예시 데이터로 [Picture 3] 의 Join set을 다뤄보도록 하겠다.\n[Picture 3] Join Set Inner Join Join중에서 제일 많이 쓰는 기본형 JOIN.\n두 테이블에서 공통적으로 가지고 있는 데이터를 꺼내올 때 사용한다. table1 을 기준으로 table2에 있는 값이라고 해도 table1에 없으면 값이 누락된다. 이런때는 공통분모만 뽑아오는 Inner join이 아니라 커버 해주는 범위가 넓은 다른 JOIN 연산들을 써야 한다.\nLeft Join 왼쪽 테이블을 기준으로 데이터를 합치는 JOIN.\nJoin 연산에서 기준이 되는 테이블이 table1이다 보니까 쿼리를 짤 때도 데이터를 뽑아오는 중심이 되는 테이블을 table1 자리에 두는 것 같다. 그러다보니 table2의 데이터 존재 여부와 상관없이 일단 table1에 있는 데이터는 다 뽑아오는 Left Join을 Right Join 보다 많이 사용한다.\nLeft Join 예시\n1 2 3 SELECT * FROM users u LEFT JOIN orders o ON u.id = o.user_id; Left Join 결과\nid name number addr active id user_id cost 100 Peter 01012345678 대전광역시 Y 4 100 33 100 Peter 01012345678 대전광역시 Y 5 100 11 200 Lee 01087654321 경기도 N 6 200 2 200 Lee 01087654321 경기도 N 7 200 100 100 Peter 01012345678 대전광역시 Y 8 100 20 300 Jamie 403926999 Calgary Alberta Canda N \\0 \\0 \\0 Order에 300번 손님이 주문한 데이터가 없다고 해도 table1로 users 을 설정했기 때문에 결과에서도 300번을 보여준다.\nRight Join 오른쪽 테이블을 기준으로 데이터를 합치는 JOIN.\nLeft Join이 있는데 Right Join이 왜 필요할까? Left Join 하나만 있고 테이블 위치만 그때그때 바꿔주면 안되나 하는 의심이 있었다. 검색을 해보니 나랑 똑같은 생각을 한 사람이 있었는데 거기에 달린 답변을 보니 테이블 2개를 사용할 경우에는 테이블 순서를 바꿔가면서 Join연산을 하면 되는데 여러개의 테이블을 사용하면 그럴 수 없었다.\nRight Join 예시\n1 2 3 SELECT * FROM users u RIGHT JOIN orders o ON u.id = o.user_id; Right Join 결과\nid name number addr active id user_id cost 100 Peter 01012345678 대전광역시 Y 4 100 33 100 Peter 01012345678 대전광역시 Y 5 100 11 200 Lee 01087654321 경기도 N 6 200 2 200 Lee 01087654321 경기도 N 7 200 100 100 Peter 01012345678 대전광역시 Y 8 100 20 \\0 \\0 \\0 \\0 \\0 9 400 200 Left Join을 했을때와는 다르게, users에서만 존재하던 300번 유저에 대한 정보는 사라졌고 orders에만 존재하는 400번 유저에 대한 구매 내역이 나왔다.\nFull Outer Join Full Outer Join = 1 Time of Inner Join + Left Join + Right Join\n합집함을 위한 Join이다. 테이블 1과 테이블 2를 합칠 때 생기는 중복을 처리하고 돌려준 값이라고 생각하면 된다. 아쉽게도 Mysql 에서는 Full Outer Join을 지원하지 않는데 Left Join과 Right Join을 사용해서 똑같은 결과를 만들어낼 수 있다.\nMySQL Full Outer Join 예시\n1 2 3 4 5 6 7 8 9 SELECT * FROM users U LEFT JOIN orders O ON U.id = O.user_id UNION SELECT * FROM users U RIGHT JOIN orders O ON U.id = O.user_id MySQL Full Outer Join 결과\nid name number addr active id user_id cost 100 Peter 01012345678 대전광역시 Y 4 100 33 100 Peter 01012345678 대전광역시 Y 5 100 11 200 Lee 01087654321 경기도 N 6 200 2 200 Lee 01087654321 경기도 N 7 200 100 100 Peter 01012345678 대전광역시 Y 8 100 20 300 Jamie 403926999 Calgary Alberta Canda N \\0 \\0 \\0 \\0 \\0 \\0 \\0 \\0 9 400 200 재미있는 점은 Left Join과 Right Join을 했을 때 각각 Inner Join을 한 것 과 같은 교집합 값이 생기는데 UNION을 해서 중복을 제거했다는 점이다! 만약 UNION ALL을 사용했으면 중복이 그대로 포함된다.\nReference [Difference Between Join and Inner Join] [The History of SQL Standards] [What\u0026rsquo;s the Difference Between Having Multiple Tables in FROM and Using JOIN?] [Why do we have Left Join and Right Join in SQL, if we can use Left Join to get same result as of Right Join by just changing the position of tables?] ","date":"2021-07-14","permalink":"https://leeleelee3264.github.io/post/2021-07-14-mysql-history-of-join-and-join-set/","tags":["Infra"],"title":"[Infra] MySQL Study (4/5) - Join \u0026 Join Set"},{"content":"\nMySQL string function에 대해서 알아본다: LENGTH(), RIGHT(), MID(), INSERT() and referential integrity: Restrict, No Action, Cascade, Set NULL.\nIndex\nMySQL에서 String 함수 참조 무결정 소소한 하이디 SQL \u0026amp; markdown 팁 Reference 리뷰 후에 알게된 부분들 [Markdown 각주/미주 달기] 1 SUBSTRING_INDEX(문자열, 구분자, 가져올 구문 갯수) 1 2 3 4 SELECT SUBSTRING_INDEX(addr, \u0026#39; \u0026#39;, 2) FROM users; # 경기도 성남시 판교동 대왕판교로 --\u0026gt; 경기도 성남시 까지 나옴 도메인 무결성과 고유 무결성(Unique Column Values) 도메인 무결성은 만약 Gender를 표현 한다고 했을 때 남성/여성/무성 등 미리 정의가 되어있는 도메인 (convention같은 느낌)에 속한 값이 들어가야 한다. 고유 무결성은 row에 속해있는 특정한 column 들이 유니크한 고유의 값일때만 업데이트와 인서트를 허가한다는 뜻이다. UNIQUE key constraints이 이 무결성을 지키는 역할을 한다. A unique value rule defined on a column (or set of columns) allows the insert or update of a row only if it contains a unique value in that column (or set of columns) \u0026hellip; That is, no two rows of a table have duplicate values in a specified column or set of columns.\nMySQL에서 String 함수 [Picture 1] MySQL String 함수 1 [Picture 2] MySQL String 함수 2 Example LENGTH(), CHAR_LENGTH() LENGTH 는 바이트로 글자를 카운트하고 CHAR_LENGHT는 글자 자체의 길이를 카운트한다. 한글의 경우 1글자가 2바이트라서 제이미 세글자면 6바이트가 나올 것 같은데 DB에 설정된 utf8 charset은 한글을 3바이트로 저장하기 때문에 총 9바이트가 나왔다.\nLENGTH(), CHAR_LENGTH() 예제\n1 2 3 4 5 6 7 SELECT NAME, LENGTH(NAME), CHAR_LENGTH(NAME) FROM users # where 조건에도 사용이 가능해서 검색을 할 때 응용해서 사용하면 좋을 것 같다. SELECT NAME FROM users WHERE CHAR_LENGTH(NAME) \u0026gt; 3 LENGTH(), CHAR_LENGTH() 결과\nNAME LENGTH(NAME) CHAR_LENGTH(NAME) Peter 5 5 제이미 9 3 RIGHT(), LEFT(), MID() RIGHT(), LEFT(), MID() 는 파이썬 String 연산과 동일해서 놀랐다. 어떻게 활용을 할까 했는데 where로 검색을 하기에는 이미 더 편한 like가 다 커버를 해주고 있다. where절에 걸기보다는 select 절에 걸어서 간단하게 데이터를 조작하고 비식별화 하는 것에 사용하면 좋아보인다.\n예제를 작성하다보니 이래저래 간편하게 쓰기 좋아보이는 유틸성 함수같다. SUBSTRING()과 SUBSTR()도 비슷한 맥락에서 사용이 되면 좋을 것 같은데 이건 좀 더 정교한 조작을 할 때 이용!\nRIGHT(), LEFT(), MID() 예제\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 SELECT NAME FROM users WHERE LEFT(NAME, 1) = \u0026#39;이\u0026#39;; # like로 더 쉽게 찾을 수 있다. SELECT NAME FROM users WHERE NAME LIKE (\u0026#39;이%\u0026#39;); # concat()연산과 함께 사용해서 데이터 비식별화..? SELECT concat(left(NAME,1), \u0026#39;**\u0026#39;) FROM users WHERE LEFT(NAME, 1) = \u0026#39;이\u0026#39;; # Replace() 연산과 함께 사용해도 가능! SELECT replace(name, RIGHT(NAME, 2), \u0026#39;**\u0026#39;) FROM users WHERE LEFT(NAME, 1) = \u0026#39;이\u0026#39;; # 작정하고 만들면 이렇게 data format을 씌워줄 수 있는데 여러번 써야 한다면 엑셀에서 수정을 하거나 #프로그래밍으로 정규식을 입혀주는 게 더 간편하다는 생각이 든다. SELECT concat(left(number,3), \u0026#39;-\u0026#39;, MID(NUMBER, 4, 4), \u0026#39;-\u0026#39;, RIGHT(NUMBER, 4)) FROM users WHERE LEFT(NAME, 1) = \u0026#39;이\u0026#39;; RIGHT(), LEFT(), MID() 결과\nconcat(left(number,3), \u0026lsquo;-\u0026rsquo;, MID(NUMBER, 4, 4), \u0026lsquo;-\u0026rsquo;, RIGHT(NUMBER, 4)) number 010-8765-4321 01087654321 INSERT() INSERT() 예제\n1 2 3 4 5 6 SELECT INSERT(NAME, CHAR_LENGTH(NAME), 5, \u0026#39; 고객님\u0026#39;) FROM users # concat으로 해결 SELECT CONCAT(NAME, \u0026#39; 고객님\u0026#39;) FROM users; INSERT() 결과\nINSERT(NAME, CHAR_LENGTH(NAME), CHAR_LENGTH(NAME), \u0026rsquo; 고객님\u0026rsquo;) Pete 고객님 제이 고객님 Peter 고객님과 제이미 고객님을 뽑고 싶었으나 내 의도대로 나오지 않았다. 받는 인자가 INSERT(문자열, 시작위치, 길이, 새로운 문자열 순서인데 길이가 어떤 역할을 하는지 잘 모르겠다. 스트링을 조작할 때는 INSERT()로 하려 하지 말고 CONCAT() 으로 간단하게 끝내야 할 것 같다.\nLPAD(), RPAD() LPAD는 왼쪽으로, RPAD는 오른쪽으로 해당 길이 만큼 입력된 글자들을 채워주는 기능. 단순하게 보면 글자수를 맞춰서 채워주는 기능인데 화면에서나 문서에서 글자들이 모두 같은 글자수를 가지고 있어야 할 때 쓸 일이 있어보인다. RPAD(NAME, 10, ' ') 로 자릿수만 채워줄 수 도 있다.\nLPAD(), RPAD() 예제\n1 2 SELECT RPAD(NAME, 10, \u0026#39;*\u0026#39;), LPAD(NAME, 10, \u0026#39;\u0026amp;\u0026#39;) FROM users; LPAD(), RPAD() 결과\nRPAD(NAME, 10, \u0026lsquo;*\u0026rsquo;) LPAD(NAME, 10, \u0026lsquo;\u0026amp;\u0026rsquo;) Peter***** \u0026amp;\u0026amp;\u0026amp;\u0026amp;\u0026amp;Peter 제이미******* \u0026amp;\u0026amp;\u0026amp;\u0026amp;\u0026amp;\u0026amp;\u0026amp;제이미 PAD라는 단어가 익숙해서 어디서 들어봤나 했는데 저번 회차에서 주임님이 말씀하셨던 db엔진에서 글자수 비교를 할 때 CHAR형은 지정된 글자수를 맞춰주기 위해 그보다 짧은 단어들에 공백을 채워서 비교해준다는 PAD Attribute와 같은 기능이었다.\n[Picture 3] PAD의 뜻 참조 무결정 관계를 맺은 테이블들의 데이터 사이에서 일관성을 유지하기 위해 foreign key를 만들 때 참조무결성 제약조건이 만들어진다. 강의를 듣는 처음에는 참조무결성이 왜 필요할까? 왜 foreign key를 사용하지? 라는 의구심이 들었는데 데이터를 일관적으로 관리 한다는 성격 때문에 참조무결성을 사용하는 것 같다.\nforeign key의 특징 본테이블에 없는 데이터를 sub 테이블 (참조하는 테이블)에 넣을 수 없다. 이 특성이라면 프로그래밍적 오류를 db단에서 막을 수 도 있다. 본테이블에 없는 데이터로 sub의 foreign key를 수정할 수 없다. update와 delete에 cascade 옵션을 걸어놨을 경우 본테이블에 변경이 있으면 아무것도 안해도 sub 테이블에 알아서 반영을 해준다. 예를 들어 admin_user가 본테이블이고 admin_favorite이 sub 테이블이면 직원 중 하나가 퇴사를 하면 본테이블에 delete을 했을때 sub테이블에서도 자동으로 delete이 되어서 따로 관리를 할 필요가 없다. stateful 하지 않은 history 데이터도 foreign key 의미가 있을까? 외래키를 설정하는게 이렇게 1:1로 데이터가 stateful할 떄 (ex: 사용자-디바이스 정보) 유용하지 그냥 밑으로 확장하는 history성 테이블에서도 의미가 있을까? 또 의심을 했는데 일단 history성이여도 본테이블에 없는 데이터를 넣지 못하는 일관성이 보장이 되기 때문에 필요 자체가 없는 기능은 아닌 것 같다.\nforeign key action Restrict : 본테이블에 있는 데이터를 바꾸거나 삭제하려는데 다른테이블이 참조를 하고 있을 경우 action을 무시한다. No Action : mysql에서는 restrict 와 동일하다. Cascade : 본테이블에서 수정/삭제가 일어났을 때 참조하고 있는 테이블도 동일하게 수정/삭제가 적용된다. Set Null : 본테이블에서 수정/삭제가 일어났을 때 참조하고 있는 테이블의 Foreign Key를 NULL로 만든다. 수정에서 보다 삭제에서 더 유용한 설정 생각해 볼 부분 만약 데이터를 정말 지워버리는 Hard Delete말고 flag를 설정해줘서 지운 척을 하는 Soft Delete에서는 Foreign Key Delete Action을 어떻게 설정해주는 게 좋을까?\n소소한 하이디 SQL \u0026amp; markdown 팁 하이디 sql에서 격자행 내보내기 할 때 대다수의 경우 csv 파일로 내보내기를 했는데 wiki나 블로그 작성 용으로 결과를 마크다운으로도 내보낼 수 있는 걸 오늘 발견했다.\n이렇게 하면 [Table Generator] 에 가서 셀 하나하나를 만들거나 결과창을 캡처 해 올 필요가 없어서 굉장히 편리하게 포스팅을 할 수 있다.\n[Picture 4] 하이디 SQL 격자 행 내보내기 [Picture 5] 하이디 SQL 격자 행 내보내기 결과 Reference [Mysql String Function Official Document]\n[MySQL 문자열 함수]\n[MySQL RESTRICT and NO ACTION]\n[Mysql Foreign Key Action Official Document] 1: 주석에 관한 설명을 이곳에\u0026hellip;\n","date":"2021-07-08","permalink":"https://leeleelee3264.github.io/post/2021-07-08-mysql-stringfunction-and-refrential-integrity/","tags":["Infra"],"title":"[Infra] MySQL Study (3/5) - String Function \u0026 Referential Integrity"},{"content":"\nMySQL SQL \u0026amp; key \u0026amp; collation에 대해서 알아본다, SQL: DML, DDL, DCL. Key: primary, foreign, super, candidate, alternative, surrogate. Collation: utf8, utf8mb4.\nIndex\nStructured Query Language Key Collation Reference 리뷰 후에 알게된 부분들 Primary Key가 들어간 column 의 이름도 아닌 제약조건에 이름을 만드는 이유\n주임님이 주신 의견: 기본키를 항상 하나의 column으로 만드는 건 아니고 여러가지 키를 섞어서 만들었을 때 보기 편하게 하기 위해. Charset = Symbol (plain text) + encoding\nmysql 에서 지원하는 Collation을 쿼리할 때 나오는 pad-attribute란 무엇인가?\n가변형인 VARCHAR 말고 CHAR 형의 텍스트를 비교할 때 뒤에 공백을 넣어서 비교하는지 아닌지의 여부. 모든 DB가 그런것은 아니다. [Reference: MySQL에서 ‘a’ = ‘a ‘가 true로 평가된다?] 테이블에 걸린 제약조건을 확인하는 방법 (테이블 하나씩 검색하는 방법은 없고 sql 서버 전체에 걸려있는게 나와서 where 절로 뽑아내야 한다)\n1 2 3 4 5 select * from information_schema.table_constraints; # use where select * from information_schema.table_constraints WHERE CONSTRAINT_SCHEMA = \u0026#39;test\u0026#39; 외래키는 고유하게 식별이 가능한 데이터면 되기 때문에 꼭 primary key말고 unique key를 이용해서도 외래키를 만들 수 있다.\nForeign key should be made with primary key? Structured Query Language DML Data Manipulation Language\nSELECT, INSERT, DELETE, UPDATE 데이터를 조작하기 위한 SQL 커맨드 개발을 할 때 기본이 되는 CRUD (Create/Read/Update/Delete)이 해당된다. 응용프로그램으로 접근할 수 있는 유일한 SQL이다. DDL Data Definition Language\nCREATE, ALTER, RENAME, DROP, TRUNCATE 데이터베이스의 스키마를 정의/조작 하기 위한 SQL 커맨드 데이터베이스의 구조와 제약조건에 대한 전체적인 명세를 가지고 있는 메타성 데이터의 집합을 스키마라고 한다. DB에 가장 크리티컬하게 영향을 미치는 커맨드라 DBA나 DB 설계자가 자주 사용하는 커맨드 ALTER, RENAME 예시\n1 2 3 4 5 # ALTER 으로 테이블 이름 변경하기 ALTER TABLE old_name RENAME new_name; # RENAME 으로 테이블 이름 변경하기 RENAME TABLE old_name TO new_name; DROP과 TRUNCATE 예시\n1 2 3 4 5 6 7 8 9 10 11 12 # DROP으로 테이블을 지워버리기 DROP TABLE table_name; # TRUNCATE으로 테이블 초기화하기 TRUNCATE TABLE table_name; # DROP으로 테이블 초기화하기 DROP TABLE table_name; CREATE TABLE table_name ( field_name field_type ... ) Drop은 테이블 자체를 데이터베이스에서 지워버리는데 Truncate은 테이블의 컬럼, 제약조건들은 남겨두고 데이터만 지워준다. 조건을 이용해서 레코드 하나씩 지워가는 Delete 보다 Truncate이 실행속도는 더 빠르다고 한다.\nDDL인데도 불구하고 응용프로그램에서도 Truncate을 사용하는 경우가 있다고 하는데 응용프로그램에서 테이블의 데이터를 모두 지워버리는 행위는 너무 위험하다고 생각한다.\nDCL Date Control Language\nCOMMIT, ROLLBACK, GRANT, REVOKE DB의 권한과 무결성을 지키기 휘한 Transaction을 위한 SQL 커맨드 COMMIT과 ROLLBACK을 따로 빼서 TCL (Transaction Control Language)라고 하기도 한다. Transaction은 커맨드를 테스트 할 때 데이터의 변경 없이 테스트를 하고 싶을 때 활용할 수 있다. TRANSACTION 예시\n1 2 3 4 5 6 7 START TRANSACTION; SELECT * FROM users; DELETE FROM users WHERE NAME = \u0026#39;jamie\u0026#39;; SELECT * FROM users; ROLLBACK; 이런식으로 Transaction을 걸면 각 줄이 실행되면서 SELECT로 결과를 보여주고 Rollback 커맨드로인해 테스트를 하기 전의 상태로 데이터베이스가 돌아간다. 일반 SELECT가 아닌 조건이 복잡한 DELETE, UPDATE, INSERT 쿼리를 사용해야 할 때 쓰면 좋아보인다.\n그런데 저렇게 짠 쿼리 중에 오류가 있으면 마지막 ROLLBACK까지 오지가 않기 때문에 Transaction 처리가 안된다. 아무리 Transaction을 걸었다고 해도 조심을 해야할 필요가 있어보인다.\n[Picture 1] Transaction 실패 Key Key와 제약조건의 차이 Key 키는 테이블의 단일 필드에 걸거나 복합적인 필드에 걸 수 있다. 테이블에서 조건에 따라 데이터를 가져오기 위해 사용되며 다른 테이블과 뷰 사이의 관계 또한 생성을 할 수 있다. 내가 생각했던 것 처럼 키 자체를 제약조건이라고 보기는 힘들다 . 제약조건 제약조건은 데이블의 데이터를 위한 특정한 규칙들이다. 만약 data action(SQL command로 데이터를 변경하기 위한 action들)이 제약조건을 위배한다면 그 data action을 취소해버린다. 제약조건은 테이블을 생성할 시기나, 생성한 이후에 걸 수 있다. DBMS에서의 Key와 MySQL에서의 키 DBMS의 키와 Mysql에서 사용하는 키의 차이점은 DBMS에서 말하는 Key들은 Concept에 가깝다. Mysql은 실제로 Primary Key와 Foreign Key 기능을 사용할 수 있고 나머지 Key들은 데이터베이스를 설계할 때 고려가 되는 개념들이었다.\nDBMS Primary Key 테이블 안에서 중복되지 않은 값을 사용해서 데이터들(row)을 고유하게 식별한다. 테이블 안에서 Primary Key 는 하나밖에 있을 수 없다. NULL은 absence of value 기 때문에 값이라고 할 수 없다. 그래서 Primary Key에 NULL값을 넣을 수 없다. 물리적인 인덱스를 생성하는데 값이 비어있으면 순서를 정할 수 없어서 NULL값을 넣을 수 없다. Primary Key = Unique Constraint + NOT NULL Foreign Key가 걸려있으면 Primary Key를 변경할 때 이를 참조하고 있는 Foreign Key까지 함께 변경해줘야 한다. 이떄 ON UPDATE CASCADE 기능을 사용한다. Foreign Key 테이블들 사이에 관계를 만들고, 참조 무결성을 유지하는데 사용된다. 테이블을 연결시켜 주기 때문에 어떤 데이터인지 고유하게 식별이 가능해야 해서 Foreign Key를 만들기 위해서는 Primary Key를 사용해야 한다. NULL 값이 들어갈 수 있다. Foreign Key로 이미 관계는 만들어졌으나 참조하는 테이블에서 Primary Key를 비롯한 데이터가 아직 만들어지지 않았을 수 있기 때문이다,. Super Key 테이블에서 데이터를 식별할 수 있게 해주는 Key 또는 Key Set 식별만 가능하면 되기 때문에 여러가지 Column들을 조합해서 여러개의 Super Key 를 만들 수 있다. Primary Key도 Super Key에 들어있는 Key 중 하나이다. Candidate Key Super Key와 동일한 성격을 가지고 있는 Key 데이터를 식별해주나 최소한의 Column을 사용해야 한다. (Minimal Super Key) Primary Key도 Candidate Key 에서 뽑힌다. [Picture 2] Key 다이어그램 Super Key와 Candidate Key 예시\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 # Example Of Super Key and Candidate Key # 전화번호에 중복이 없다는 전제 | Id | Number | Name | |----|---------------|----------| | 1 | 111-1111-1111 | Seungmin | | 2 | 222-2222-2222 | Jamie | | 3 | 333-3333-3333 | LeeLee | # Super Key {Id} {Number} {Id, Number} {Id, Name} {Id, Number, Name} {Number, Name} # Candidate Key {Id} {Number} Alternate Key Primary Key로 선정되지 않은 Key들 Compound Key 와 Composite Key 여러개의 Column을 묶어서 테이블에서 고유하게 식별가능하게 만든 복합키 Compound Key는 Column 중에 Foreign Key 를 가질 수 있다. Composite Key는 Column 중에 Foreign Key 를 가질 수 도 있고 가지고 있지 않을 수 도 있다. Compound Key와 Composite Key를 구별하는 게 특별이 중요하지 않아서 자주 바꿔서 부른다고 한다. Surrogate Key 각 데이터들을 고유하게 식별하기 위한 대리로 만들어진 키 데이터 자체에는 아무런 의미도 없고, 정보도 없다. Primary Key 로 자주 사용되는 Auto increment 가 걸린 Id가 여기에 해당된다. Id 자체에는 아무런 뜻이 없으나 서비스 에서는 유저들을 식별하는 정보로 사용된다. Natural Key라고 반대 개념이 있는데 이것은 정보 자체를 사용해서 데이터들을 식별한다. 예를 들어 유저를 식별하는 값으로 주민등록번호 사용. Surrogate Key VS Natural Key Surrogate Key와 Natural Key 사이에서 Primary Key 를 고르는 문제가 빈번히 대두되는데 아무리 고유한 값이라도 일말의 바뀔 여지가 있는 Natural Key 보다는 (주민등록 번호 변경 등) 비즈니스 안에서는 절대 변할 수 없는 Surrogate Key 를 Primary Key 로 등록하는게 좋다.\nMySql Key Key를 만들면 Index가 만들어진다. 결국 일반 Key는 Index와 동일하다. Primary Key Unique Key Foreign Key Collation Charset: 글자들을 어느정도 되는 크기에 집어 넣을 것인지 Collation: 텍스트들을 어떻게 정렬할 것인지 MySQL에서 지원하는 Collation utf8과 utf8mb4의 Charset으로만 이루어진 Collation도 67개나 지원을 하고 있다. Mysql에서는 총 322개의 Collation을 지원하고 있다.\nMySQL의 Collation을 확인하는 쿼리 예시\n1 2 3 4 5 SHOW COLLATION # 제일 자주 사용하는 charset인 utf를 지원하는 Collation SHOW COLLATIOn WHERE CHARSET LIKE (\u0026#39;%utf8%\u0026#39;) [Picture 3] SQL Charset UTF charset 한글을 사용하려면 utf8을 사용하면 된다. 웹에서는 자연스럽게 utf8로만 설정을 하면 되지만, mysql에서는 utf8, utf8mb4, utf16, utf32 로 다양한 utf를 지원한다.\n나는 charset 크기가 점점 커져서 utf8 → utf8mb4 → utf16 → utf32 이런 순서로 만들어진 줄 알았는데 그렇지는 않았다. 사용하는 공간이 큰 utf16과 utf32보다는 utf8과 utf8mb4가 많이 쓰인다.\nutf8mb4? utf8에서 utf8mb4가 생겨난 이유는 이모지들을 저장하기 위해서다. 원래대로라면 utf8은 8bits, 즉 1byte만 들어갈 것 같지만 사실 utf8은 1~4byte 까지 사용할 수 있는 가변크기를 가지고 있다. 그래서 plain text인 영어와 한국어는 각각 1byte, 3byte 크기를 이용해서 저장이 된다.\nMysql을 설계할때도 utf8형이 3byte를 넘어갈 일이 없다고 생각을 했어서인지 utf8이 실제로 저장할 수 있는 공간을 3byte로 제한을 해버렸다. 그런데 한자나 이모지들은 길이가 4byte인 문자열들이라서 Mysql에서 utf8을 쓰다가는 데이터가 다 들어가지를 않는다.\n그래서 원래의 utf8의 취지에 맞게 1에서 4byte 까지 가변적으로 크기를 가질 수 있는 charset인 utf8mb4를 만들었다. 그리고 대부분의 경우 utf8mb4 Charset을 사용한다.\n자주 사용하는 Collation 실제 서비스에서는 텍스트를 관리하는 Column은 대부분 utf8mb4_unicode_ci를 사용하고, 옛날에 실수로 잘못 설정을 했을 때 utf8mb4_general_ci를 사용했다. 여기서 ci란 Case Insensitive 의 약자로 대소문자를 구별하지 않겠다는 뜻이다.\nInsensitive Charset 예시\n1 2 3 4 5 6 7 8 9 10 # use case insensitive charset SELECT * FROM users WHERE name = \u0026#39;Jamie\u0026#39;; SELECT * FROM users WHERE name = \u0026#39;jamie\u0026#39;; # 대소문자를 구별하지 않기 때문에 SELECT의 결과는 동일하다. utf8mb4_bin 바이너리 값을 그대로 저장한다. bin은 binary의 약자이다. utf8mb4를 사용하지만 텍스트가 아닌 바이너리값이 중심이라 파일이나 이미지를 저장하는 Column에 사용하면 된다. utf8mb4_unicode_ci Unicode 규칙을 따라서 정렬이 되었고 언어들 사이에서 제일 많이 선택받는 Collation. 특별한 기호들을 사용할때도 unicode_ci를 사용하는 편이라고 한다. utf8mb4_general_ci 텍스트 형태로 정렬을 해준다. 한국어, 영어, 중국어, 일본어는 general_ci와 unicode_ci collation의 결과값이 동일하다. general_ci는 속도개선을 중점으로 하고 있기 때문에 정렬과 비교를 할 때 공식적인 Unicode 규칙을 따르지 않고 내부에서 따로 디자인을 했다. 그래서 때에 따라서 우리가 기대하는 (Unicode 규칙에 따른) 값이 나오지 않을 수 있다. 요즘은 CPU의 성능이 많이 좋아져서 Unicode 규칙을 따르지 않는 general_ci를 써가면서 성능향상을 할 필요가 없다! Reference [Difference Between Key and Constraint] [Difference Between Composite and Compound Key] [Primary Key - Surrogate Key VS Natural Key] [deep dive into utf8 and utf16] [Difference Between unicode_ci and general_ci] ","date":"2021-06-30","permalink":"https://leeleelee3264.github.io/post/2021-06-30-mysql-sql-and-key-collection/","tags":["Infra"],"title":"[Infra] MySQL Study (2/5) - SQL \u0026 key \u0026 collation"},{"content":"\nMySQL Index에 대해서 알아본다: clustered, non-clustered structure and algorithm. B-Tree, R-Tree. Side effect of index.\nIndex\nClustered Index / Non-Clustered Index - Concept Clustered Index / Non-Clustered Index - Structure and Index algorithm Side effects of Indexes Related SQL Command with Indexes Reference 리뷰 후에 알게된 부분들 Non-Clustered Index가 여러 개 생성이 가능하지만 무제한으로 만들 수 있는 것은 아니다. 조회는 Clustered Index가 빠르지만, 수정과 입력은 Non-Clustered Index가 빠르다. 왜냐하면 Non-Clustered Index는 물리적인 정렬을 가지고 있지 않아서 새 데이터가 들어와도 순서대로 정렬을 안 해도 된다. Clustered Index / Non-Clustered Index - Concept Clustered Index와 Non-Clustered Index 둘 다 테이블에 있는 데이터 엑세스를 빠르게 하기 위한 용도이지만 둘의 성격이 조금 다르다.\n가장 큰 차이는 Clustered Index는 테이블 당 딱 1개만 만들어지고, Non-Clustered Index는 복수개가 만들어 질 수 있다는 점과 Clustered Index를 생성하는 Column은 중복을 허용하지 않지만 Non-Clustered Index는 중복을 허용한다는 점이라고 생각한다.\nClustered Index Primary Key 로 만들어진다. 테이블에 단 하나만 만들 수 있다. 중복된 데이터를 허용하지 않는다. 테이블에서 데이터를 순서대로 저장하게 해준다. 크기가 큰 테이블에서 데이터를 빠르게 찾아오게 하기 위해서 꼭 만들어주는 게 좋다. 속도가 Non-Clustered Index 보다 빠르다. Non-Clustered Index Unique Key와 Key 로 만들어 진다. Mysql의 일반 Key는 인덱스를 생성하는 역할만 수행한다. Unique Key의 특성상 해당 키로 인덱스를 생성했다면 중복값이 있을 수 없다. 그래서 Unique Key의 Index를 Unique Non-Clustered Index라고 하기도 한다. 테이블에 여러개를 만들 수 있다. 중복된 데이터를 허용한다. 데이터 저장에 아무런 영향을 미치지 않는다. 과도한 Non-Clustered Index 생성은 퍼포먼스를 떨어뜨리기 때문에 잘 설계해서 꼭 필요한 부분만 만들어야 한다. 속도가 Clustered Index 보다 느리다. Clustered Index / Non-Clustered Index - Structure and algorithm Clustered Index Structure 트리 구조로 만들어진다. (B-tree) 각 노드에는 실제 데이터가 저장된다. 실 데이터를 가지고 있기 때문에 IO 작업이 적어 데이터 찾기가 훨씬 빠르다. (1)속도가 빠르고 (2)모든 Column을 필요로 하는 읽기 전용 어플리케이션이라면 Clustered Index를 만들면 된다. [Picture 1] Cluster structure Non-Clustered Index Structure 트리 구조로 만들어진다. (B-tree) 각 노드에는 포인터 (주소)가 저장되어 있고, 이 주소들은 Clustered Index의 노드들을 가르키고 있다. 인덱스 안에 데이터들은 논리적으로 순서를 가지고 있기 때문에 물리적으로는 다른 순서를 가지고 있을 수 있다. Index에 해당되는 Column에 따라 논리적으로 정렬이 되어있다. Non-Clustered Index를 사용해 검색을 했을 경우 결국 Clustered Index까지 거쳐서 결과가 나오게 된다. Clustered Index가 없는 테이블의 Non-Clustered Index는 Heap을 가르킨다. Heap in MySQL Clustered Index를 설정하지 않은 테이블을 Heap이라고 한다. Heap은 순서없이 저장된 데이터들을 가지고 있다. Heap은 순서를 신경쓰지 않아 크기가 크고, 순서가 중요하지 않은 데이터들의 Insert 연산이 필요한 테이블에 사용된다. [Picture 2] Non-cluster structure Index algorithm B-Tree O(logN)의 시간 복잡도를 가진다. 하나의 노드에 여러가지 데이터를 가질 수 있고, 때문에 참조 포인터가 적어 빠른 메모리 접근이 가능하다. 탐색 뿐 아니라 수정과 저장에서도 O(logN)의 시간복잡도를 가진다. B-Tree를 쓰는 이유 시간복잡도가 O(1)인 Hash 대신에 O(logN) 시간이 걸리는 B-tree를 사용하는 이유는 Hash는 Key값을 가지고 단 하나의 데이터에 엑세스 하기 때문이다. 그래서 연산에서 동일한 값을 찾아내는 = 만 사용이 가능하고, 대소비교를 하거나 Between 을 사용할 수 없게 된다. 데이터 엑세스가 빠른 배열을 사용하지 않고 포인터로 분산되어있는 B-Tree를 쓰는 이유는 배열은 탐색을 할 때만 빠르고 다른 연산에 대해서는 B-Tree보다 더 느리기 때문이다. [Picture 3] B-Tree index algorithm R-Tree 공간 정보 탐색을 위한 알고리즘 (기하학) Mysql에서 공간정보를 관리하는 타입인 Geometry와 하위 타입들에서 쓰인다고 한다. [R-Tree 알고리즘] Side effects of Indexes Clustered Index가 있으면 데이터를 Insert/update 할 때 마다 순서대로 다시 정렬을 해줘야 하기 때문에 속도가 느리다. Non-Clustered Index는 디스크에 분산이 되어 저장이 되는데 인덱스를 만들 때 Column 값이 중복으로 저장이 되기 때문에 디스크 낭비가 발생할 수 있다. Index를 너무 많이 만들어두면 오히려 탐색 성능까지 떨어지게 된다. 탐색 성능 저하 시나리오\nINDEX1는 A와 B Column을 가지고 있고, INDEX2는 A와 C Column을 가지고 있다. 이런 경우 디스크에는 B가 2 번 기록이 된다. 이런 상태에서 B와 C를 조회한다면 INDEX 1,2 를 둘 다 사용해서 포인터를 찾고, 테이블에 있는 데이터를 찾기 시작한다.\nRelated SQL Command with Indexes 인덱스와 관련된 Command는 DML이 아니라 DDL (Date Define Language) 다!\n인덱스 쿼리 예시\n1 2 3 4 5 6 7 8 9 10 11 12 13 # 생성되어있는 인덱스 확인 SHOW INDEX FROM table_name # 인덱스 추가 ALTER TABLE table_name ADD INDEX index_name (column1, column2); # 인덱스 삭제 ALTER TABLE table_name DROP INDEX index_name; # 인덱스를 타는지 안 타는지 실행계획 보기 EXPLAIN query ex) EXPLAIN SELECT * FROM table_name EXPLAIN 쿼리 EXPLAIN 쿼리를 사용하면 작성한 쿼리의 성능과 index 사용 여부 등을 자세히 알 수 있다.\n[Picture 4] EXPLAIN 쿼리 예시 Id SELECT 문의 순서 서브 쿼리가 있을 경우 순서를 부여해준다. select_type SELECT 문의 유형 SIMPLE: UNION이나 서브 쿼리를 사용하지 않은 SELECT PRIMARY: 서브 쿼리를 사용했을 때 제일 바깥에 있는 SELECT SUBQUERY: 서브쿼리 SELECT DERIVED: FROM 절에 서브쿼리가 있는 SELECT type SELECT 문을 실행하기 위해 테이블을 어떻게 조회했는지를 나타낸다. ALL: table full scan, 테이블을 처음부터 끝까지 조회했다. (제일 나쁨) INDEX: index full scan, 인덱스를 처음부터 끝까지 조회했다. RANGE: 특정 범위 안에서 인덱스를 사용해서 조회했다. INDEX_MERGE: 두 개의 인덱스를 병합해서 조회했다. REF: 조인을 할 때 Primary 나 Unique 가 아닌 Key로 매칭해서 조회했다. CONST: 매칭되는 row가 단 한 건이며, Primary 나 Unique를 사용해서 조회했다. possible_keys 해당 Column을 찾기 위해 사용된 인덱스 이 값이 NULL이라면 사용된 인덱스가 없는 것이다. (이걸 보고 인덱스를 타게 수정이 가능하다) key 최적화를 위해 Mysql 옵티마이저가 사용하기로 결정한 인덱스 ref 데이터를 추출하기 위해 키와 함께 사용된 컬럼 또는 상수 rows 쿼리를 수행하기 위해 검색해야 할 Row의 갯수 Extra Mysql 옵티마이저가 추가로 해석한 정보. using index : 인덱스를 이용해 자료 추출 using where : where 조건으로 데이터를 추출, 만약 type이 NULL인데 이 값이 나왔다면 쿼리 성능이 좋지 않다는 뜻 using temporary: 쿼리 안에서 임시 테이블을 생성/사용 using filesort: 데이터 정렬 연산이 포함됨. Reference [Clustered Vs Non Clustered Index] [데이터베이스 인덱스는 왜 B-Tree를 선택하였는가] [R-tree 알고리즘] [Mysql Explain 실행계획 사용법 및 분석] ","date":"2021-06-21","permalink":"https://leeleelee3264.github.io/post/2021-06-21-mysql-index/","tags":["Infra"],"title":"[Infra] MySQL Study (1/5) - Index"},{"content":"\nUbuntu 설치와 초기 설정 방법에 대해 알아본다. From basic: installation USB, network, static ip, SSH, user and sudoer to custom: prompt, build-essential, installation Maria DB.\nIndex\n리눅스 설치 초기 네트워크 설정 SSH 설정 사용자 설정 기타 설정 MariDB 설치/설정 리눅스 설치 리눅스 설치용 usb 리눅스를 설치하려면 설치용 usb를 먼저 만들어야 한다. [리눅스 usb 만들기] 이 블로그에 나오는 포스팅을 참고해서 Rufus 로 우분투 20 server 용 부팅 usb 를 만들었다.\n부팅용 usb가 만들어지면 다른 운영체제를 설치할때와 마찬가지로 부팅시에 boot configuration으로 들어가는 버튼을 열심히 눌러서 usb drive를 선택해서 부팅하면 리눅스 설치는 끝이 난다.\n리눅스 server VS desktop 사실 이전에 한가지 실수를 했는데 우분투 20 desktop 부팅 usb를 만들어서 서버를 설치했다. 노트북을 닫을때 운영체제에서 일어나는 동작을 컨트롤 할 수 있기 때문에 닫는 동작을 무시하면 서버가 계속 켜저있을 거라고 생각했는데 1주일 정도 노트북을 닫고 서버를 돌리니 서버가 다운되었다. 찾아보니 우분투를 서버로 돌릴 목적이라면 초반부터 꼭! 우분투 서버용을 사용해야 했다.\n[노트북 덮어도 서버 유지] 이 포스팅에 노트북 닫을 때 우분투의 작동 제어 방법이 나와있다.\n리눅스 설치 노트북 리눅스를 설치할 노트북은 dell xps 14 2012 모델로 InsydeH20 bios를 사용하고 있다. 처음에는 이 bios 설정 사항들을 잘 몰라서 시간을 많이 썼는데 이 [블로그] 를 보면서 따라할 수 있었다.\n초기 네트워크 설정 포스팅에서는 네트워크를 와이파이로 연결했는데, 네트워크 속도가 너무 느려 나중에는 결국 이더넷 연결을 했다.\n와이파이 연결 집에서 KT 와이파이를 쓰기 때문에 우분투 서버에서도 와이파이에 연결하려고 했는데 초기에는 서버를 설치하고 나서는 랜선에 우선 연결을 해야 한다.\n와이파이에 연결하려면 몇개의 패키지를 받아야 하는데 네트워크가 연결되지 않은 상태로는 다운을 받을 수 없다\u0026hellip; 우분투 데스크톱에는 기본으로 깔려있으나 우분투 서버에서는 직접 다운 받아줘야 한다. 여러가지 포스팅을 찾아보다가 [wpa로 와이파이 연결] 보고 그대로 따라하며 와이파이 연결에 성공했다.\nwpa 실행\n1 sudo wpa_supplicant -c /etc/wpa_supplicant.conf -i wlp4s0 이 커맨드에서 \u0026amp;을 사용해서 백그라운드로 돌리려고 했는데 아무리 해도 백그라운드로 실행이 안 되어서 ctrl + alt + f1 ~f6 으로 전환할 수 있는 터미널 중 하나에서 저 커맨드를 실행하고 다른 작업들은 다른 터미널에서 진행했다.\n그런데 포스트를 쓰고 있는 지금 보니까 wpa_supplicant에서 백그라운드를 지원하는 커맨드가 따로 있었다. \u0026amp;로 직접 백그라운드로 돌리는게 아니었다. 아래와 같은 커맨드를 사용하면 된다.\nwpa 백그라운드 실행\n1 sudo wpa_supplicant -B -c /etc/wpa_supplicant.conf -i wlp4s0 고정 ip 설정하기 이더넷이나 와이파이를 연결해서 사용할때면 서버가 설치된 컴퓨터를 종료했다가 다시 키면 할당된 ip주소가 바뀔때가 있다. 얼마전에 회사에서 정전이 되어서 내부에서 사용하던 서버 컴퓨터 전원이 나갔다가 다시 들어왔는데 ssh로 접속이 안되어서 컴퓨터에 화면을 연결해서 직접 보니 IP 주소가 바뀌어있었다. 이런 불상사를 피하고 싶다면 고정 ip를 할당해주면 된다.\n매번 이더넷으로만 고정 ip를 설정했는데 와이파이도 큰 차이 없이 아래와 같이 하면 된다. 아래에 작성된 파일은 netplan에서 사용하는 yaml 파일인데 수정하고 적용하려면 꼭 netplan apply 커맨드를 입력해주자. 그리고 netplan을 사용하는 우분투 버전은 18 부터다.\n고정 ip를 위한 netplan 파일\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # This is the network config written by \u0026#39;subiquity\u0026#39; network: ethernets: enp7s0: dhcp4: true version: 2 wifis: wlp8s0: dhcp4: no dhcp6: no addresses: [ 172.30.1.100/24 ] gateway4: 172.30.1.254 nameservers: addresses: [ 8.8.8.8, 1.1.1.1 ] access-points: \u0026#34;KT_GiGA_2G_FO1F\u0026#34;: password: \u0026#34;wifi_password\u0026#34; SSH 설정 사실 서버 컴퓨터는 직접 작동을 시킬 일이 거의 없다. 대부분 ssh를 이용해서 외부에서 서버 컴퓨터 속의 서버에 접속을 한다. 이제 ssh 접속을 위한 세팅과 ssh 로 접속할 유저를 위한 세팅을 해보자.\nSSH server 설치\n1 2 3 4 5 6 # 설치 sudo apt-get install openssh-server # ssh port open sudo ufw enable sudo ufw allow 22 설정파일에 ssh_config와 sshd_config 두개가 있는데 대부분의 조작은 sshd_config에서 바꿔줘야한다. 리눅스 네임컨밴션을 생각해보면 sshd_config는 ssh가 데몬프로세스로 백그라운드 실행이 될때를 컨트롤 하는 파일이 아닐까?\n비밀번호로 SSH 접속 이번에는 서버 직접 설치라서 상관이 없는데 만약 aws와 같은 클라우드에서 서버를 만들어서 구동한다면 pem 키를 발급해준다. 문제는 이 pem키를 한 번 발급해주기 때문에 여러명이 해당 서버에 접속하려면 불편하다. 한명이 pem키로 서버에 접속을 하고 ssh를 설치해서 key 뿐만 아니라 비밀번호로도 접속을 할 수 있게 만들어줘야 한다.\nsshd_config 파일\n1 2 # 비밀번호 접속허용 PasswordAuthentication yes 사용자 설정 ssh를 설정 완료 했다면 ssh를 이용해서 로그인할 사용자와 그룹을 만들어주자. 앞으로 ssh를 사용해서 로그인을 하면 이 사용자가 되어 서버를 돌아다니기 때문에 꼭 sudoer까지 설정을 해줘야 한다. 접속할때 아예 해당 유저를 넣어서 접속을 한다. ssh user@address\n그룹과 사용자 생성 그룹, 사용자 생성\n1 2 3 4 5 6 7 8 9 10 11 # make group sudo addgroup \u0026#34;group_name\u0026#34; # make user sudo adduser \u0026#34;user_name\u0026#34; # put the user in the group gpasswd -a \u0026#34;user_name\u0026#34; \u0026#34;group_name\u0026#34; # check the user is now in the group getent group \u0026#34;group_name\u0026#34; 참고로 서버 안에 있는 그룹들과 유저들을 확인하기 위해서는 /etc에 있는 group과 passwd 파일을 직접 조회하는데 아래와 같은 커맨드를 쓰면 좀 더 깔끔하게 볼 수 있다.\n그룹, 사용자 조회\n1 2 3 4 5 # group list cut -d : -f1 /etc/group # user list cut -d : -f1 /etc/passwd 사용자에게 권한 부여해주기 딱 두가지 권한이 필요하다.\n서버에서 sudo 커맨드를 쓰기 위해 sudoer 권한이 필요하고, 소스코드나 실행파일등 개발한 것들을 /opt 파일에 두고 조작을 하기 때문에 소유가 root로 되어있는 opt디렉터리를 사용자로 바꿔줘야 한다. 안 바꾸면 매번 디렉터리 안에서 뭘 조작하려면 root 권한이 필요해서 sudo를 사용해야 한다.\netc 디렉터리의 sudoers 파일 수정을 통해 사용자에게 권한을 부여해줄 수 있다.\nsudoers 파일\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 sudo vi /etc/sudoers # User privilege specification root ALL=(ALL:ALL) ALL dev ALL=(ALL:ALL) ALL # 아래처럼 하면 sudo 실행시 비밀번호를 안 넣어도 된다. dev ALL=(ALL:ALL) NOPASSWD: ALL # 디렉터리 소유권 바꾸기 (사용자) sudo chown \u0026#34;user_name\u0026#34; opt # 디렉터리 소유권 바꾸기 (그룹) sudo chown :\u0026#34;group_name\u0026#34; opt 기타 설정 프롬프트 개인화 처음 서버를 파면 프롬프트가 보기가 다소 불편하다. 이것도 입맛에 맞게 바꿀 수 있는데 사용자 설정파일인 .bashrc안에 있는 PS1 부분을 수정해주면 된다. 사용자별로 설정이 되기 때문에 해당 사용자의 홈 디렉터리에 들어가서 수정을 해주도록 하자.\n.bashrc 파일\n1 2 3 4 5 6 7 8 9 10 11 vi ~/.bashrc IP=$( ifconfig | grep \u0026#39;inet addr:\u0026#39; | grep -v \u0026#39;127.0.0.1\u0026#39; | tail -1 | cut -d: -f2 | awk \u0026#39;{ print $1}\u0026#39;) if [ \u0026#34;$color_prompt\u0026#34; = yes ]; then PS1=\u0026#39;🎄HOME `date +%T` ${debian_chroot:+($debian_chroot)}\\[\\033[01;32m\\]\\u@$IP\\h\\[\\033[00m\\]:\\[\\033[01;34m\\]\\w\\[\\033[00m\\]\\$ \u0026#39; else PS1=\u0026#39;🎄HOME `date +%T` ${debian_chroot:+($debian_chroot)}\\u@$IP\\h:\\w\\$ \u0026#39; fi # apply bashrc source ~/.bashrc 간단하게 서버 이름 + 시간 + 유저이름이 보이도록 수정을 했는데 프롬프트는 무궁무진하게 수정이 가능하다. [프롬프트 테마] 를 보면 더 다양한 형태로 수정을 할 수 있다.\n시간대 변경 서버 시간은 UTC로 하는 것이 업계 표준이다. 앞으로는 꼭 준수하도록 하자.\n서버를 처음 시작하면 시간대가 기본 UTC로 설정이 되어있다. 글로벌 서비스를 하면 세계 기준시가 UTC라서 그대로 내버려둬도 되는데 개인이 한국에서 쓰는 서버라서 KST로 변경했다. 서버의 시간대는 date 를 찍어보면 맨 뒤에 어떤 시간대인지 나온다.\n시간대 변경\n1 2 3 4 5 # 어떤 시간대를 지원하나 먼저 확인 timedatectl list-timezones # 시간대를 서울로 변경 timedatectl set-timezone Asia/Seoul 서버 설치 후 설치할 것들 build-essential build-essential 패키지는 개발에 필요한 기본 라이브러리와 헤더파일들을 가지고 있는데 C 컴파일러 등등이 포함되어있다. 우분투에 설치되는 기본 패키지 상세 정보는 [우분투 패키지]에서 확인이 가능하다.\nbuild-essential 패키지 설치\n1 2 # install build-essential sudo apt-get install build-essential Java 개발을 자바로 하다보니까 서버를 설치하면 필수적으로 자바를 설치해야 한다. 서버에서 어떤 자바 버전들을 지원하는지 보고 원하는 버전을 설치해주면 된다.\n자바 설치\n1 2 3 4 5 # 설치 가능한 jdk 확인 sudo apt search jdk # 설치 sudo apt-get install \u0026#34;java_version\u0026#34; 이정도로 하면 초기에 기본으로 설정해줘야하는 세팅들이 어느정도 정리가 된다. 밑에는 서버 사용하면서 그때그때 생각나는 편의를 위한 세팅들을 기록할 예정이다.\nvi들어가면 줄번호 나오게 변경 기본 디렉토리의 .vimrc 를 수정해주면 된다.\n.vimrc 파일\n1 2 3 4 5 # open vim setting for user vi ~/.vimrc # add this in vimrc file set number 프롬프트 ip 주소 나오게 변경 vi ~/.bashrc 로 user별 설정 파일을 열어주고 PS1 있는 부분을 아래와 같이 수정해주면 된다. 그럼 user_name@|ip_address| 이런 형태로 프롬프트가 변한다.\n.bashrc 파일\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # Set the prompt to include the IP address instead of hostname function get_ip () { IFACE=$(ip -4 route | grep default | head -n1 | awk \u0026#39;{print $5}\u0026#39;) if [ ! -z $IFACE ]; then echo -n \u0026#34;|\u0026#34;; ip -4 -o addr show scope global $IFACE | awk \u0026#39;{gsub(/\\/.*/, \u0026#34;|\u0026#34;,$4); print $4}\u0026#39; | paste -s -d \u0026#34;\u0026#34; else echo -n \u0026#34;||\u0026#34; fi } if [ \u0026#34;$color_prompt\u0026#34; = yes ]; then PS1=\u0026#39;${debian_chroot:+($debian_chroot)}\\[\\033[01;32m\\]\\u\\[\\033[01;34m\\]@\\[\\033[32m\\]$(get_ip)\\[\\033[00m\\]:\\[\\033[01;34m\\]\\w\\[\\033[00m\\]\\$ \u0026#39; else PS1=\u0026#39;${debian_chroot:+($debian_chroot)}\\u@$(get_ip):\\w\\$ \u0026#39; fi unset color_prompt force_color_prompt 우분투 기본 shell 설정 변경 가끔 도커를 쓰거나 새로 서버를 만들면 쉘에 아무것도 나오지 않고 $ 표시만 나오는 떄가 있다. 우분투에서 기본으로 사용하는 쉘이 c shell로 되어있기 때문이고, 기본 쉘을 bash shell로 바꿔주면 익숙한 형태의 터미널 쉘이 된다.\n사용자에 따라 기본으로 설정된 쉘이 다른데, 이는 /etc/passwd 파일에서 확인을 할 수 있다. 위의 이미지처럼 사용하는 계정을 찾아 /bin 뒤의 부분을 바꿔주면 된다. 나는 dev라는 계정을 쓰고 있고, bash shell로 바꾸고 싶었기 때문에 /bin/bash 로 수정해주었다. 수정을 해주고 로그인을 다시 해주면 터미널이 변경된 쉘로 나온다!\n[Picture 1] Bash MariDB 설치/설정 Maria DB 설치하기 개인 프로젝트에서 DB를 사용하게 되어서 우분투에 마리아 DB를 설치했는데 MySQL과 중첩이 되는 둥 몇 번 잘못 설치를 해서 서버를 다시 밀고 설치하기를 반복했다. 결국은 [우분투에 Maria DB 설치]보면서 무사히 DB 설치를 마쳤다.\nMaria DB 설치\n1 2 3 sudo apt update sudo apt install mariadb-server sudo mysql_secure_installation sudo mysql_secure_installation 을 실행하면 Maria DB에서 기본적으로 설정되는 보안 옵션을 조금 더 보완할 수 있게 옵션을 변경할 수 있다.\nmysql_secure_installation 설정 변경 프롬프트\n1 2 3 4 5 6 7 8 # localhost 에서만 access 가능한 계정 GRANT ALL ON *.* TO \u0026#39;admin\u0026#39;@\u0026#39;localhost\u0026#39; IDENTIFIED BY \u0026#39;password\u0026#39; WITH GRANT OPTION; # 모든 IP에서 access가 가능하게 와일드카드를 부여한 계정 GRANT ALL ON *.* TO \u0026#39;admin\u0026#39;@\u0026#39;%\u0026#39; IDENTIFIED BY \u0026#39;password\u0026#39; WITH GRANT OPTION; # mysql 계정과 관련된 작업 후에는 꼭 flush를 해서 refresh 를 해주자! FLUSH PRIVILEGES; DB를 조작할 때는 되도록 ROOT를 사용하지 말아야 하기 때문에 DB 설치를 진행하면서 default로 사용할 계정도 함께 만들어줬다. 이 계정을 localhost 에서만 사용을 한다면 여기서 끝이지만 remote 환경에서 해당 계정으로 접속을 가능하게 하려면 몇가지 설정을 더 해줘야 한다.\nMaria DB 리모트 접속 허용 원격 환경에서 DB에 접속하기 위해서는 아래의 3가지 사항을 설정해야 한다.\n리모트 접속 설정\nmysql 계정을 만들 때 와일드 카드 %를 넣던가 access IP를 추가해준다. mysql conf 파일에서 bind-access가 localhost로 되어있는데 0.0.0.0으로 변경 해 모두 허용해준다. (안된다면) 우분투에서도 mysql port인 3306 방화벽을 내려준다. 만약 aws와 같은 cloud 도 이용하고 있다면 cloud 의 설정도 변경해야 한다. mysql conf를 변경하고 난 후 mysql을 재시작해주면 이제 외부에서도 방금 만든 계정으로 DB에 접속할 수 있다.\nmysql 재시작\n1 sudo /etc/init.d/mysql restart DB 리모트 접속 허용에 대한 보안 이슈 mysql conf에서 bind-access 를 0.0.0.0으로 해두면 모든 remote IP에서 접속이 가능하기 때문에 보안상으로 문제가 있지는 않을까 찾아봤다.\nbind-access는 mysql이 들을 수 있는(listen) 특정 네트워크들을 뜻한다. 이렇게 0.0.0.0 으로 열어버리면 역시나 보안상으로 좋지 않다고 한다. default 설정처럼 그냥 로컬 머신에서만 DB와 연결하는 게 제일 안전하다.\nbind-access 설정에 따른 리모트 접속 차이\nIf MySQL binds to 127.0.0.1, then only software on the same computer will be able to connect (because 127.0.0.1 is always the local computer). If MySQL binds to 192.168.0.2 (and the server computer\u0026rsquo;s IP address is 192.168.0.2 and it\u0026rsquo;s on a /24 subnet), then any computers on the same subnet (anything that starts with 192.168.0) will be able to connect. If MySQL binds to 0.0.0.0, then any computer which is able to reach the server computer over the network will be able to connect. 보안 이슈를 해결하려면? 외부에서 접속을 해야 해서 bind-address를 활짝 열어버렸는데 보안을 조금이라도 강화하려면 mysql user 설정을 할 때 와일드 카드로 모든 IP에서 access 할 수 있게 할 게 아니라 특정 IP에서만 access 할 수 있게 해야 겠다.\n","date":"2021-04-21","permalink":"https://leeleelee3264.github.io/post/2021-04-21-linux-server-init-setting/","tags":["Infra"],"title":"[Infra] Ubuntu 설치와 초기 설정하기"},{"content":"\nGithub Action을 활용해서 Netflix 트위터 봇을 만들어보자. 기술 스택: Python, tweepy, Twitter API, auth, Github action, Github action workflow.\n[github]\n[트위터 계정]\nIndex\n프로젝트 목표 프로젝트 구현 다음 프로젝트 목표 프로젝트 목표 프로젝트 진행 동기 예전부터 자동으로 응답을 해주는 카톡봇이나 자동으로 트윗을 해주는 트윗봇을 만드는 프로젝트를 하고 싶었는데 어떤걸 만들면 좋을지 몰라 미뤄두고 있었다. 그런데 얼마전에 다음에서 넷플릭스 상영 예정작을 알려주는 페이지를 만든걸 보고 일주일에 한 번씩 넷플릭스 상영 예정작을 트위터 자동봇으로 만들면 편할 것 같아 프로젝트를 진행했다.\nGithub Action?\n깃허브에서 CI/CD를 위해 사용되는 기능이다. Action을 등록할 때 파이썬을 지원해주고 있었다. 때문에 Action에 파이썬으로 작성한 스크립트를 등록하면 별도의 서버를 만들지 않고도 크롤링을 해서 트위터에 게시할 수 있기에 프로젝트에서 사용하기로 결정했다.\n개발 목표\n별도의 서버를 띄우지 않기 위해 github action을 트리거로 사용한다. python과 tweepy 라이브러리를 사용한다. Twitter 에서 개발자 계정을 발급받아 API를 사용한다. Dev Stack stack info Backend language python Backend api twitter api Server server less Scheduler github action 프로젝트 구현 Twitter Bot Flow 트위터 봇 플로우는 [Picture 1] 과 같다.\n[Picture 1] Twitter Bot Flow 리소스 프로젝트에서 활용할 리소스들을 정리했다.\n[Twitter API] [Python tweepy 라이브러리] [다음 넷플릭스 페이지] 트위터 api와 tweepy로 트윗하기 트위터 api 계정을 만들고 대시보드에 들어가면 총 네개의 인증키를 받을 수 있다. 현재 트위터 api에서는 Oauth1, 2와 기본인증법 여러개의 인증을 지원하기 때문에 사용할 인증법을 선택해주면 된다. 트위터 auth에 대한 [더 자세한 사항]을 확인해보자.\n발급되는 키\nApi key Api key secret Bearer Token Access Token Secret tweepy 라이브러리 인증 코드 트위터가 지원하는 인증 방법이 다양하기 때문에 tweepy도 이에 맞춰 다양한 지원을 한다. tweepy에서 제공하는 [인증 기능 문서] 에서 자세히 살필 수 있다.\nauth.py 더보기 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 # Tweepy # Copyright 2009-2020 Joshua Roesslein # See LICENSE for details. import logging import requests import six from requests.auth import AuthBase from requests_oauthlib import OAuth1, OAuth1Session from six.moves.urllib.parse import parse_qs from tweepy.api import API from tweepy.error import TweepError WARNING_MESSAGE = \u0026#34;\u0026#34;\u0026#34;Warning! Due to a Twitter API bug, signin_with_twitter and access_type don\u0026#39;t always play nice together. Details https://dev.twitter.com/discussions/21281\u0026#34;\u0026#34;\u0026#34; log = logging.getLogger(__name__) class AuthHandler(object): def apply_auth(self, url, method, headers, parameters): \u0026#34;\u0026#34;\u0026#34;Apply authentication headers to request\u0026#34;\u0026#34;\u0026#34; raise NotImplementedError def get_username(self): \u0026#34;\u0026#34;\u0026#34;Return the username of the authenticated user\u0026#34;\u0026#34;\u0026#34; raise NotImplementedError class OAuthHandler(AuthHandler): \u0026#34;\u0026#34;\u0026#34;OAuth authentication handler\u0026#34;\u0026#34;\u0026#34; OAUTH_HOST = \u0026#39;api.twitter.com\u0026#39; OAUTH_ROOT = \u0026#39;/oauth/\u0026#39; def __init__(self, consumer_key, consumer_secret, callback=None): if type(consumer_key) == six.text_type: consumer_key = consumer_key.encode(\u0026#39;ascii\u0026#39;) if type(consumer_secret) == six.text_type: consumer_secret = consumer_secret.encode(\u0026#39;ascii\u0026#39;) self.consumer_key = consumer_key self.consumer_secret = consumer_secret self.access_token = None self.access_token_secret = None self.callback = callback self.username = None self.request_token = {} self.oauth = OAuth1Session(consumer_key, client_secret=consumer_secret, callback_uri=self.callback) def _get_oauth_url(self, endpoint): return \u0026#39;https://\u0026#39; + self.OAUTH_HOST + self.OAUTH_ROOT + endpoint def apply_auth(self): return OAuth1(self.consumer_key, client_secret=self.consumer_secret, resource_owner_key=self.access_token, resource_owner_secret=self.access_token_secret, decoding=None) def _get_request_token(self, access_type=None): try: url = self._get_oauth_url(\u0026#39;request_token\u0026#39;) if access_type: url += \u0026#39;?x_auth_access_type=%s\u0026#39; % access_type return self.oauth.fetch_request_token(url) except Exception as e: raise TweepError(e) def set_access_token(self, key, secret): self.access_token = key self.access_token_secret = secret def get_authorization_url(self, signin_with_twitter=False, access_type=None): \u0026#34;\u0026#34;\u0026#34;Get the authorization URL to redirect the user\u0026#34;\u0026#34;\u0026#34; try: if signin_with_twitter: url = self._get_oauth_url(\u0026#39;authenticate\u0026#39;) if access_type: log.warning(WARNING_MESSAGE) else: url = self._get_oauth_url(\u0026#39;authorize\u0026#39;) self.request_token = self._get_request_token(access_type=access_type) return self.oauth.authorization_url(url) except Exception as e: raise TweepError(e) def get_access_token(self, verifier=None): \u0026#34;\u0026#34;\u0026#34; After user has authorized the request token, get access token with user supplied verifier. \u0026#34;\u0026#34;\u0026#34; try: url = self._get_oauth_url(\u0026#39;access_token\u0026#39;) self.oauth = OAuth1Session(self.consumer_key, client_secret=self.consumer_secret, resource_owner_key=self.request_token[\u0026#39;oauth_token\u0026#39;], resource_owner_secret=self.request_token[\u0026#39;oauth_token_secret\u0026#39;], verifier=verifier, callback_uri=self.callback) resp = self.oauth.fetch_access_token(url) self.access_token = resp[\u0026#39;oauth_token\u0026#39;] self.access_token_secret = resp[\u0026#39;oauth_token_secret\u0026#39;] return self.access_token, self.access_token_secret except Exception as e: raise TweepError(e) def get_xauth_access_token(self, username, password): \u0026#34;\u0026#34;\u0026#34; Get an access token from an username and password combination. In order to get this working you need to create an app at http://twitter.com/apps, after that send a mail to api@twitter.com and request activation of xAuth for it. \u0026#34;\u0026#34;\u0026#34; try: url = self._get_oauth_url(\u0026#39;access_token\u0026#39;) oauth = OAuth1(self.consumer_key, client_secret=self.consumer_secret) r = requests.post(url=url, auth=oauth, headers={\u0026#39;x_auth_mode\u0026#39;: \u0026#39;client_auth\u0026#39;, \u0026#39;x_auth_username\u0026#39;: username, \u0026#39;x_auth_password\u0026#39;: password}) credentials = parse_qs(r.content) return credentials.get(\u0026#39;oauth_token\u0026#39;)[0], credentials.get(\u0026#39;oauth_token_secret\u0026#39;)[0] except Exception as e: raise TweepError(e) def get_username(self): if self.username is None: api = API(self) user = api.verify_credentials() if user: self.username = user.screen_name else: raise TweepError(\u0026#39;Unable to get username,\u0026#39; \u0026#39; invalid oauth token!\u0026#39;) return self.username class OAuth2Bearer(AuthBase): def __init__(self, bearer_token): self.bearer_token = bearer_token def __call__(self, request): request.headers[\u0026#39;Authorization\u0026#39;] = \u0026#39;Bearer \u0026#39; + self.bearer_token return request class AppAuthHandler(AuthHandler): \u0026#34;\u0026#34;\u0026#34;Application-only authentication handler\u0026#34;\u0026#34;\u0026#34; OAUTH_HOST = \u0026#39;api.twitter.com\u0026#39; OAUTH_ROOT = \u0026#39;/oauth2/\u0026#39; def __init__(self, consumer_key, consumer_secret): self.consumer_key = consumer_key self.consumer_secret = consumer_secret self._bearer_token = \u0026#39;\u0026#39; resp = requests.post(self._get_oauth_url(\u0026#39;token\u0026#39;), auth=(self.consumer_key, self.consumer_secret), data={\u0026#39;grant_type\u0026#39;: \u0026#39;client_credentials\u0026#39;}) data = resp.json() if data.get(\u0026#39;token_type\u0026#39;) != \u0026#39;bearer\u0026#39;: raise TweepError(\u0026#39;Expected token_type to equal \u0026#34;bearer\u0026#34;, \u0026#39; \u0026#39;but got %s instead\u0026#39; % data.get(\u0026#39;token_type\u0026#39;)) self._bearer_token = data[\u0026#39;access_token\u0026#39;] def _get_oauth_url(self, endpoint): return \u0026#39;https://\u0026#39; + self.OAUTH_HOST + self.OAUTH_ROOT + endpoint def apply_auth(self): return OAuth2Bearer(self._bearer_token) oauth1_session1.py 더보기 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 from __future__ import unicode_literals try: from urlparse import urlparse except ImportError: from urllib.parse import urlparse import logging from oauthlib.common import add_params_to_uri from oauthlib.common import urldecode as _urldecode from oauthlib.oauth1 import SIGNATURE_HMAC, SIGNATURE_RSA, SIGNATURE_TYPE_AUTH_HEADER import requests from . import OAuth1 log = logging.getLogger(__name__) def urldecode(body): \u0026#34;\u0026#34;\u0026#34;Parse query or json to python dictionary\u0026#34;\u0026#34;\u0026#34; try: return _urldecode(body) except Exception: import json return json.loads(body) class TokenRequestDenied(ValueError): def __init__(self, message, response): super(TokenRequestDenied, self).__init__(message) self.response = response @property def status_code(self): \u0026#34;\u0026#34;\u0026#34;For backwards-compatibility purposes\u0026#34;\u0026#34;\u0026#34; return self.response.status_code class TokenMissing(ValueError): def __init__(self, message, response): super(TokenMissing, self).__init__(message) self.response = response class VerifierMissing(ValueError): pass class OAuth1Session(requests.Session): \u0026#34;\u0026#34;\u0026#34;Request signing and convenience methods for the oauth dance. What is the difference between OAuth1Session and OAuth1? OAuth1Session actually uses OAuth1 internally and its purpose is to assist in the OAuth workflow through convenience methods to prepare authorization URLs and parse the various token and redirection responses. It also provide rudimentary validation of responses. An example of the OAuth workflow using a basic CLI app and Twitter. \u0026gt;\u0026gt;\u0026gt; # Credentials obtained during the registration. \u0026gt;\u0026gt;\u0026gt; client_key = \u0026#39;client key\u0026#39; \u0026gt;\u0026gt;\u0026gt; client_secret = \u0026#39;secret\u0026#39; \u0026gt;\u0026gt;\u0026gt; callback_uri = \u0026#39;https://127.0.0.1/callback\u0026#39; \u0026gt;\u0026gt;\u0026gt; \u0026gt;\u0026gt;\u0026gt; # Endpoints found in the OAuth provider API documentation \u0026gt;\u0026gt;\u0026gt; request_token_url = \u0026#39;https://api.twitter.com/oauth/request_token\u0026#39; \u0026gt;\u0026gt;\u0026gt; authorization_url = \u0026#39;https://api.twitter.com/oauth/authorize\u0026#39; \u0026gt;\u0026gt;\u0026gt; access_token_url = \u0026#39;https://api.twitter.com/oauth/access_token\u0026#39; \u0026gt;\u0026gt;\u0026gt; \u0026gt;\u0026gt;\u0026gt; oauth_session = OAuth1Session(client_key,client_secret=client_secret, callback_uri=callback_uri) \u0026gt;\u0026gt;\u0026gt; \u0026gt;\u0026gt;\u0026gt; # First step, fetch the request token. \u0026gt;\u0026gt;\u0026gt; oauth_session.fetch_request_token(request_token_url) { \u0026#39;oauth_token\u0026#39;: \u0026#39;kjerht2309u\u0026#39;, \u0026#39;oauth_token_secret\u0026#39;: \u0026#39;lsdajfh923874\u0026#39;, } \u0026gt;\u0026gt;\u0026gt; \u0026gt;\u0026gt;\u0026gt; # Second step. Follow this link and authorize \u0026gt;\u0026gt;\u0026gt; oauth_session.authorization_url(authorization_url) \u0026#39;https://api.twitter.com/oauth/authorize?oauth_token=sdf0o9823sjdfsdf\u0026amp;oauth_callback=https%3A%2F%2F127.0.0.1%2Fcallback\u0026#39; \u0026gt;\u0026gt;\u0026gt; \u0026gt;\u0026gt;\u0026gt; # Third step. Fetch the access token \u0026gt;\u0026gt;\u0026gt; redirect_response = raw_input(\u0026#39;Paste the full redirect URL here.\u0026#39;) \u0026gt;\u0026gt;\u0026gt; oauth_session.parse_authorization_response(redirect_response) { \u0026#39;oauth_token: \u0026#39;kjerht2309u\u0026#39;, \u0026#39;oauth_token_secret: \u0026#39;lsdajfh923874\u0026#39;, \u0026#39;oauth_verifier: \u0026#39;w34o8967345\u0026#39;, } \u0026gt;\u0026gt;\u0026gt; oauth_session.fetch_access_token(access_token_url) { \u0026#39;oauth_token\u0026#39;: \u0026#39;sdf0o9823sjdfsdf\u0026#39;, \u0026#39;oauth_token_secret\u0026#39;: \u0026#39;2kjshdfp92i34asdasd\u0026#39;, } \u0026gt;\u0026gt;\u0026gt; # Done. You can now make OAuth requests. \u0026gt;\u0026gt;\u0026gt; status_url = \u0026#39;http://api.twitter.com/1/statuses/update.json\u0026#39; \u0026gt;\u0026gt;\u0026gt; new_status = {\u0026#39;status\u0026#39;: \u0026#39;hello world!\u0026#39;} \u0026gt;\u0026gt;\u0026gt; oauth_session.post(status_url, data=new_status) \u0026lt;Response [200]\u0026gt; \u0026#34;\u0026#34;\u0026#34; def __init__( self, client_key, client_secret=None, resource_owner_key=None, resource_owner_secret=None, callback_uri=None, signature_method=SIGNATURE_HMAC, signature_type=SIGNATURE_TYPE_AUTH_HEADER, rsa_key=None, verifier=None, client_class=None, force_include_body=False, **kwargs ): \u0026#34;\u0026#34;\u0026#34;Construct the OAuth 1 session. :param client_key: A client specific identifier. :param client_secret: A client specific secret used to create HMAC and plaintext signatures. :param resource_owner_key: A resource owner key, also referred to as request token or access token depending on when in the workflow it is used. :param resource_owner_secret: A resource owner secret obtained with either a request or access token. Often referred to as token secret. :param callback_uri: The URL the user is redirect back to after authorization. :param signature_method: Signature methods determine how the OAuth signature is created. The three options are oauthlib.oauth1.SIGNATURE_HMAC (default), oauthlib.oauth1.SIGNATURE_RSA and oauthlib.oauth1.SIGNATURE_PLAIN. :param signature_type: Signature type decides where the OAuth parameters are added. Either in the Authorization header (default) or to the URL query parameters or the request body. Defined as oauthlib.oauth1.SIGNATURE_TYPE_AUTH_HEADER, oauthlib.oauth1.SIGNATURE_TYPE_QUERY and oauthlib.oauth1.SIGNATURE_TYPE_BODY respectively. :param rsa_key: The private RSA key as a string. Can only be used with signature_method=oauthlib.oauth1.SIGNATURE_RSA. :param verifier: A verifier string to prove authorization was granted. :param client_class: A subclass of `oauthlib.oauth1.Client` to use with `requests_oauthlib.OAuth1` instead of the default :param force_include_body: Always include the request body in the signature creation. :param **kwargs: Additional keyword arguments passed to `OAuth1` \u0026#34;\u0026#34;\u0026#34; super(OAuth1Session, self).__init__() self._client = OAuth1( client_key, client_secret=client_secret, resource_owner_key=resource_owner_key, resource_owner_secret=resource_owner_secret, callback_uri=callback_uri, signature_method=signature_method, signature_type=signature_type, rsa_key=rsa_key, verifier=verifier, client_class=client_class, force_include_body=force_include_body, **kwargs ) self.auth = self._client @property def token(self): oauth_token = self._client.client.resource_owner_key oauth_token_secret = self._client.client.resource_owner_secret oauth_verifier = self._client.client.verifier token_dict = {} if oauth_token: token_dict[\u0026#34;oauth_token\u0026#34;] = oauth_token if oauth_token_secret: token_dict[\u0026#34;oauth_token_secret\u0026#34;] = oauth_token_secret if oauth_verifier: token_dict[\u0026#34;oauth_verifier\u0026#34;] = oauth_verifier return token_dict @token.setter def token(self, value): self._populate_attributes(value) @property def authorized(self): \u0026#34;\u0026#34;\u0026#34;Boolean that indicates whether this session has an OAuth token or not. If `self.authorized` is True, you can reasonably expect OAuth-protected requests to the resource to succeed. If `self.authorized` is False, you need the user to go through the OAuth authentication dance before OAuth-protected requests to the resource will succeed. \u0026#34;\u0026#34;\u0026#34; if self._client.client.signature_method == SIGNATURE_RSA: # RSA only uses resource_owner_key return bool(self._client.client.resource_owner_key) else: # other methods of authentication use all three pieces return ( bool(self._client.client.client_secret) and bool(self._client.client.resource_owner_key) and bool(self._client.client.resource_owner_secret) ) def authorization_url(self, url, request_token=None, **kwargs): \u0026#34;\u0026#34;\u0026#34;Create an authorization URL by appending request_token and optional kwargs to url. This is the second step in the OAuth 1 workflow. The user should be redirected to this authorization URL, grant access to you, and then be redirected back to you. The redirection back can either be specified during client registration or by supplying a callback URI per request. :param url: The authorization endpoint URL. :param request_token: The previously obtained request token. :param kwargs: Optional parameters to append to the URL. :returns: The authorization URL with new parameters embedded. An example using a registered default callback URI. \u0026gt;\u0026gt;\u0026gt; request_token_url = \u0026#39;https://api.twitter.com/oauth/request_token\u0026#39; \u0026gt;\u0026gt;\u0026gt; authorization_url = \u0026#39;https://api.twitter.com/oauth/authorize\u0026#39; \u0026gt;\u0026gt;\u0026gt; oauth_session = OAuth1Session(\u0026#39;client-key\u0026#39;, client_secret=\u0026#39;secret\u0026#39;) \u0026gt;\u0026gt;\u0026gt; oauth_session.fetch_request_token(request_token_url) { \u0026#39;oauth_token\u0026#39;: \u0026#39;sdf0o9823sjdfsdf\u0026#39;, \u0026#39;oauth_token_secret\u0026#39;: \u0026#39;2kjshdfp92i34asdasd\u0026#39;, } \u0026gt;\u0026gt;\u0026gt; oauth_session.authorization_url(authorization_url) \u0026#39;https://api.twitter.com/oauth/authorize?oauth_token=sdf0o9823sjdfsdf\u0026#39; \u0026gt;\u0026gt;\u0026gt; oauth_session.authorization_url(authorization_url, foo=\u0026#39;bar\u0026#39;) \u0026#39;https://api.twitter.com/oauth/authorize?oauth_token=sdf0o9823sjdfsdf\u0026amp;foo=bar\u0026#39; An example using an explicit callback URI. \u0026gt;\u0026gt;\u0026gt; request_token_url = \u0026#39;https://api.twitter.com/oauth/request_token\u0026#39; \u0026gt;\u0026gt;\u0026gt; authorization_url = \u0026#39;https://api.twitter.com/oauth/authorize\u0026#39; \u0026gt;\u0026gt;\u0026gt; oauth_session = OAuth1Session(\u0026#39;client-key\u0026#39;, client_secret=\u0026#39;secret\u0026#39;, callback_uri=\u0026#39;https://127.0.0.1/callback\u0026#39;) \u0026gt;\u0026gt;\u0026gt; oauth_session.fetch_request_token(request_token_url) { \u0026#39;oauth_token\u0026#39;: \u0026#39;sdf0o9823sjdfsdf\u0026#39;, \u0026#39;oauth_token_secret\u0026#39;: \u0026#39;2kjshdfp92i34asdasd\u0026#39;, } \u0026gt;\u0026gt;\u0026gt; oauth_session.authorization_url(authorization_url) \u0026#39;https://api.twitter.com/oauth/authorize?oauth_token=sdf0o9823sjdfsdf\u0026amp;oauth_callback=https%3A%2F%2F127.0.0.1%2Fcallback\u0026#39; \u0026#34;\u0026#34;\u0026#34; kwargs[\u0026#34;oauth_token\u0026#34;] = request_token or self._client.client.resource_owner_key log.debug(\u0026#34;Adding parameters %s to url %s\u0026#34;, kwargs, url) return add_params_to_uri(url, kwargs.items()) def fetch_request_token(self, url, realm=None, **request_kwargs): r\u0026#34;\u0026#34;\u0026#34;Fetch a request token. This is the first step in the OAuth 1 workflow. A request token is obtained by making a signed post request to url. The token is then parsed from the application/x-www-form-urlencoded response and ready to be used to construct an authorization url. :param url: The request token endpoint URL. :param realm: A list of realms to request access to. :param \\*\\*request_kwargs: Optional arguments passed to \u0026#39;\u0026#39;post\u0026#39;\u0026#39; function in \u0026#39;\u0026#39;requests.Session\u0026#39;\u0026#39; :returns: The response in dict format. Note that a previously set callback_uri will be reset for your convenience, or else signature creation will be incorrect on consecutive requests. \u0026gt;\u0026gt;\u0026gt; request_token_url = \u0026#39;https://api.twitter.com/oauth/request_token\u0026#39; \u0026gt;\u0026gt;\u0026gt; oauth_session = OAuth1Session(\u0026#39;client-key\u0026#39;, client_secret=\u0026#39;secret\u0026#39;) \u0026gt;\u0026gt;\u0026gt; oauth_session.fetch_request_token(request_token_url) { \u0026#39;oauth_token\u0026#39;: \u0026#39;sdf0o9823sjdfsdf\u0026#39;, \u0026#39;oauth_token_secret\u0026#39;: \u0026#39;2kjshdfp92i34asdasd\u0026#39;, } \u0026#34;\u0026#34;\u0026#34; self._client.client.realm = \u0026#34; \u0026#34;.join(realm) if realm else None token = self._fetch_token(url, **request_kwargs) log.debug(\u0026#34;Resetting callback_uri and realm (not needed in next phase).\u0026#34;) self._client.client.callback_uri = None self._client.client.realm = None return token def fetch_access_token(self, url, verifier=None, **request_kwargs): \u0026#34;\u0026#34;\u0026#34;Fetch an access token. This is the final step in the OAuth 1 workflow. An access token is obtained using all previously obtained credentials, including the verifier from the authorization step. Note that a previously set verifier will be reset for your convenience, or else signature creation will be incorrect on consecutive requests. \u0026gt;\u0026gt;\u0026gt; access_token_url = \u0026#39;https://api.twitter.com/oauth/access_token\u0026#39; \u0026gt;\u0026gt;\u0026gt; redirect_response = \u0026#39;https://127.0.0.1/callback?oauth_token=kjerht2309uf\u0026amp;oauth_token_secret=lsdajfh923874\u0026amp;oauth_verifier=w34o8967345\u0026#39; \u0026gt;\u0026gt;\u0026gt; oauth_session = OAuth1Session(\u0026#39;client-key\u0026#39;, client_secret=\u0026#39;secret\u0026#39;) \u0026gt;\u0026gt;\u0026gt; oauth_session.parse_authorization_response(redirect_response) { \u0026#39;oauth_token: \u0026#39;kjerht2309u\u0026#39;, \u0026#39;oauth_token_secret: \u0026#39;lsdajfh923874\u0026#39;, \u0026#39;oauth_verifier: \u0026#39;w34o8967345\u0026#39;, } \u0026gt;\u0026gt;\u0026gt; oauth_session.fetch_access_token(access_token_url) { \u0026#39;oauth_token\u0026#39;: \u0026#39;sdf0o9823sjdfsdf\u0026#39;, \u0026#39;oauth_token_secret\u0026#39;: \u0026#39;2kjshdfp92i34asdasd\u0026#39;, } \u0026#34;\u0026#34;\u0026#34; if verifier: self._client.client.verifier = verifier if not getattr(self._client.client, \u0026#34;verifier\u0026#34;, None): raise VerifierMissing(\u0026#34;No client verifier has been set.\u0026#34;) token = self._fetch_token(url, **request_kwargs) log.debug(\u0026#34;Resetting verifier attribute, should not be used anymore.\u0026#34;) self._client.client.verifier = None return token def parse_authorization_response(self, url): \u0026#34;\u0026#34;\u0026#34;Extract parameters from the post authorization redirect response URL. :param url: The full URL that resulted from the user being redirected back from the OAuth provider to you, the client. :returns: A dict of parameters extracted from the URL. \u0026gt;\u0026gt;\u0026gt; redirect_response = \u0026#39;https://127.0.0.1/callback?oauth_token=kjerht2309uf\u0026amp;oauth_token_secret=lsdajfh923874\u0026amp;oauth_verifier=w34o8967345\u0026#39; \u0026gt;\u0026gt;\u0026gt; oauth_session = OAuth1Session(\u0026#39;client-key\u0026#39;, client_secret=\u0026#39;secret\u0026#39;) \u0026gt;\u0026gt;\u0026gt; oauth_session.parse_authorization_response(redirect_response) { \u0026#39;oauth_token: \u0026#39;kjerht2309u\u0026#39;, \u0026#39;oauth_token_secret: \u0026#39;lsdajfh923874\u0026#39;, \u0026#39;oauth_verifier: \u0026#39;w34o8967345\u0026#39;, } \u0026#34;\u0026#34;\u0026#34; log.debug(\u0026#34;Parsing token from query part of url %s\u0026#34;, url) token = dict(urldecode(urlparse(url).query)) log.debug(\u0026#34;Updating internal client token attribute.\u0026#34;) self._populate_attributes(token) self.token = token return token def _populate_attributes(self, token): if \u0026#34;oauth_token\u0026#34; in token: self._client.client.resource_owner_key = token[\u0026#34;oauth_token\u0026#34;] else: raise TokenMissing( \u0026#34;Response does not contain a token: {resp}\u0026#34;.format(resp=token), token ) if \u0026#34;oauth_token_secret\u0026#34; in token: self._client.client.resource_owner_secret = token[\u0026#34;oauth_token_secret\u0026#34;] if \u0026#34;oauth_verifier\u0026#34; in token: self._client.client.verifier = token[\u0026#34;oauth_verifier\u0026#34;] def _fetch_token(self, url, **request_kwargs): log.debug(\u0026#34;Fetching token from %s using client %s\u0026#34;, url, self._client.client) r = self.post(url, **request_kwargs) if r.status_code \u0026gt;= 400: error = \u0026#34;Token request failed with code %s, response was \u0026#39;%s\u0026#39;.\u0026#34; raise TokenRequestDenied(error % (r.status_code, r.text), r) log.debug(\u0026#39;Decoding token from response \u0026#34;%s\u0026#34;\u0026#39;, r.text) try: token = dict(urldecode(r.text.strip())) except ValueError as e: error = ( \u0026#34;Unable to decode token from token response. \u0026#34; \u0026#34;This is commonly caused by an unsuccessful request where\u0026#34; \u0026#34; a non urlencoded error message is returned. \u0026#34; \u0026#34;The decoding error was %s\u0026#34; \u0026#34;\u0026#34; % e ) raise ValueError(error) log.debug(\u0026#34;Obtained token %s\u0026#34;, token) log.debug(\u0026#34;Updating internal client attributes from token data.\u0026#34;) self._populate_attributes(token) self.token = token return token def rebuild_auth(self, prepared_request, response): \u0026#34;\u0026#34;\u0026#34; When being redirected we should always strip Authorization header, since nonce may not be reused as per OAuth spec. \u0026#34;\u0026#34;\u0026#34; if \u0026#34;Authorization\u0026#34; in prepared_request.headers: # If we get redirected to a new host, we should strip out # any authentication headers. prepared_request.headers.pop(\u0026#34;Authorization\u0026#34;, True) prepared_request.prepare_auth(self.auth) return tweepy를 사용한 구현체 tweepy 라이브러리를 사용해서 실제로 구현을 진행했다.\n트윗하기 구현체\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 def OAuth(): try: api_key = os.environ.get(\u0026#39;TWITTER_API_KEY\u0026#39;) api_key_secret = os.environ.get(\u0026#39;TWITTER_API_SECRET\u0026#39;) access_token = os.environ.get(\u0026#39;TWITTER_ACCESS_TOKEN\u0026#39;) access_token_secret = os.environ.get(\u0026#39;TWITTER_ACCESS_TOKEN_SECRET\u0026#39;) auth = tweepy.OAuthHandler(api_key, api_key_secret) auth.set_access_token(access_token, access_token_secret) return auth except Exception as e: return None def post_tweet(container: dict, date): print(\u0026#39;work calling\u0026#39;) oauth = OAuth() api = tweepy.API(oauth) _current_dir = os.path.dirname(os.path.abspath(__file__)) _path = Path(_current_dir) BASE_DIR = _path.parent.absolute() IMG_DIR = f\u0026#39;{BASE_DIR}/img/netflix/{date}\u0026#39; print(f\u0026#39;{IMG_DIR}\u0026#39;) for key in container: tTitle = key tFile = f\u0026#39;{IMG_DIR}/{tTitle}.png\u0026#39; print(f\u0026#39;{tFile}\u0026#39;) reTitle = regex.change_hyphen(tTitle) tweet_format = f\u0026#39;[{reTitle}]\\n 공개 여정일:{container[key]}\u0026#39; api.update_with_media(tFile, status=tweet_format) Github Action Repository Secret 실제 사용 코드를 보면 TWITTER_API_KEY, TWITTER_API_SECRET 등 environ 을 이용해 환경에서 받아온 변수들이 있다. 트위터 api를 사용하려면 여러개의 인증키가 필요한데 코드를 프라이빗 repo로 올려도 되지만 Repository Secret 으로 인증키들을 등록하는 방법을 선택했다.\n[깃허브 공식 가이드]에 secret을 등록하는 방법이 나와있다. 따라해보다 보면 사진처럼 repo에 노출하지 않아도 사용할 수 있는 secret key들이 만들어진다.\n[Picture 2] Repository Secret Repository Secret 사용하기 사용 시나리오\n액션을 등록할 때 secret을 넘겨준다. 실행시에 코드로 환경변수에 접근해서 코드 단에서 환경을 통해 받아온다. 자바에서 main 실행할 때 args들을 통해 환경값을 넘기는 것 파이썬에서 environ를 통해 환경값을 넘기는 것 스택 오버 플로우에 있는 [github action에서 secret을 넘겨 python으로 받아오기]를 레퍼런스로 코드를 만들었다. 중요한 점은 secret으로 등록한 키들을 파이썬을 실행하는 스크립트를 실행한 후에 던져주는 것이었다.\nGithub Action Workflow 깃허브에서는 액션을 등록하기 위해서는 yml로 스크립트를 작성해야 한다. 그 스크립트 안에는 한 작업이 아니라 여러가지 작업이 하나씩 순서대로 처리가 되는데 그래서 액션을 등록하는 파일 이름을 workflow라고 하는 게 아닐까?\n액션을 성공적으로 실행시키기 위해서는 이 workflow 스크립트를 잘 짜는 것이 제일 중요하다. 또 액션이 update (commit) 마다, 스케쥴링에 따라 실행이 되기 때문에 결과를 확인하기 위해서는 짧아도 1~2분 정도는 기다려야 해서 여러번 수정하다보면 많은 시간을 소비하게 된다.\nworkflow 시나리오\n파이썬 실행을 위해 플라스크와 requirements.txt 속의 라이브러리들을 설치 상영 예정작 정보가 있는 사이트 크롤링 크롤링한 이미지들을 저장하기 위한 commit/push 실제로 트윗을 하는 파이썬 코드 실행 트위터 봇을 위한 workflow.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 # This workflow will install Python dependencies, run tests and lint with a single version of Python # For more information see: https://help.github.com/actions/language-and-framework-guides/using-python-with-github-actions name: Netflix_Crawl on: schedule: - cron: \u0026#39;0 0 * * Sat\u0026#39; push: branches: [ master ] pull_request: branches: [ master ] jobs: build: runs-on: ubuntu-latest steps: - uses: actions/checkout@v2 - name: Set up Python 3.9 uses: actions/setup-python@v2 with: python-version: 3.9 - name: Install dependencies run: | python -m pip install --upgrade pip pip install flake8 pytest if [ -f requirements.txt ]; then pip install -r requirements.txt; fi - name: Lint with flake8 run: | # stop the build if there are Python syntax errors or undefined names flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics # exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics - name: Run netflix crawler with python run: | python3 \u0026#34;./crawl/netflix.py\u0026#34; - name: Commits run: | git config --local user.email \u0026#34;absinthe4902@naver.com\u0026#34; git config --local user.name \u0026#34;AUTO_ADD_GIT_ACTION\u0026#34; git add . git commit -m \u0026#34;AUTO ADD: commit downloaded image\u0026#34; - name: Push uses: ad-m/github-push-action@master with: branch: \u0026#39;master\u0026#39; github_token: $ - name: Tweet for final run: | python3 \u0026#34;./crawl/tweeting.py\u0026#34; env: TWITTER_API_KEY: ${{ secrets.TWITTER_API_KEY }} TWITTER_API_SECRET: ${{ secrets.TWITTER_API_SECRET }} TWITTER_ACCESS_TOKEN: ${{ secrets.TWITTER_ACCESS_TOKEN }} TWITTER_ACCESS_TOKEN_SECRET: ${{ secrets.TWITTER_ACCESS_TOKEN_SECRET }} 매주 토요일 00:00시에 스케쥴러가 돌아가도록 작성했는데 시간대의 기준은 UTC이다.\n삽질 구간\n아까 스택오버플로우에서 말한 것처럼 환경변수는 파이썬 실행 후 넘겨주고 있다. 제일 헤매던 부분은 프로젝트에서 파이썬 코드가 있는 디렉터리를 찾는 부분이었다. 예를 들어 crawl 디렉터리 안에 있는 netflix.py를 실행할 때 동일한 디렉터리 안에 있는 유틸성 파일 regex.py를 찾지 못했다.\n액션은 repo를 기준으로 동작하기 때문에 current dir가 repo였던 twitter_project였을텐데 무슨 이유로 다른 파일들을 찾지 못했는지 모르겠다. 여러번의 수정을 거친 다음에 ./crawl/netflix.py 로 실행을 하니 정상적으로 작동을 했다.\n실제 동작 화면 [Picture 3] 트위터 계정 [Picture 4] 트윗 화면 1 [Picture 5] 트윗 화면 2 다음 프로젝트 목표 똑같이 twitter api를 사용 현재 한국에서 공연 하고 있는 연극과 뮤지컬을 알려주는 자동봇 tweepy를 사용하지 않고 직접 구현하기 최소한 인증을 받는 부분이라도 구현하기 (oauth 공부) 동일하게 server-less로 git action 사용 Update in 2022 위에서 기획했던 트위터 봇을 만들었다.\n[지금 공연중인 뮤지컬 알림봇] [오늘의 뮤지컬 스케줄] 프로젝트 구조 [Picture 6] 프로젝트 구조 Dev Stack stack info Backend language python Backend api twitter api Server Ubuntu 20 Scheduler Linux cron job ","date":"2021-04-16","permalink":"https://leeleelee3264.github.io/post/2021-04-16-twitterbot-with-git-action/","tags":["Project"],"title":"[Project] Github Action을 활용한 Netflix 트위터 봇 만들기"},{"content":"\nShell script로 무중단 배포를 흉내내는 방법을 알아본다. 또한 deployment method에 대해서 다룬다: Blue Green Deployment, Rolling Development, Canary Deployment.\nIndex\n무중단 배포란? 무중단 배포 구현하기 결론 무중단 배포란? 무중단 배포란 업데이트를 위해 배포를 할 때 어플리케이션이 멈추지 않는 것이다. 즉 배포를 할 때 서비스가 중단되는 다운타임이 발생하지 않는다.\n무중단 배포 방식\nBlue Green Deployment Rolling Deployment Canary Deployment Blue Green Deployment Blue와 Green 이라는 동일한 배포 환경을 준비한다. Green 은 live 되고 있는 환경을 의미한다. Blue는 새로운 버전을 가지고 있는 환경을 의미한다. Blue에서 새로운 버전을 테스트하고, 테스트 중에는 로드밸런서기 Green에 리퀘스트를 보낸다. 테스트가 완료되면 Blue로 리퀘스트를 보낸다. 만약 문제가 발생한다면 Green으로 롤백한다. 장점 롤백 하기가 쉽다. 다운타임이 없다. 단점 리소스가 두 배로 든다. 이는 두 배의 비용으로 이어질 수 있다. 두 개의 환경을 up-to-date 상태로 싱크하기 번거롭다. [Picture 1] Blue Green Deployment Rolling Deployment 점진적으로 running 중인 인스턴스를 새로운 버전으로 교체한다. 기존 버전과 새로운 버전이 함께 live 된다. 최소 N+1 개의 인스턴스가 필요하고, 이 한 개의 추가 인스턴스는 새로운 버전을 실행할 노드이다. 새로운 버전을 담은 인스턴스가 무사히 배포되면 다른 인스턴스들을 돌아가면서 새로운 버전으로 배포한다. 모든 인스턴스가 새로운 버전이 될 때 까지 반복한다. 쿠버네티스에서 사용하는 default 배포방식이다. 장점 기존 버전이 live 되어 있어, 최소한의 다운 타임만 존재한다. 최소 하나의 추가 인스턴스만 있으면 가능한 배포 방식이다. 반면 Blue Green은 하나의 infrastructure 가 있어야 한다. 롤백을 해야 한다면 기존 버전으로 트레픽을 돌리는 것이 가능하다. 단점 인스턴스의 개수에 따라 배포 시작과 live 사이에 상당한 지연이 있을 수 있다. 배포가 중간에 실패하면 롤백하는 것에 많은 시간이 소요될 수 있다. [Picture 2] Rolling Deployment Canary Deployment 서버 극히 일부를 새로운 버전으로 배포하여 지정된 유저들에게 서비스한다. 퍼포먼스, 이슈 등을 테스트와 모니터링하기에 좋은 배포 방식이다. 테스트를 할 유저들을 먼저 지정해야 한다. (베타 유저 등) 서비스의 앞단에 있는 로드밸런서, API 게이트웨이, 서비스 프록시등을 재설정하거나 feature 플래그를 두는 방식으로 구현할 수 있다. 문제가 없다면 기존 버전을 모두 새로운 버전으로 릴리즈한다. 장점 일부의 실 사용자들에게 테스트를 할 수 있다. 실패하더라도 일부의 사용자들에게만 영향이 미친다. feature flag를 사용한다면 최소한의 infrastructure 가 필요하다. 롤백이 간단하고 빠르다. 단점 일부의 유저들에게만 새 버전을 라우트 하는 것이 기술적으로 어렵다. feature flag는 cost-effective 한 방식이다. 옵저빌리티와 메트릭스, 분석 환경이 특히나 미리 잘 갖춰있어야 한다. [Picture 3] Canary Deployment 무중단 배포 구현하기 구현 환경 백업 서버를 따로 두지 않아서 맨날 서버를 업데이트할 때 서비스가 죽는다. 사내에서 소규모로 사용되는 서비스이지만 업데이트가 좀 잦은 편인데 계속 배포를 할 때 마다 서비스가 죽어버리면 사용하기가 힘들 것 같아 무중단 배포를 구현해보리고 했다.\n그런데 EC2 리소스가 좀 타이트 해서 평상시에 서버 두대를 띄우기는 힘들어 보였다. EC2 안에 앱서버가 두 대 뜨면 로드밸런스에서 하나가 업데이트 하느라 죽어도 다른 쪽으로 보내줄텐데 이런 형태는 지금 상황에서는 불가능했다. 그래서 평소에는 앱서버를 한 대 만 띄워두고 업데이트를 할 때 만 한 대 를 더 띄워 무중단 배포를 하기로 했다.\n구현할 무중단 배포 시나리오 업데이트 이전 상태 로드 밸런서인 nginx는 계속 두 포트(A: 4000, B: 4001) 모두를 바라본다. nginx가 날려준 리퀘스트를 실제로 처리하는 머신에서는 하나의 서버 A만 운영이 되고 있고, 서버 B는 다운 상태이다. 업데이트 진행 업데이트시에는 A보다 이전 버전인 B를 업데이트 된다. 서버 B를 가동한다. B가 완전히 뜨기 전까지 nginx는 A,B 둘 다 바라보니 리퀘스트 처리가 가능한 A가 받아서 처리한다 B서버가 올라간 걸 확인하면 A 서버를 죽인다. 업데이트 후 그럼 이제 요청이 띄워져있는 서버 인 B로 올라간다. 다음번에 업데이트를 해야 한다면 B보다 옛날 버전인 A가 업데이트 된다. 배포 시나리오 플로우 차트 플로우 차트 더보기 [Picture 4] 무중단 배포 시나리오 flow chart Nginx 작업을 하면서 nginx 는 어떻게 로드밸런싱을 할까 궁금해서 찾아봤다. 내가 하는 것처럼 앱서버를 두 개 두면 nginx는 필수적으로 요청을 분산하게 되어있다. [nginx로-로드밸런싱-하기]\n그리고 이 작업을 하다가 알게 되었는데 항상 nginx에서 설정이 바뀌면 nginx 서비스를 재시작(restart) 했는데 이것보다는 새로고침(reload) 하는게 좋다고 한다. restart는 말 그대로 재시작이라서 nginx를 한번 shutdown 하고 다시 시작한다. 반면 reload는 서버가 돌아가는 상태에서 설정 파일만 다시 불러와서 적용을 해준다. 서버 중단이 없다는 것 때문에 reload가 더 적합하다!\nNginx reload, restart 커맨드\n1 2 3 4 5 6 7 8 // reload, restart 등을 할 때 config 파일이 문법에 맞는지 꼭 먼저 점검을 한다. nginx -t // reload sudo systemctl reload nginx // restart sudo systemctl restart nginx 목표로 하고 있는 client - 로드 밸런서 - 앱서버 까지의 모식도를 그려봤다.\n[Picture 5] Nginx flow 무중단 배포 구현하기 무중단 배포를 위한 디렉터리 구조\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 tc-admin ├── blue_green_deploy.sh ├── blue_green_deploy.txt ├── dnx-touchcare-admin-0.1-SNAPSHOT.jar ├── gc.log ├── log_rotate.sh ├── server.log └── start.sh tc-admin-prod ├── blue_green_deploy.sh ├── dnx-touchcare-admin-0.1-SNAPSHOT.jar ├── gc.log ├── hs_err_pid29917.log ├── hs_err_pid29917.txt ├── log_rotate.sh ├── server.log └── start.sh backup-tc-admin ├── api │ └── dnx-touchcare-admin-0.1-SNAPSHOT.jar └── tpi └── dnx-touchcare-admin-0.1-SNAPSHOT.jar 배포 스크립트\n이 스크립트에서 사용하는 환경은 tpi로, tc-admin을 이용한다. 백업 jar가 올라가는 곳은 backup-tc-admin 으로 tpi/api 로 디렉터리를 분리했다.\nprofile 은 jar 를 실행하는 환경의 정보인데 prod와 백업용 back-prod를 두었고 포트는 각각 4000 과 4001로 할당했다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 #! /bin/sh echo \u0026#34;\u0026#34; echo \u0026#34;\u0026#34; echo \u0026#34;++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\u0026#34; echo \u0026#34;Blue Green Deployment\u0026#34; echo \u0026#34;Author: Seungmin Lee\u0026#34; echo \u0026#34;Date: 2021-02-26\u0026#34; echo \u0026#34;This script is only for blue green deploy, not for back up server.\u0026#34; echo \u0026#34;\u0026#34; echo \u0026#34;++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\u0026#34; echo \u0026#34;\u0026#34; echo \u0026#34;\u0026#34; # have to change for api and tpi SETTING=tpi PROFILE=prod PROFILE_BACKUP=back-prod PORT=4000 PORT_BACKUP=4001 CURRENT_PATH=/opt/tc-admin BACKUP_PATH=/opt/backup-tc-admin/${SETTING} SUB_URL=/tcadmin/api/test/profile CHECK_URL=https://${SETTING}.example.kr${SUB_URL} echo \u0026#34;\u0026gt;Check which profile is active...\u0026#34; PROFILE_CURRENT=$(curl -s $CHECK_URL) echo \u0026#34;\u0026gt;$PROFILE_CURRENT\u0026#34; if [ $PROFILE_CURRENT = $PROFILE ] then IDLE_PROFILE=$PROFILE_BACKUP IDLE_PORT=$PORT_BACKUP IDLE_DIR=$BACKUP_PATH elif [ $PROFILE_CURRENT = $PROFILE_BACKUP ] then IDLE_PROFILE=$PROFILE IDLE_PORT=$PORT IDLE_DIR=$CURRENT_PATH else echo \u0026#34;\u0026gt;Current profile is not matching with any of them...\u0026#34; echo \u0026#34;\u0026gt;Will use default $PROFILE for profile\u0026#34; IDLE_PROFILE=$PROFILE IDLE_PORT=$PORT IDLE_DIR=$CURRENT_PATH fi # update idle one and run # planed to seperate starting shell, but I have to copy/run first and kill the current one echo \u0026#34;\u0026#34; echo \u0026#34;\u0026#34; echo \u0026#34;++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\u0026#34; echo \u0026#34;Update JAR file to new one and Run\u0026#34; echo \u0026#34;target dir is ${IDLE_DIR}\u0026#34; echo \u0026#34;target property is ${IDLE_PROFILE}\u0026#34; echo \u0026#34;target port is ${IDLE_PORT}\u0026#34; echo \u0026#34;++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\u0026#34; echo \u0026#34;\u0026#34; echo \u0026#34;\u0026#34; cp -f ~/admin-0.1-SNAPSHOT.jar ${IDLE_DIR}/. echo \u0026#34;\u0026#34; echo \u0026#34;+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\u0026#34; echo \u0026#34;Now Deploy New Server....\u0026#34; echo \u0026#34;+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\u0026#34; echo \u0026#34;\u0026#34; echo \u0026#34;\u0026#34; DATE=`date +\u0026#39;%Y%m%d\u0026#39;` ETC_JAVA_OPTS=-XX:+UseStringDeduplication nohup java -Xms128m -Xmx128m -XX:NewRatio=1 -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -Xloggc:./gc.log -Dspring.profiles.active=${IDLE_PROFILE} $* -jar ${IDLE_DIR}/dnx-touchcare-admin-0.1-SNAPSHOT.jar \u0026gt;\u0026gt; ./server.log \u0026amp; UP_CHECK=\u0026#34;\u0026#34; while [ -z \u0026#34;$UP_CHECK\u0026#34; ] do UP_CHECK=$(curl -s http://localhost:${IDLE_PORT}/tcadmin/api/test/profile) done echo \u0026#34;\u0026#34; echo \u0026#34;Now killed old server...\u0026#34; echo \u0026#34;\u0026#34; KILL_PROCESS_PID=$(pgrep -f \u0026#34;profiles.active=${PROFILE_CURRENT} -jar\u0026#34;) echo \u0026#34;\u0026gt; kill process ${KILL_PROCESS_PID}\u0026#34; if [ -z $KILL_PROCESS_PID ] then echo \u0026#34;\u0026gt;There are no pid...\u0026#34; else echo \u0026#34;\u0026gt;kill -9 $KILL_PROCESS_PID\u0026#34; kill -9 ${KILL_PROCESS_PID} fi tail -f server.log 스크립트 상세 초반에 환경과 프로파일들, 포트와 jar가 있는 path들을 정해준다. curl 을 이용해서 현재 띄워져있는 서버의 프로파일을 알아낸다. 이때는 도메인을 이용해서 외부에서 서버로 요청을 보내는 것처럼 동작한다. 서버의 profile에 따라서 업데이트를 할 Idle 프로파일과 포트, 패스를 지정한다. 현재 프로파일이 prod면 back-prod로, back-prod면 prod 로 Idle이 설정된다. Idle profile 로 설정된 프로파일이 업데이트가 되면 해당 jar 파일을 띄운다. prod와 back-prod 2개의 서버가 떠있는 상태이다. curl 을 이용해서 Idle jar 가 완전히 구동하고 있다는 응답을 받을 때까지 요청을 보낸다. 이때는 localhost 와 Idle port 를 이용해서 내부에서 서버로 요청을 보낸다. 확인이 되면 이전버전으로 돌아가고 있는 jar의 프로세스를 종료시킨다. 앱서버가 완전히 뜬 것을 확인하는 방법 5을 구현하기 위해 많은 고민을 했다. 어떻게 해야 단순히 서버가 뜨는 것이 아닌 서비스 요청을 받아서 처리하는 수준의 완전히 구동된 서버의 상태를 알 수 있을까?\n앱서버 구동 확인 시도들\n1 2 3 4 5 6 7 8 9 10 11 // 1 try : 임의의 sleep 을 주기 sleep(10000) // 2 try : 해당 프로세스로 확인하기 pgrep -f \u0026#34;pfofiles.active=${IDLE_PROFLE} -jar\u0026#34; // 3 try : 해당 포트로 확인하기 ss -tlp | grep ${IDLE_PORT} // 4 try : 내부로 요청을 보내기 curl http://localhost/{IDLE_PORT} 1 try : 임의의 sleep 을 주기 처음에는 서버가 완전히 뜰 수 있게 임의의 sleep 값을 주었는데 제일 나쁜 방법이었다. 서버가 뜨는 속도가 일정하지 않기 때문에 sleep을 초과하고도 뜨지 않아서 서비스가 멈출 수 있었다.\n2 try : 해당 프로세스로 확인하기 두번째로는 프로세스가 떠있는 것으로 확인을 했는데 프로세스는 jar를 실행하자마자 뜨는 것이기 때문에 서버가 완전히 뜨는 것보다 훨씬 더 빠른 시점이다. 프로세스가 떠있다고 과거의 프로세스를 죽인다면 실상 서버는 완전히 뜬게 아니라서 서비스가 멈춘다.\n3 try : 해당 포트로 확인하기 세번째로는 포트가 열려있는 것으로 확인을 했다. jar를 실행시키자마자 생기는 프로세스보다는 포트가 뒤늦게 열리지만 포트 또한 서버가 완전히 뜨는 여부와는 상관이 없이 일찍 열리기 때문에, 포트만 믿고 과거의 프로세스를 죽이면 서비스가 멈춘다.\n4 try : 내부로 요청을 보내기 어쩌지 고민을 하다가 외부로 요청을 보내면 이전에 떠있는 A서버가 처리를 했는지 B서버가 처리를 했는지 몰라서 무용지물이었다.\n그런데 알고보니 curl은 외부에 있는 nginx를 거쳐서 내부의 서버에 요청을 하는 것 뿐만 아니라 localhost와 port 로 내부에서 내부의 서버에 요청이 가능했다!! 이 방법으로\n(1) 내가 원하는 포트로\n(2) 응답이 올때까지 요청을 보내고\n(3) 응답이 오면 이전의 서버를 죽일 수 있었다.\n결론 이런식으로 쉘스크립트를 이용해서 업데이트 배포를 하면서 서비스는 죽지 않게 무중단으로 운영될 수 있는 환경을 만들었다. 한국에서는 무중단 배포라고 하지만 영어로는 blue green deploy 라고 한다.\n그리고 팀장님이 spring의 context event를 써보면 어떻겠냐고 하셨는데 무중단 배포를 위한 기능은 아닌 것 처럼 보였다. 그래도 spring event가 굉장히 유용한 기능이기에 링크를 남겨 나중에 보기로 했다.\n[Spring Event] Update in 2022 해당 포스팅은 AWS ec2를 하나를 사용하고 그 안에서 앱 서버를 여러 개 띄워서 무중단 배포를 하는 형식이었다.\n하지만 ec2를 하나만 사용하는 경우는 거의 없고, AWS EKS를 사용하여 오케스트레이션을 사용하여 무중단 배포를 하는 형식이 대다수이다. EKS는 AWS에서 제공하는 쿠버네티스로, 평소에 쿠버네티스의 제일 작은 단위인 파드를 여러 개 띄워두고 순차적으로 배포하는 형태이다.\n","date":"2021-03-15","permalink":"https://leeleelee3264.github.io/post/2021-03-15-blue-green-deploy/","tags":["Project"],"title":"[Project] Shell Script로 무중단 배포 흉내내보기"},{"content":"\nRaoul-Gabriel Urma 의 저서 [Modern Java in Action]을 요약한다. Chapter 1에서는 Java 8의 새로운 트랜드를 다룬다. Chapter 2에서는 behavior parameterization를 다루며, Chapter 3에서는 lambda를 다룬다.\nIndex\nChapter 1 Java 8, 9, 10, 11 무슨 일이 일어나고 있는가? Chapter 2 동작 파라미터화 코드 전달하기 Chapter 3 람다 표현식 Chapter 1 Java 8, 9, 10, 11 무슨 일이 일어나고 있는가? 지금까지의 대부분의 자바 프로그램은 코어 중 하나만을 사용했다. 자바 8이 등장하기 이전에는 나머지 코어를 활용하려면 스레드를 사용하는 것이 좋다고 조언했으나 스레드를 사용하면 관리하기 어렵고 많은 문제가 발생할 수 있다. 자바는 이러한 병렬 실행 환경을 쉽게 관리하고 에러가 덜 발생하는 방향으로 진화하려고 노력했다. 자바 9에서는 리액티브 프로그래밍이라는 병렬 실행 기법을 지원한다.\n자바 8은 간결한 코드, 멀티고어 프로세서의 쉬운 활용이라는 두 가지 요구사항을 기반으로 한다. 자바 8은 데이터베이스 질의 언어에서 표현식을 처리하는 것처럼 병렬 연산을 지원하는 스트림이라는 새로운 API를 제공한다. 즉, 스트림을 이용하면 에러를 자주 일으키며 멀티코어 CPU를 이용하는 것보다 비용이 훨씬 비싼 키워드 synchronized 를 사용하지 않아도 된다.\n자바 8의 핵심 기능\n스트림 API 코드를 전달하는 간결 기법 AKA 동작 파라미터화 인터페이스의 디폴트 메서드 예전이라면 복잡해보이는 익명클래스를 이용해서 동작이 담긴 코드를 넘겼지만, 메서드에 코드를 전달하는 기법을 이용하면 새롭고 간결한 방법으로 동작 파라미터화를 구현할 수 있다. 또한 자바 8은 객체지향과 정반대의 개념에 있는 함수형 프로그래밍에서 위력을 발휘한다. 코드를 전달하고, 조합을 하는 등의 특성은 함수형프로그래밍이다.\n왜 아직도 자바는 변하는가? 언어는 필요성에 따라서 만들어지고 도태된다. 예를 들어 C와 C++은 프로그래밍 안전성이 부족해 바이러스가 침투하기 쉬우나 런타임 풋프린트가 적어서 다영한 임베디드 시스템에서 인기를 끌고 있다.\n자바의 과거 자바는 시작부터 스레드와 락을 이용한 동시성을 지원하고, 유용한 라이브러리도 많이 가지고 있었다. 코드를 JVM 바이트 코드로 컴파일 하는 특징 때문에 (모든 브라우저는 가상머신 코드를 지원) 인터넷 애플릿 프로그램의 주요 언어가 되었다.\n자바의 미래 프로그램 생태계는 빅테이터라는 도전에 직면하면서 멀티코어 컴퓨터나 컴퓨팅 클러스터를 이용해 빅데이터를 효과적으로 처리할 필요성이 커졌다. 즉, 병렬 프로세싱을 이용해야 했고, 이는 자바에 부족한 기술이었다.\n새로운 하드웨어, 새로운 프로그래밍이 등장하는 것처럼 기후가 변하고 식물에 영향을 미치면서 기존 식물을 대신해서 새로운 식물을 길러햐 하는 것처럼 새로운 프로젝트에는 다른 언어를 선택해야 하고, 자바는 또 선택을 받기 위해 노력해야 한다.\n자바 8의 밑바탕이 된 설계 스트림 처리 스트림이란 한 번에 한 개씩 만들어지는 연속적인 데이터 항목들의 모임이다. 이론적인 프로그램은 입력 스트림에서 데이터를 한 개 읽어들이며 마찬가지로 출력 스트림으로 데이터를 한 개씩 기록한다. 우선은 스트림 API가 공장의 조립 라인처럼 어떤 항목을 연속으로 제공하는 어떤 기능이라고 단순하게 생각하자.\n유닉스가 명령을 스트림으로 처리하는 대표적인 예시이다. 스트림으로 처리하기 때문에 cat과 tr 등의 앞의 명령어가 파일을 끝까지 처리하고 있지 않아도 sort나 tail 이 작동할 수 있다.\n유닉스 스트림 처리 예시\n1 cat file1 file2 | tr \u0026#34;[A-Z]\u0026#34; \u0026#34;[a-z]\u0026#34; | sort | tail -3 자바 또한 유닉스가 복잡한 파이프라인을 만드는 것처럼 많은 메서드를 지원하는데 중요한 건 딱 하나다! 우리가 원하는 쿼리를 실행하기 위해 SQL문을 돌리고 밑에서는 C로 어떤 일이 일어나는지 전혀 모르는 것처럼, 우리가 스트림 API를 쓴다면 밑에서는 무슨 일이 일어나는지 전혀 모르면서 스트림 형태의 기능을 쓸 수 있다.\n또한 스트림의 가장 큰 장점은 우리가 조금의 변경사항을 준다면 작업을 여러 CPU 코어에 쉽게 할당해서 병렬성을 얻으면서도 스레드라는 복잡한 작업을 하지 않아도 된다.\n동작 파라미터화로 메서드에 코드 전달하기 이전에도 익명 클래스로 코드를 전달할 수는 있었으나 너무 복잡했다. 코드를 넘긴다는 개념이 잘 안 와닿는다면 이렇게 sorting해주세요~ 하고 컨디션 값 하나만 넘기는게 아니라 아예 sorting을 해주는 코드 자체를 넘겨준다고 생각하면 된다!\n익명 클래스 예시\n1 2 3 4 5 6 // 대표적인 익명 클래스 형식 Collections.sort(inventory, new Comparator\u0026lt;Apple\u0026gt;() { public int compare(Apple al, Appple a2) { return al.getWeight().compareTo(a2.getWeight()); } }); 병렬성과 공유 가변 데이터 스트림 메서드로 전달하는 코드는 다른 코드와 동시에 실행하더라도 안전하게 실행될 수 있어야 한다. 다른 코드와 동시에 실행 하더라도 안전하게 실행할 수 있는 코드를 만들려면 공유된 가변 데이터에 접근하지 않아야 한다. 두 프로세스가 공유된 변수를 동시에 바꾸려하면 어떻게 될지 생각해보라.\n기존처럼 어렵게 synchronized를 이용해서 공유된 가변 데이터를 보호하는 규칙을 만들 수 는 있지만 자바 스트림 API는 자바 스레드 API보다 설정 몇개로 더 쉽게 병렬성을 활용할 수 있다.\nChapter 2 동작 파라미터화 코드 전달하기 동작 파라미터화란 아직은 어떻게 실행할 것인지 결정하지 않은 코드 블록을 의미한다. 이 코드 블록은 나중에 프로그램에서 호출한다. (람다로 넘겨주게 된다)\nChapter2에서는 동작 파라미터화 aka 함수 전달하기가 어떤 과정으로 진화가 되어왔나를 보여주고 있다. 함수 자체를 넘기는 이유는 이게 더 자잘하게 플래그 넘기는 것 보다 확장성이 있기 때문. 어쨌든 중요한 점은 시시각각 변하는 사용자 요구 사항에 비용이 가장 최소화 될 수 있는 것!\n동작 전달 발전 단계\n플래그로 동작 분기 (최악) 함수형 인터페이스 익명 클래스 람다 플래그로 동작 분기 플래그 동작 분기 예시\n1 2 3 4 5 6 7 8 9 10 11 12 List\u0026lt;Apple\u0026gt; filterApples(List\u0026lt;Apple\u0026gt; apples, Color color, int weight, boolean flag) { List\u0026lt;Apple\u0026gt; result = new ArrayList\u0026lt;\u0026gt;(); for(Apple apple : apples) { if((flag \u0026amp;\u0026amp; apple.getColor().equals(color)) || (!flag \u0026amp;\u0026amp; apple.getWeight() \u0026gt; weight)) { result.add(apple); } } return result; } 실전에서는 이런 방법을 쓰면 안된다. true와 flase가 뭘 의미하는지도 모르겠고 앞으로 필터링 요구 사항이 추가되면 확장을 하기도 정말 힘들다. 이렇게 변수 하나로 분기를 하고 동작을 달리 하는 것보다 기본 동작은 똑같고 메인이 되는 (여기서는 필터링 조건 등등) 동작 자체를 다르게 하는 게 효율적이다. 이게 바로 동작 파라메터화 이다.\n함수형 인터페이스 요구 상황에 따라 출력을 해야 하는 정보가 다른 메서드가 있다. 아까와는 달리 값을 이해하기 어려운 플래그로 동작을 분기하지 않고 함수형 인터페이스를 넘겨줘서 동작을 달리해보았다.\n함수형 인터페이스 예시\n이런 형태를 전략 디자인 패턴 Strategy design pattern 이라고 한다. 전략 디자인 패턴은 각 알고리즘을 캡슐화하는 알고리즘 패밀리(구현)들을 정의해둔 다음에 런타임에 알고리즘을 선택하는 기법이다.\n예제에서는 컬렉션 탐색 로직과 적용할 동작을 분리했다! 이렇게 하면 한 메서드가 다른 동작을 수행하도록 재활용을 할 수 있다. → 캡슐화\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 interface PrintPredicate { String print(Apple apple); } class PrintWeightPredicate implements PrintPredicate { @Override public String print(Apple apple) { return Integer.toString(apple.getWeight()); } } class PrintHeavyLightPredicate implements PrintPredicate { int middleWeight = 20; static final String APPLE_HEAVY = \u0026#34;heavy\u0026#34;; static final String APPLE_LIGHT = \u0026#34;light\u0026#34;; @Override public String print(Apple apple) { if(apple.getWeight() \u0026gt; middleWeight) { return APPLE_HEAVY; } return APPLE_LIGHT; } } public class Quiz2Dash1 { public static void prettyPrintApple(List\u0026lt;Apple\u0026gt; inventory, PrintPredicate p) { for(Apple apple : inventory) { String output = p.print(apple); System.out.println(output); } } public static void main(String[] args) { Apple one = new Apple(Color.GREEN, 100); Apple two = new Apple(Color.RED, 20); Apple three = new Apple(Color.RED, 10); List\u0026lt;Apple\u0026gt; apples = Arrays.asList(one, two, three); prettyPrintApple(apples, new PrintHeavyLightPredicate()); prettyPrintApple(apples, new PrintWeightPredicate()); String test = \u0026#34;test\u0026#34;; } } 코드 상세 설명\n일단 오직 하나의 추상 메서드만 있는 인터페이스(함수형 인터페이스)를 선언한다. (line 1) 각자 요구 사항에 맞춰 인터페이스를 구현한 클래스들을 만든다. (line 5, 13) 이 인터페이스를 파라메터로 (동작 파라메터) 받는 메서드를 만든다. (line 31) 실제로 실행 단에서 이 메소드를 호출하며 동시에 동작을 담고 있는 클래스를 던져준다. (line 46, 47) 대표적인 함수형 인터페이스로는 정렬을 하는 Comparator의 sort, 쓰레드를 이용해서 코드 블록을 실행하는 Runnable의 run, 메소드를 호출하는 Callable의 call이 있다.\n익명 클래스를 이용 아무래도 위의 방법은 인터페이스를 만들고 구현하는 등 자질구레한 일들이 굉장히 많아서 귀찮다! 이를 조금이라도 줄이고자 자바는 클래스 선언과 인스턴스화를 동시에 수행하는 익명 클래스를 만들었다.\n익명클래스는 자바의 local class와 비슷한 개념이다. 이름이 없는 클래스로, 이를 이용하면 클래스 선언과 인스턴스화를 동시에 할 수 있고 즉석에서 필요한 구현을 만들어 사용할 수 있다.\n익명 클래스 예시\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 prettyPrintApple(apples, new PrintPredicate() { @Override public String print(Apple apple) { return Integer.toString(apple.getWeight()); } }); prettyPrintApple(apples, new PrintPredicate() { int middleWeight = 20; static final String APPLE_HEAVY = \u0026#34;heavy\u0026#34;; static final String APPLE_LIGHT = \u0026#34;light\u0026#34;; @Override public String print(Apple apple) { if(apple.getWeight() \u0026gt; middleWeight) { return APPLE_HEAVY; } return APPLE_LIGHT; } }); 이렇게 하면 클래스들을 만들고 인스턴스로 만든 다음에 쓰는 과정을 확 줄일 수는 있는데 딱 한 번을 사용할 수 있고 또 쓰려면 익명 클래스로 또 만들어줘야 해서 만약 반복 사용을 한다고 하면 좋은 방법은 아닌 것 같다. 그리고 익명 클래스의 형태가 익숙하지 않은 사람들도 많다.\n람다 표현식 이용 람다 예시\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 // 람다로 던져보기 prettyPrintApple(apples, (Apple apple) -\u0026gt; Integer.toString(apple.getWeight()) ); prettyPrintApple(apples, (Apple apple) -\u0026gt; { int middleWeight = 20; String APPLE_HEAVY = \u0026#34;heavy\u0026#34;; String APPLE_LIGHT = \u0026#34;light\u0026#34;; if(apple.getWeight() \u0026gt; middleWeight) { return APPLE_HEAVY; } return APPLE_LIGHT; }); 내가 만든 예제는 람다를 던져도 극단적으로 간단해 보이지는 않는데 람다를 사용하면 명시적으로 클래스를 안 만들어도 되기 때문에 훨씬 더 간단한 형태로 보인다.\n결론 코드 전달 기법을 이용하면 동작을 메서드의 인수로 전달할 수 있다. 자바 8이전에는 이런 작업을 하고 싶다면 상당수의 코드가 추가되었다. 익명 클래스가 있다고 해도 인터페이스를 상속받아 여러 클래스를 구현해야 하는 수고는 여전했는데 람다로 이걸 해결했다.\nChapter 3 람다 표현식 람다를 쓰는 이유는 익명 클래스로 다양한 동작을 구현할 수 있지만 만족할 만큼 코드가 깔끔하지 않아서다.\n아무튼 중요한 사실은 익명클래스도 람다도 코드를 인수로 전달 할 수 있는 귀중한 기능이다. 더 정확하게는 람다 표현식은 메서드로 전달할 수 있는 익명 함수를 단순화한 것이라고 할 수 있다. 람다 표현식은 이름이 없지만, 파라미터 리스트, 바디, 반환 형식, 발생할 수 있는 예외 리스트는 가질 수 있다.\n람다의 특징 익명: 이름이 없어서 익명이다. 메서드를 안 만드니 작명 걱정도 안하고 구현할 코드에 대해 걱정이 줄어든다. 함수: 람다는 메서드처럼 특정 클래스에 종속이 안되어서 그냥 함수라고 부른다. 전달: 람다는 코드를 인수로 전달하는 귀중한 기능이다. 간결성: 익명 클래스보다 훨씬 보기 쉽고 깔끔, 자질구레한 것이 많이 줄었다. 람다의 구조 [Picture 1] 람다의 구조 Argument: 파라메터의 리스트, Array Token: → 모양은 람다의 파라미터 리스트와 바디를 구분하는 용도이고, Statements: -\u0026gt; 뒤가 람다의 바디이다. 람다의 바디를 {}로 감쌓기도 하는데 나중가면 저렇게 싼 형태가 더 알아보기 힘들다고 한다. 람다 표현식에는 return이 함축되어있어서 웬만하면 return을 직접 명시하지 않는다. expression과 statements의 차이 무슨 차이가 있나 했더니 statement가 최종 값을 넣기 때문에 완성된 형태라서 expression을 포함하고 있다.\n예시\n1 2 3 4 5 6 7 8 9 (param) -\u0026gt; expression (param) -\u0026gt; { statements; } // expression 표현식 b + 1 // statements 구문 a = b + 1; // expression statement a++; 어디에, 어떻게 람다를 사용하나? 람다를 사용할 수 있는 부분은 함수형 인터페이스를 파라메터로 받는 자리이다. 함수형 인터페이스란 오직 하나의 추상메서드만 가지고 있는 인터페이스이다. 예시의 코드처럼 @FunctionalInterface 어노테이션이 함께 있다.\n람다 표현식으로 함수형 인터페이스의 추상 메서드 구현을 직접 전달할 수 있으므로 전체 표현식을 함수형 인터페이스의 인스턴스로 취급한다. 따지고보면 함수형 인터페이스를 구현한 클래스의 인스턴스로 취급하는 것이다.\n함수형 인터페이스 예시\n1 2 3 4 5 6 7 8 9 10 11 @FunctionalInterface public interface Predicate\u0026lt;T\u0026gt; { /** * Evaluates this predicate on the given argument. * * @param t the input argument * @return {@code true} if the input argument matches the predicate, * otherwise {@code false} */ boolean test(T t); 람다가 이름도 없이 그냥 띡 던져놓기만 해도 관련 메소드를 착착 만들 수 있었던 이유는.. 함수형 메서드가 가지고 있는 메서드는 하나니까 그 메소드를 만들어서 던지겠지 하는 것도 있고 람다가 던지는 파라메터 리스트, 반환 값을 맞춰본다.\n람다 활용: 실행 어라운드 패턴 자원 처리(데이터 베이스, 파일 등등)에 사용하는 순환 패턴은 자원을 열고, 처리한 다음에 자원을 닫는 순서로 이루어진다. 이걸 실행 어라운드 패턴이라고 한다.\n여기서 어떻게 람다를 활용하나? 어차피 실행부를 감쌓는 자원열고닫기 부분은 동일하다. 실행부에서 뭘 하는지가 중요한데 그 실행부를 동작 파라메터화를 시키고, 실제로 호출을 할 때 내 입맛에 맞게 람다로 만든 표현식을 던지면 일이 간단해진다.\n실행 어라운드 예시\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 @FunctionalInterface interface FileProcess { void work (BufferedReader b); } public class ExFile { static void processFile(FileProcess p) { // try-with-resource 형태라서 따로 자원을 닫을 필요가 없어졌다! try (BufferedReader reader = new BufferedReader(new FileReader(\u0026#34;C:\\\\Temp\\\\file1.txt\u0026#34;))) { p.work(reader); } catch (Exception e){ System.out.println(e.getMessage()); } } public static void main(String[] args) { // 1그냥 파일 읽기 processFile((BufferedReader r) -\u0026gt; { try { System.out.println(r.readLine()); } catch (IOException e) { e.printStackTrace(); } }); // 2 파일 내용 복사하기 processFile((BufferedReader r ) -\u0026gt; { try (BufferedWriter r2 = new BufferedWriter(new FileWriter(\u0026#34;C:\\\\Temp\\\\file2.txt\u0026#34;))) { r2.write(r.readLine()); } catch (IOException e) { e.printStackTrace(); } }); } } 코드 상세 설명\n함수형 인터페이스를 만든다. (line 1) 함수형 인터페이스를 파라메터로 받는 재활용 메서드 만든다. (line 8) 재활용 메서드 안에서 함수형 인터페이스의 단 하나 뿐인 추상메서드 호한다. (line 12) 실제 실행 단에서 재활용 메서드 호출 할 때 원하는 행동 람다로 만들어서 던진다. (line 21, 30) 책에서 말한 것처럼 실행어라운드는 동일하게 사용하고 자원을 실제로 어떻게 사용할 것인지는 람다로 전달해 내 마음대로 실행을 했다. try-with-resource 형식을 잘 기억하자. 이걸 그대로 사용하면 매번 따로 자원을 닫아주는 번거로운 일을 하지 않아도 된다. 그냥 자원을 열 때 try () 안에서 연다는 걸 알면 된다.\n함수형 인터페이스 사용 함수형 인터페이스의 추상 메서드는 람다 표현식의 시그니처 (반환값, 파라메터값)을 묘사한다. 함수형 인터페이스의 추상 메서드 시그니처를 함수 디스크립터라고 한다.\n앞에서도 말한 것처럼 람다는 함수형 인터페이스에서 사용이 가능한데 자바는 이미 다양한 형태의 built-in 함수 인터페이스를 상황별로 만들어놨고 우리는 필요에 따라서 반환값과 파라메터값을 고려해서 쓰기만 하면 된다. (Comparable, Runnable, Callable 등등)\n[Picture 2] built-in 함수형 인터페이스 Predicate 사용 예시\n객체를 사용해서 boolean 값 return, 한마디로 true/false 값 나누고 싶을 때 사용한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 public class ExPredicate { // 맨 앞 \u0026lt;T\u0026gt; 왜 있나 했는데 filter 에서 쓰이는 T를 정의해주는 느낌이다 public static \u0026lt;T\u0026gt; List\u0026lt;T\u0026gt; filter(List\u0026lt;T\u0026gt; list, Predicate\u0026lt;T\u0026gt; p) { List\u0026lt;T\u0026gt; results = new ArrayList\u0026lt;\u0026gt;(); for(T t: list) { if(p.test(t)) { results.add(t); } } return results; } // 람다의 준비물 // 1.functional method // 2.recycle method // 3.call recycle method with lambda public static void main(String[] args) { List\u0026lt;String\u0026gt; sample = Arrays.asList(\u0026#34;longlong\u0026#34;, \u0026#34;short\u0026#34;, \u0026#34;\u0026#34;); List\u0026lt;String\u0026gt; notShort = filter(sample, (String s) -\u0026gt; s.length() \u0026gt; 3); System.out.println(notShort); IntPredicate same = (int i) -\u0026gt; i == 1000; Predicate\u0026lt;Integer\u0026gt; sameBoxing = (Integer i)-\u0026gt; i == (new Integer(1000)); System.out.println(same.test(1000)); System.out.println(sameBoxing.test(1000)); } } Consumer 사용 예시\n객체를 인수로 받아서 어떤 행동을 하고 싶을 때 사용한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 public class ExConsumer { // Consumer 은 반환값이 없어서 뭔가를 실행하고자 할 때 사용한다 public static \u0026lt;T\u0026gt; void doSomething(List\u0026lt;T\u0026gt; list, Consumer\u0026lt;T\u0026gt; c) { for(T t: list) { c.accept(t); } } public static void main(String[] args) { List\u0026lt;Integer\u0026gt; test = Arrays.asList(1,2,3); doSomething(test, (Integer i) -\u0026gt; System.out.println(i/2)); } } Function 사용 예시\n객체를 인수로 받아서 또 다른 타입의 객체를 반환할 때 사용한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 public class ExFunction { // Function 은 입력 값도 있고 반환 값도 있는 형식이다. static \u0026lt;T, R\u0026gt; Map\u0026lt;T, R\u0026gt; map(List\u0026lt;T\u0026gt; list, Function\u0026lt;T, R\u0026gt; f) { Map\u0026lt;T, R\u0026gt; valueMap = new HashMap\u0026lt;\u0026gt;(); for(T t:list) { valueMap.put(t, f.apply(t)); } return valueMap; } public static void main(String[] args) { List\u0026lt;String\u0026gt; test = Arrays.asList(\u0026#34;one\u0026#34;, \u0026#34;two\u0026#34;, \u0026#34;Three\u0026#34;); Map\u0026lt;String, Integer\u0026gt; testLen = map(test, String::length); System.out.println(testLen); } } 기본형 특화 (IntPredicate, IntSupplier etc)\n기본형 값들을 위의 메소드에 사용하려면 참조형만 값으로 받는 제네릭의 특성 때문에 Integer, Double등으로 박싱을 해야 한다. 최신 자바는 오토박싱을 해주기는 하는데 박싱은 객체라서 힙에 차곡차곡 쌓이게 된다. 결국 메모리를 더 쓰게 되고, 기본형을 가져올때도 메모리 탐색을 해야 한다.\n이것이 모이면 결국 자원낭비라서 그냥 기본형으로 함수형인터페이스를 사용한다면 기본형 특화를 쓰는게 좋다.\n형식 검사, 형식 추론, 제약 다이아몬드 연산자 람다 표현식으로 함수형 인터페이스의 인스턴스를 만드는 것은 일종의 추론이라고 할 수 있다. 함수형 인터페이스의 유일한 추상 메서드의 파라메터 값과 반환값 형태와 동일한 람다 형식을 쓰면 아~ 그건가보다~ 하고 납득을 하기 때문에.\n그런데 이런 경우가 하나 더 있다. 바로 다이아몬드 연산자 이다.\n다이아몬드 연산자 예시\n1 2 List\u0026lt;String\u0026gt; listOfStrings = new ArrayList\u0026lt;\u0026gt;(); List\u0026lt;Integer\u0026gt; listOfIntegers = new ArrayList\u0026lt;\u0026gt;(); 이렇게 다이아몬드 연산자를 사용해서 콘텍스트에 따라 제네릭 형식을 추론하고 있다.\n형식추론 이미 간단한 형태의 람다를 더 간단한 형태로 만들수 있다면? 람다 생성은 완전히 자바 컴파일러가 추론을 하는 형태로 이루어져있다. 결과적으로 컴파일러는 람다 표현식의 파라미터 형식에 접근을 할 수 있기 때문에 람다 문법에서는 파라미터 타입을 생략해도 된다.\n파라메터 타입 생략 예시\n1 2 3 4 5 6 7 8 9 10 11 12 public static void main(String[] args) { List\u0026lt;Integer\u0026gt; test = Arrays.asList(1,2,3); doSomething(test, (Integer i) -\u0026gt; System.out.println(i/2)); } // 파라미터 제거 public static void main(String[] args) { List\u0026lt;Integer\u0026gt; test = Arrays.asList(1,2,3); doSomething(test, (i) -\u0026gt; System.out.println(i/2)); } 두 개의 코드는 완전히 동일한 코드이다. 상황에 따라 명시적으로 형식을 포함하는 것이 좋을 때도 있고 형식을 배제하는 것이 가독성을 향상시킬 때고 있다. 어떤 방법을 해야 할지는 역시나 개발자 스스로가 직접 판단을 해야 한다.\n제약: 지역 변수 사용 람다 표현식에서는 다른 메서드들과 마찬가지로 자유 변수(파라메터로 넘겨진 변수 말고 외부에서 선언이 된 변수)를 활용할 수 있다. → 람다 캡쳐링\n그런데 이렇게 람다에서 외부에서 선언된 변수에 접근을 하려면 그 변수는 딱 한 번 만 값을 넣을 수 있는 final 변수여야 한다. 이름에서부터 알 수 있는 것처럼 \u0026lsquo;캡처링\u0026rsquo;이니까 값이 바뀌면 캡쳐의 개념이 아니게 되는 것이다.\n아니면 Effective final 이라고 그냥 그 변수가 선언이 되고 딱 한 번만 할당이 되었으면 final이라고 명시를 안 해도 컴파일러가 알아서 final 처럼 취급을 해준다. 이런 제약이 있는 이유는 인스턴스 변수와 지역 변수가 태생부터 다르기 때문이다.\n인스턴스 변수 VS 지역 변수\n인스턴스 변수: 힙에 저장이 된다. (조금 더 오래 살아남는 값들이 힙에 저장이 되는 편) 지역변수: 스택에 저장이 된다. (기본타입과, 참조타입들의 이름들이 여기에 저장이 되는 편) 특정 메소드에서 사용이 되는 변수가 지역변수이다. 얘들은 메소드가 호출이 되었을 때 스택에 값이 차곡차곡 쌓였다가 메소드 연산이 끝나면 순서대로 다 팝팝팝 해서 터진다.\n람다와 call-by-value 람다에서 접근을 하는 외부 선언 변수는 람다가 그 외부 선언 변수 값 자체에 접근을 하는 게 아니고, 외부 선언 변수 값의 복사본에 접근을 하는 것이다. (call-by-value 형식이라고 할 수 있음). 그래서 그 복사본의 값이 바뀌지 않아야 해서 값을 한 번 만 할당하게 하는 것이다.\n애초에 이런 형식으로 된 이유는 스레드 세이프하게 만들기 위해서이다. 지역변수 값은 스택에 존재하기 때문에 자신을 정의한 스레드와 생존을 같이 해야 한다. 람다가 스레드 A 에서 실행된다면 변수를 할당한 다른 스레드 B가 사라져버려 변수 할당이 해제 되었는데도 람다에서는 계속 그 변수에 접근을 하려고 할 수 있다.\n따라서, 람다는 자신이 정의된 메서드의 지역 변수의 값은 바꿀 수 없다. 람다가 정의된 메서드의 지역 변수값은 final 변수기 때문에 람다는 변수가 아닌 값에 국한되어 어떤 동작들을 수행한다.\n참고하면 좋을 자료들 [자바 메모리 관리 - 스택 \u0026amp; 힙] [JVM의 메모리 구조 및 할당과정] [Stack Memory and Heap Space in Java | Baeldung] 메서드 참조 메서드 참조를 이용하면 기존의 메서드 정의를 재활용해서 람다처럼 전달을 할 수 있다. (동작 파라메터). 때때로 람다 표현식을 쓰는 것보다 메서드 참조를 사용하는 게 더 가독성이 좋고 자연스러울 수 있다. 왜냐하면 메서드 참조는 어떤클래스.어떤메소드 형태로 넘기기 때문에 확실히 명시된다는 느낌이 있다.\n메서드 참조 예시\n1 2 3 4 5 6 7 8 9 10 11 12 static List\u0026lt;Apple\u0026gt; filterApples(List\u0026lt;Apple\u0026gt; inventory, Predicate\u0026lt;Apple\u0026gt; p) { List\u0026lt;Apple\u0026gt; result = new ArrayList\u0026lt;\u0026gt;(); for(Apple apple: inventory) { if(p.test(apple)) { result.add(apple); } } return result; } List\u0026lt;Apple\u0026gt; result1 = filterApples(apples, Apple::isHeavyApple); List\u0026lt;Apple\u0026gt; result2 = filterApples(apples, Apple::isGreenApple); 어떤 메소드를 실행해야 하는지 대놓고 이름을 가져와서 보여주고 있다. 훨씬 더 명시된 모습. 그런데 여기서 실제로 메서드를 호출하는 건 아니고 람다에서 이렇게이렇게 동작을 하세요! 하고 코드를 짜서 넘겨주는 것처럼 실제 실행은 다른 곳에서 하기 때문에 메서드 참조를 할 때 메서드 뒤에 ()을 쓰지 않아도 된다.\n메서드 참조 유형\n정적 메서드 참조 Integer::parseInt (parseInt는 static 메서드) 인스턴스 메서드 참조 String::length (length는 일반 메서드) 기존 객체의 인스턴스 메서드 참조 기존 객체의 인스턴스 메서드 참조 예시\n1 2 String test = new String(\u0026#34;hello\u0026#34;); someMethod(test::length); 3번의 형태를 쓰는 경우는 람다 외부에서 (final로) 생성이 된 변수들의 메서드를 람다 안에서 사용을 할 때 쓰는 형태라고 한다. 이런식은 클래스 안에 private 하게 전용 핼퍼 메서드를 정의한 상황에서 유용하게 쓸 수 있다고 한다.\n컴파일러는 람다 표현식의 형식을 검사하던 방식과 비슷한 과정으로 메서드 참조가 주어진 함수형 인터페이스와 호환하는지 확인한다. 메서드 참조는 콘텍스트의 형식과 일치해야 한다.\n생성자 참조 예시\n1 2 Supplier\u0026lt;Apple\u0026gt; c1 = Apple:new; Apple a1 = c1.get(); Map으로 카테고리별로 생성자 참조를 해놔서 값 맵핑해서 필요한 인수들이 다 다른 상태에서 객체 만드는 건 좋아보인다. 객체를 생성할 때에는 자바 built-in 함수인터페이스인 Function이나 BiFunction를 사용하면 좋을 것 같다.\n람다 표현식을 조합할 수 있는 유용한 메서드 여러가지 유틸성 함수형 메서드들을 조합해서 사용을 할 수 있다. 한 마다로 간단한 여러개의 람다 표현식을 조합해서 복잡한 람다 표현식을 만들 수 있다.\n함수형 인터페이스가 이렇게 만능으로 구현이 될 수 있는 이유는 default메서드로 만들어져있기 때문이다. 미래를 생각해서 이것저것 다 넣어놓고 구현 안 해도 되는 default를 추가한 것 같다. 이는 결국 내가 함수형 메서드를 만들었을 때도 미래지향적으로 핼퍼 메서드를 이거저거 만들어 놓으면 연결연결연결 해서 쓸 수 있다는 얘기다.\n심지어 이렇게 람다 표현식을 복잡하게 조합을 해도 코드 자체가 문제를 잘 설명한다고 한다.\nComperator 연결 예시\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 // 역정렬 inventory.sort(comparing(Apple:getWeight).reversed()); /** * Returns a comparator that imposes the reverse ordering of this * comparator. * * @return a comparator that imposes the reverse ordering of this * comparator. * @since 1.8 */ default Comparator\u0026lt;T\u0026gt; reversed() { return Collections.reverseOrder(this); } // 값이 같다면 다른 조건 하나 더 넣어서 비교 inventory.sort(comparing(Apple:getWeight).reversed() .thenComparing(Apple:getCountry)); default Comparator\u0026lt;T\u0026gt; thenComparing(Comparator\u0026lt;? super T\u0026gt; other) { Objects.requireNonNull(other); return (Comparator\u0026lt;T\u0026gt; \u0026amp; Serializable) (c1, c2) -\u0026gt; { int res = compare(c1, c2); return (res != 0) ? res : other.compare(c1, c2); }; } Predicate 연결 예시\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 // and 조건 Predicate\u0026lt;Apple\u0026gt; redAntHeavyApple = redApple.and(apple -\u0026gt; apple.getWeight() \u0026gt;150); // or 조건 Predicate\u0026lt;Apple\u0026gt; redAndHeavyAppleOrGreen = readApple.and(apple -\u0026gt; apple.getWeight() \u0026gt; 150) .or(apple -\u0026gt; GREEN.equals(a.getColor())); default Predicate\u0026lt;T\u0026gt; or(Predicate\u0026lt;? super T\u0026gt; other) { Objects.requireNonNull(other); return (t) -\u0026gt; test(t) || other.test(t); } default Predicate\u0026lt;T\u0026gt; and(Predicate\u0026lt;? super T\u0026gt; other) { Objects.requireNonNull(other); return (t) -\u0026gt; test(t) \u0026amp;\u0026amp; other.test(t); } Function 연결 예시\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 // andThen 주어진 함수를 먼저 적용한 결과를 다른 함수의 입력으로 전달 Function\u0026lt;Integer, Integer\u0026gt; f = x -\u0026gt; x + 1; Function\u0026lt;Integer, Integer\u0026gt; g = x -\u0026gt; x * 2; Function\u0026lt;Integer, Integer\u0026gt; h = f.andThen(g); int result = h.apply(1); //4가 나온다. // compose 인수로 주어진 함수를 먼저 실행한 다름에 결과를 외부 함수로 전달 Function\u0026lt;Integer, Integer\u0026gt; f = x -\u0026gt; x + 1; Function\u0026lt;Integer, Integer\u0026gt; g = x -\u0026gt; x * 2; Function\u0026lt;Integer, Integer\u0026gt; h = f.compose(g); int result = h.apply(1); //3가 나온다. // 결국 andThen과 compose 의 차이는 앞을 먼저 계산하냐 뒤를 먼저 계산하냐의 차이 default \u0026lt;V\u0026gt; Function\u0026lt;V, R\u0026gt; compose(Function\u0026lt;? super V, ? extends T\u0026gt; before) { Objects.requireNonNull(before); return (V v) -\u0026gt; apply(before.apply(v)); } default \u0026lt;V\u0026gt; Function\u0026lt;T, V\u0026gt; andThen(Function\u0026lt;? super R, ? extends V\u0026gt; after) { Objects.requireNonNull(after); return (T t) -\u0026gt; after.apply(apply(t)); } 이렇게 람다로 호출을 바로 못하고 함수형 메서드로 여러개를 만들어서 체인을 걸어야 하는 이유는 저 메서드들이 함수형 메서드에 들어있는 애들이다 보니까 확실하게 함수형 메서드로 객체 선언이 되어야 한다. 저렇게 만들어놓으면 람다로 던지지 않고 그냥 redAndHeavyApple 이렇게 자체를 던져서 사용을 하면 되겠다.\n실제 호출 예시\n1 2 3 4 5 Predicate\u0026lt;Apple\u0026gt; redApple = apple -\u0026gt; apple.getColor().equals(Color.RED); Predicate\u0026lt;Apple\u0026gt; redAntHeavyApple = redApple.and(apple -\u0026gt; apple.getWeight() \u0026gt;150); List\u0026lt;Apple\u0026gt; finalResult = filterApples(apples, redAntHeavyApple); System.out.println(finalResult); ","date":"2021-02-14","permalink":"https://leeleelee3264.github.io/post/2021-02-14-java-in-action-part1/","tags":["Book"],"title":"[Book] [Modern Java in Action] 노트 정리 - Chapter 1,2,3"},{"content":"\nCheat sheet for Docker. It consists of Dockerfile like FROM, RUN, ENTRYPOINT, VOLUME and Docker command like docker build, docker image, docker run.\nIndex\nDockerFile Cheat Sheet Docker Command Cheat Sheet Dockerfile Cheat Sheet I have to make image before running docker container with the image. How to get images I need? I can just pull images from my/someone else\u0026rsquo;s docker hub repository or I can even make one all by myself.\nDo this, I must write Dockerfile. Dockerfile is just like a recipe for building images.\n[Picture 1] DockerFile Cheat Sheet Docker Command Cheat Sheet Docker is working like linux. It\u0026rsquo;s tiny which contains minimum setting to run server I can tell. I have to type docker dedicated command to manipulate docker container.\nContainer [Picture 2] Container 1 [Picture 3] Continer 2 Debug [Picture 4] Debug Duplicate with k8s [Picture 5] k8s 1 [Picture 6] k8s 2 ","date":"2021-01-30","permalink":"https://leeleelee3264.github.io/post/2021-01-30-docker-cheet-sheet/","tags":["Cheat"],"title":"[Cheat Sheet] (en) Docker Cheat Sheet"},{"content":"\nHandles running Shiny server with Docker.\nIndex\nHow I got into Docker Making Shiny Docker Troubleshooting with Network Reference How I got into Docker Docker make server infra as a code and we can share anywhere!\nNightmare of installing library for Shiny I had wondered why many developers love docker so much for a while. A few days ago, I got a chance to install docker in my company\u0026rsquo;s test server. The thing is that we have shiny server written in R in the test server. I still remember I had to be though install a bunch of library to exec shiny server and in installing process, our server even stop because of lack of CPU and memory. So for me, it\u0026rsquo;s like nightmare to make infra for shiny-server.\nHow I can move Shiny to other host machine? And few month later, we have to move our server (on aws) to bigger one. It means I have to get shiny to the new server too. Finally, I decided using docker for this. Here is what I had been though running shiny in docker. Furthermore, now I can answer the question. It\u0026rsquo;s all about convenience. What make docker so special is that it make server just a program run by code. Writing code (dockerfile) and run it to have an image. The image is program to run server. In addition, we can use this image in any other host server.\nDocker Hub In terms of sharing, there is Docker hub which is docker version of git hub. When I make an image with Dockerfile, then push to my Docker hub. After that, I can pull it in other host. No need to write Dockerfile nor run Dockerfile to make an image. I can use the pulled image right away. I can say I saved a lot of time with making one. Making image in my localhost which prevent stopping the test server because of installing a tons of R library.\nMaking Shiny Docker [Picture 1] is my goal of migration Shiny to Docker.\n[Picture 1] Shiny Docker Blue Print Write Dockerfile Dockerfile\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 FROM rocker/shiny:3.6.3 RUN apt-get update \u0026amp;\u0026amp; apt-get install -y \\ sudo \\ gdebi-core \\ pandoc \\ pandoc-citeproc \\ libcurl4-gnutls-dev \\ libcairo2-dev \\ # normal libmysqlclient-dev does not work default-libmysqlclient-dev \\ libsodium-dev \\ libxt-dev \\ xtail \\ wget # package install # sodium error RUN sudo R -e \u0026#34;install.packages(\u0026#39;sodium\u0026#39;)\u0026#34; # Download and install library RUN sudo R -e \u0026#34;install.packages(c(\u0026#39;shinydashboard\u0026#39;, \u0026#39;shinyjs\u0026#39;, \u0026#39;V8\u0026#39;))\u0026#34; # lib in ui.R RUN sudo R -e \u0026#34;install.packages(c(\u0026#39;shiny\u0026#39;, \u0026#39;lubridate\u0026#39;))\u0026#34; # lib in server.R RUN sudo R -e \u0026#34;install.packages(c(\u0026#39;RMySQL\u0026#39;, \u0026#39;DBI\u0026#39;, \u0026#39;ggplot2\u0026#39;, \u0026#39;scales\u0026#39;, \u0026#39;doBy\u0026#39;, \u0026#39;gridExtra\u0026#39;, \u0026#39;dplyr\u0026#39;, \u0026#39;DT\u0026#39;))\u0026#34; #COPY ./app_by_r /srv/shiny-server RUN chmod -R +r /srv/shiny-server EXPOSE 3838 CMD [\u0026#34;/usr/bin/shiny-server.sh\u0026#34;] Dockerfile detail\nChose Docker Base Image So, this is Dockerfile for my shiny-server. In FROM, we can chose base env for a container we are building. It could be ununtu, centos and even not just server, but server with installation, ubuntu + R for example.\nAt first, I went for R base container, but it didn\u0026rsquo;t work out. To save time, I decided to use other people\u0026rsquo;s docker images. FROM rocker/shiny:3.6.3, it contains basic linux + R + shiny-server.\nInstall Package Now all we have to do are installing linux package to make proper shiny base server and library we used in the project code. In EXPOSE command, it does not mean this container expose its 3838 to outside. It\u0026rsquo;s more like telling me they will be exposed when running docker with -p options.\nCopy or Mount? As you can see, I tried to build image with the source code just like all in one package. In the last part of Dockerfile, it will COPY the source code dir in host and run it in the container. What if this is not complete project and we will change the code many time? With this Dockerfile, we have to make a new image everytime to make updated image. Without doubt, it will take a lot of time. To avoid this problem, I choose mount option.\nRun Docker Command Docker Command\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 # build docker image with Dockerfile # Let\u0026#39;s say we are in the dir which contains Dockerfile # make image with name:tag docker build shiny_try:0.2 . # check our image is maded docker images # run docker with the image # --volume=\u0026#34;local_dir_path:container_dir_path docker run -dp 3838:3838 --volume=\u0026#34;/srv/shiny-server:/srv/shiny-server\u0026#34; shiny_try:0.2 # check docker is running docker ps # see log from running docker container docker logs docker_name # connect to inside of runnign docker container with bash docker exec -it docker_name Docker options -d : run it in background -p : expose port. (host port:container port) -it : make interactive console to control docker container —rm : remove this docker process when it gets stopped Working with docker hub I was surprised that I have to match docker image name with my docker repository name if I want to push the image to the repo. So, I had to change the image name first.\nPush Docker Image to Hub\n# change docker image name docker tag shiny_try:0.1 absinthe4902/shiny_try:0.1 # login to docker hub docker login # push the image docker push absinthe4902/shiny_try:0.1 ######### # when pulling repo docker pull absinthe4902/shiny_try:0.1 And Everything is done! Now we can see the shiny docker container is running in host machine. Finally, all we have to do is configure nginx to let the docker container get request when client send a request to host machine. This is a normal way to run docker container.\nTroubleshooting with Network Sadly, this is not finished\u0026hellip; I did something wrong during the process. It was very small mistake. I think I missed exposing/connecting docker container port to host port. To solve this, I just added -p options. Before this. I had to search about Docker network.\nI was quite confused the meaning of bridge, so I searched the concept first with I understood the concept first with [nat and bridge].\nNAT Getting ip from host Using NAT, the virtual machine\u0026rsquo;s router is host. It let us control access and see deep down level of traffic ake Ip packets and Tcp datagrams. Bridge Getting ip from Router Using Bridge, the virtual machine does nothing with host. Host still see the traffic, but it\u0026rsquo;s not specific. Only Ethernet level. So basically, when I work with nat, my virtual machine get one more depth. In the other hand, bridge makes the virtual machine have the same depth as host, I can see the same subnet between host and virtual machine.\nDocker network structure [Picture 2] Docker Network Structure In Docker container, there are individual network namespace. It will have their own Ip as well. Before exposing port, this normal conatiner is not available from outside of host. (can work inside of host). With -p options, we expose our 3838 port. Let\u0026rsquo;s see what happen.\nMonitor After Exposing Port 3838\n1 2 3 4 5 6 7 # docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 307b38f4fd3c absinthe4902/all-new:0.6 \u0026#34;/usr/bin/shiny-serv…\u0026#34; 5 days ago Up 5 days 0.0.0.0:3838-\u0026gt;3838/tcp jolly_bassi # ps -ef | grep docker-proxy root 17106 26073 0 1월26 ? 00:00:00 /usr/bin/docker-proxy -proto tcp -host-ip 0.0.0.0 -host-port 3838 -container-ip 172.17.0.2 -container-port 3838 sw 20374 15675 0 12:42 pts/0 00:00:00 grep --color=auto docker-proxy There is a secret agent with docker network, docker-proxy.\nDocker Proxy When exposing 3838 port, we will see our 3838 port is active, and secretly docker proxy is listening 3838 port! Docker proxy is the one who pass the request from host to docker container. Each exposed container have one Docker proxy. It\u0026rsquo;s like this flow. Without docker proxy, the request bind by host 3838 cannot be passed to container 3838.\nRouting Flow\nClient → host → Docker proxy (kind of intercept) → docker0 (docker inner network bridge) → container.\nMonitor Chaining Network wit iptables\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 # sudo iptables -t nat -L -n Chain PREROUTING (policy ACCEPT) target prot opt source destination DOCKER all -- 0.0.0.0/0 0.0.0.0/0 ADDRTYPE match dst -type LOCAL Chain INPUT (policy ACCEPT) target prot opt source destination Chain OUTPUT (policy ACCEPT) target prot opt source destination DOCKER all -- 0.0.0.0/0 !127.0.0.0/8 ADDRTYPE match dst -type LOCAL Chain POSTROUTING (policy ACCEPT) target prot opt source destination MASQUERADE all -- 172.17.0.0/16 0.0.0.0/0 MASQUERADE tcp -- 172.17.0.2 172.17.0.2 tcp dpt:3838 Chain DOCKER (2 references) target prot opt source destination RETURN all -- 0.0.0.0/0 0.0.0.0/0 DNAT tcp -- 0.0.0.0/0 0.0.0.0/0 tcp dpt:3838 to:172.17.0.2:3838 Iptables Actually, iptables does this work instead of docker proxy. In some kind of special situations, we cannot depend on iptables, and we should use docker proxy. So it\u0026rsquo;s more like backup. As I said, in special occasion, we have no choice using docker proxy. However, this proxy needs a bit of resource, and it can be disabled as well. It depends on server status and our decision.\nReference [docker proxy] [deep down of docker] [docker network] For myself, Recommend reading \u0026lsquo;deep down of docker\u0026rsquo; one day for myself. It\u0026rsquo;s post about deep inside of docker and I might understand how docker work in linux level.\nNow I have a feeling, I had a huge confusion of docker network. I thought docker with host which is actual network between host and docker. If I want to dig container-container network, I should read Docker official document. [docker network]\n","date":"2021-01-30","permalink":"https://leeleelee3264.github.io/post/2021-01-30-build-server-with-docker/","tags":["Infra"],"title":"[Infra] (en) Running Shiny server with Docker"},{"content":"\nTest Spring boot with Junit5: Repository test, service test, controller test, integration test.\nIndex\nWhy unit test? Implement unit test Why unit test? Making test code is always big burden for me. No time for writing test code, and also don\u0026rsquo;t know how. What is @SpringBootTest and Junit?\nThe main point of making test code has to be united! Do not test whole flow! TBH I\u0026rsquo;ve usually done test e2e way. I thought test with spring boot test code is so heavy that I have to wait for server reload a lot. And now I know it happened because I tried to test entire program. It is just like the server I had written right before the test. My bad.\nThere are three parts of testing code. I\u0026rsquo;ll cover step-by-step. I got a lot of help from [Testing Spring Boot]! Should check before working with test code.\nTest Code Step\nRepository (JPA) test Service test Controller test Integrate test Implement unit test Basic libs for test in spring boot I had no idea how people use assertThat, because I only got assert. It turned out that I have to install additional lib junit. I thought all the test function was built in spring-boot-starter-test. Silly me.\nThere are so many options of test function include assertThat and assert. I\u0026rsquo;m quite sure I do have to check as soon as possible to handle junit test 100%. Take a look at [Junit test function]\njunit5 library\n1 2 3 4 5 testImplementation(\u0026#39;org.springframework.boot:spring-boot-starter-test\u0026#39;) { exclude group: \u0026#39;org.junit.vintage\u0026#39;, module: \u0026#39;junit-vintage-engine\u0026#39; } testCompile group: \u0026#39;org.junit.jupiter\u0026#39;, name: \u0026#39;junit-jupiter-api\u0026#39;, version: \u0026#39;5.7.0\u0026#39; Repository test UserRepository\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 @Repository public interface UsersRepository extends JpaRepository\u0026lt;UsersVO, Long\u0026gt; { List\u0026lt;UsersVO\u0026gt; findByName(String name); List\u0026lt;UsersVO\u0026gt; findByNameLike(String name); /** * jpa update 는 분명 모든 것을 한 큐에 업데이트 하는 그런.. 성격인가봄 * @param id * @param name */ @Modifying @Query(\u0026#34;update users u \u0026#34; + \u0026#34;set u.name = :name \u0026#34; + \u0026#34;where u.id = :id\u0026#34;) void updateName(@Param(value = \u0026#34;id\u0026#34;) long id, @Param(value = \u0026#34;name\u0026#34;) String name); } Test code for UserRepository\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 @ExtendWith(SpringExtension.class) @DataJpaTest @AutoConfigureTestDatabase(replace = AutoConfigureTestDatabase.Replace.NONE) public class UsersRepositoryTest { @Autowired private TestEntityManager entityManager; @Autowired private UsersRepository usersRepository; @Test public void whenSave_thenReturnUsers() { UsersVO jpaEntity = UsersVO.builder() .name(\u0026#34;tryNoDB\u0026#34;) .salary(20000) .build(); entityManager.persist(jpaEntity); entityManager.flush(); UsersVO saved = usersRepository.save(jpaEntity); assertThat(saved.getName()).isEqualTo(jpaEntity.getName()); } @Test public void should_update_name_by_id() { long id = 2; String name = \u0026#34;Jamie\u0026#34;; String notName = \u0026#34;rer\u0026#34;; // entityManager 은 진짜 jpa 에 있는 entity 를 위한거다. // jpa entity 가 아니면 못 쓴다는 소리임임 // entityManager.persist(id); // entityManager.flush();; usersRepository.updateName(id, name); List\u0026lt;UsersVO\u0026gt; updated = usersRepository.findByName(name); assertThat(updated.get(0).getName()).isEqualTo(notName); } } Code detail @ExtendWith(SpringExtension.class) Making a connection between spring boot and junit during test @DataJpaTest Building test env for database. It configured in memory H2 db to divide our real db during the test. TestEntityManager Putting sample data before testing. Because H2 db is empty. Mistake: Test with Real Data I\u0026rsquo;m a very beginner of JPA. I quite know nothing about JPA, but I know persistence is great deal in JPA. To keep this policy we should use EntityManager. I guess it\u0026rsquo;s something like cache before commit to real db.\nAnd I have so many experiences updating our real db record during test. The change didn\u0026rsquo;t go back either. Now I think I know why. I have to use @DataJpaTest to make fake env for db testing. Keep in mind it only takes @Entity object.\nTake a look at [JPA test doc] for more test code for JPA.\nService test UsersService Interface\n1 2 3 4 5 6 public interface UsersService { List\u0026lt;UsersVO\u0026gt; findByName(String name); List\u0026lt;UsersVO\u0026gt; findByNameLike(String name); UsersVO save(UsersVO vo); } UserService Implementation\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 @Slf4j @Service public class UsersServiceImp implements UsersService { @Autowired private UsersRepository usersRepository; @Override public List\u0026lt;UsersVO\u0026gt; findByName(String name) { try { return usersRepository.findByName(name); } catch (Exception e) { log.error(e.getMessage(), e); return Collections.emptyList(); } } @Override public List\u0026lt;UsersVO\u0026gt; findByNameLike(String name) { try { return usersRepository.findByNameLike(name); } catch (Exception e) { log.error(e.getMessage(), e); return Collections.emptyList(); } } @Override public UsersVO save(UsersVO vo) { usersRepository.save(vo); return vo; } } Test Code for UsersService\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 @ExtendWith(SpringExtension.class) class UsersServiceImpTest { // 일단 test code 에서 autowired 하는데 필요한 형식이다 @TestConfiguration static class UsersServiceTestConfiguration { @Bean public UsersService usersService() { return new UsersServiceImp(); } } @Autowired UsersService UsersService; // 이렇게 mock 으로 해두면 진짜 리포지토리 부르는 걸 우회한다. @MockBean private UsersRepository usersRepository; @BeforeEach public void setUp() { UsersVO test = UsersVO.builder() .name(\u0026#34;test_account\u0026#34;) .salary(1000) .build(); Mockito.when(usersRepository.findByName(test.getName())) .thenReturn(Collections.singletonList(test)); } @Test public void whenValidName_thenUsersShouldBeFound() { String name = \u0026#34;test_account\u0026#34;; List\u0026lt;UsersVO\u0026gt; found = UsersService.findByName(name); assertThat(found.get(0).getName()).isEqualTo(name); } @Test void whenSaved_thenUsersShouldBeReturned() { UsersVO test = UsersVO.builder() .name(\u0026#34;dummy_account\u0026#34;) .salary(1000) .build(); UsersVO saved = UsersService.save(test); assertThat(saved.getName()).isEqualTo(test.getName()); } } Service layer injects Repository (aka persistence layer) but in test, service layer don\u0026rsquo;t have to know how it works. It means we have to make full service test code without wiring repository and we can do it with Mocking function. It literally mock repository.\nCode detail @TestConfiguration\nAt first, I couldn\u0026rsquo;t understand why I use the annotation. UsersServiceImp is already made as @Bean and official doc told me the annotation is used to wire UsersServiceImp and work like bean. It\u0026rsquo;s because UserServiceImp is implementation. In normal situation, our smart spring will load UsersServiceImp even though we wrote UsersService but it\u0026rsquo;s not working in the test, so we have to do it manually. @TestConfiguration help the test to do this process. @MockBean\nIn test env, everything is independent. We have to test service without real repository unless we test both of them. @MockBean help to make fake, mocked repository for service. It\u0026rsquo;s mocked so basically it\u0026rsquo;s not connected with the real db and it means we have no data. In setUp() method, we have to make homemade preparation. Set up Mocked data and even make mock repository action there. Controller test Controller test code is just like Service test code. Don\u0026rsquo;t have to know service, only focus on controller part.\nApiController\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 @Slf4j @Controller @RequestMapping(\u0026#34;/api\u0026#34;) public class ApiController { private final UsersServiceImp usersServiceImp; public ApiController(UsersServiceImp usersServiceImp) { this.usersServiceImp = usersServiceImp; } @PostMapping(\u0026#34;/bean/valid\u0026#34;) @ResponseBody public ResponseEntity beanValid(@Valid @RequestBody MessageDTO messageDTO) { String value = messageDTO.getMessage(); return new ResponseEntity\u0026lt;\u0026gt;(\u0026#34;Your request is accepted!\u0026#34;, HttpStatus.OK); } @GetMapping(\u0026#34;/jpa/get\u0026#34;) public String getByJpa( @RequestParam @Nullable String name, Model model ) { List\u0026lt;UsersVO\u0026gt; users = usersServiceImp.findByName(name); model.addAttribute(\u0026#34;userList\u0026#34;, users); return \u0026#34;mockTest\u0026#34;; } @PostMapping(\u0026#34;/jpa/save\u0026#34;) @ResponseBody public ResponseEntity saveByJpa( @Valid @RequestBody UsersVO usersVO ) { UsersVO result = usersServiceImp.save(usersVO); return new ResponseEntity\u0026lt;\u0026gt;(result, HttpStatus.OK); } } Test Code for ApiController\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 // @SpringBootTest 이 어노테이션을 쓰면 통합 테스트가 된다. unit 테스트에서 지향해야 함. 그리고 이건 실제 db 가 엑세스가 된다. @ExtendWith(SpringExtension.class) // 이 어노가 있으면 마치 ApiController 만 있는 것처럼 스프링 부트를 제한해준다. @WebMvcTest(ApiController.class) class ApiControllerTest { // 얘가 바로 full http server 시작 안 하고 controller 테스트 할 수 있게 해주는 것! @Autowired private MockMvc mvc; @MockBean private UsersServiceImp usersServiceImp; @Autowired protected ObjectMapper objectMapper; protected String asJsonString(final Object object) throws JsonProcessingException { return objectMapper.writeValueAsString(object); } @Test public void givenUsersVO_whenGetUsersVO_thenReturnPOJO() throws Exception { UsersVO dummy = UsersVO.builder() .name(\u0026#34;jamie\u0026#34;) .salary(1000) .build(); given(usersServiceImp.save(dummy)).willReturn(dummy); mvc.perform(post(\u0026#34;/api/jpa/save\u0026#34;) .content(asJsonString(dummy)) .contentType(MediaType.APPLICATION_JSON)) .andExpect(status().isOk()) .andExpect(jsonPath(\u0026#34;$.name\u0026#34;, is(dummy.getName()))); } } Code detail @WebMvcTest\nAnnotation will make MVC infrastructure to our test condition. It makes spring boot server which only has ApiController. Furthermore, MockMvc provide super easy controller test env without loading entire server. (light and fast!) @SpringBootTest\nBlind it for integration test aka test everything! It doesn\u0026rsquo;t make mock bean, so it will do read and write your db. You should be aware this. Integrate test And in addition, I already made integration test using @SpringBootTest It did not make any mock bean so that it always access real db. In this code, I can check GET page answer too.\nControllerTestFrame.class\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 @SpringBootTest @AutoConfigureMockMvc public abstract class ControllerTestFrame { protected MockMvc jsonMock; @Autowired protected ObjectMapper objectMapper; abstract protected Object controller(); protected String asJsonString(final Object object) throws JsonProcessingException { return objectMapper.writeValueAsString(object); } @BeforeEach private void setup() { jsonMock = MockMvcBuilders.standaloneSetup(controller()) // to bind exception with mockMvc .setControllerAdvice(new MyExceptionHandler()) .addFilter(new CharacterEncodingFilter(StandardCharsets.UTF_8.name(), true)) .alwaysDo(print()) .build(); } } Test code for Integrate test\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 public class ApiControllerTestNotUnit extends ControllerTestFrame { @Autowired private ApiController apiController; @Override protected Object controller() { return apiController; } /** * mvc test with post param POJO * @throws Exception */ @Test public void beanValid() throws Exception { MessageDTO user = new MessageDTO(1, \u0026#34;\u0026#34;, \u0026#34;434\u0026#34;, \u0026#34;r3r3r\u0026#34;); jsonMock.perform(post(\u0026#34;/api/bean/valid\u0026#34;) .contentType(MediaType.APPLICATION_JSON) .content(asJsonString(user))); } /** * test get page with param * 이 형태는 아무래도 실제로 spring application 을 띄운게 아니라서 modelandview 정보만 찍어주고 실제는 파악이 힘든 듯 */ @Test public void getByJpa() throws Exception { MultiValueMap\u0026lt;String, String\u0026gt; param = new LinkedMultiValueMap\u0026lt;\u0026gt;(); param.add(\u0026#34;name\u0026#34;, \u0026#34;Sam\u0026#34;); jsonMock.perform(get(\u0026#34;/api/jpa/get\u0026#34;).params(param)) .andExpect(view().name(\u0026#34;mockTest\u0026#34;)) .andDo(MockMvcResultHandlers.print()) .andReturn(); } /** * 이래 놓으면 실제로 db에 저장이 되어버린다. 망함.. 방법은 db 분리하기다. 선택에 따라서 test 환경에서는 h2 같은 db를 쓸 수 잆다고 함 * @throws Exception */ @Test public void saveByJpa() throws Exception { UsersVO jpaEntity = UsersVO.builder() .name(\u0026#34;noinDB\u0026#34;) .salary(2000) .build(); jsonMock.perform(post(\u0026#34;/api/jpa/save\u0026#34;) .contentType(MediaType.APPLICATION_JSON) .content(asJsonString(jpaEntity))); } } (plus) MockMvc I\u0026rsquo;m not familiar with assert function. Until getting used to it, I decide to check result with my eyes. It means I want to print the response on console window.\nSpring already got a solution. In Integerate test, I mock request with mockMvc. With mockMvc, I can ues mvcResult to change result to printable Object. Here is what I do.\nMockMvc to check response\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 import org.springframework.test.web.servlet.MockMvc; @Test void logout() throws Exception { String targetUrl = String.format(\u0026#34;%s%s\u0026#34;, contextPath, \u0026#34;/logout\u0026#34;); Map\u0026lt;String, String\u0026gt; sessionMap = new HashMap\u0026lt;\u0026gt;(); sessionMap.put(\u0026#34;session_key\u0026#34;, \u0026#34;inputed_session_key\u0026#34;); //when MvcResult resultMock = mockMvc.perform(post(targetUrl) .requestAttr(XCConstant.REQ_APP_LANG, testLang) .requestAttr(XCConstant.REQ_AUTH_TOKEN, \u0026#34;\u0026#34;) .contentType(MediaType.APPLICATION_JSON) .content(asJsonString(sessionMap))) .andReturn(); // then String result = resultMock.getResponse().getContentAsString(); log.info(\u0026#34;#### {} test ####\u0026#34;, targetUrl); log.info(result); } Do not forget to attach `@WebMvcTest` or `@AutoConfigureMockMvc` to autowired mockMvc Object in your class. This will let you call mockMvc without any configuration or statement. ","date":"2021-01-26","permalink":"https://leeleelee3264.github.io/post/2021-01-26-spring-unit-test/","tags":["Backend"],"title":"[Backend] (en) Test Spring Boot with Junit5"},{"content":"\nName Convention of Host and URL: Reserved characters and unsafe characters.\nIndex\nTroubleshooting for Name Convention Name Convention Troubleshooting for Name Convention How I occurred the trouble Today, I had to be through very awful exception. There is an api server I have to send a request to update meta data. For performance issue, this server just cached meta data and uses it till updated. Usually it\u0026rsquo;s quite simple. Make request with RestTemplate in Spring boot.\nThis situation got started two days age. The server which contains api server moved to new ec2 machine, and it had new ip. So I have to send my request to the new ip. I had wrote a previous ip on application.properties. Just like this.\napplication.properties\n1 api.server.domain=http://172.22.32.1:8000 Let\u0026rsquo;s say the new ip is 180.1.1.1:8000. I could just change application.properties to http://180.1.1.1:8000, but it\u0026rsquo;s so annoying I have to change application.properties file and rebuild, re-deploy server every time when ip gets changed. I thought now is the perfect timing to use /etc/hosts file in Linux.\n/etc/hosts Domain is kine of alias of ip. In other words, every domain has its own ip. When we type domain to go to specific site, before this request goes any farther it goes to DNS server to get a real ip of the domain.\n/etc/hosts are local DNS file for own ubuntu machine. Ubuntu usually goes check the file first and goes to DNS. The file used to work like back up DNS long time age to prevent DNS error.\nNowadays, the file use to find host name which is not registered in DNS. For example, localhost. There is no real domain named localhost, it\u0026rsquo;s just living in the file.\n/etc/hosts example\n1 2 3 # /etc/hosts 127.0.0.1 localhost Like localhost, I wanted to make own host name for this inner server. These were what I did.\nmy own /etc/hosts\n1 2 # /etc/hosts in Linux 127.0.0.1 api_call_server And used the domain like this.\napi_call_server in Spring boot\n1 2 3 4 5 6 # application.properties in Spring boot api.server.domain=http://api_call_server # code in Spring boot and I use RestTemplate to send a request @Value(\u0026#34;${api.server.domain}\u0026#34;) private String serverDomain But it turned out I did something wrong. I kept getting 400 Bad request from api server. You may get unknown host exception when you send a request to a host which is not in /etc/hosts file. But I already added! On my second thought, maybe the reason for the exception is DNS caching. But it was not that.\nReason for the exception I used not allowed character in host name.\nAt that time, I should have known about host name convention better. you see, my own host name is api_call_server now let\u0026rsquo;s see what characters are allowed for naming host.\nName Convention Host Name convention a-z, A-Z 0-9 - These are all. No others. I\u0026rsquo;ll keep in mind I cannot use _ in Url. Only Hyphen would work. So I change host name from api_call_server to api-call-server. After changing, I no longer saw 400 bad request.\nUrl Name convention a-z, A-Z 0-9 - . _ ~ ( ) ' ! * : @ , ; Not safe to use [Picture 1] Not Safe in URL In not safe, there are two concepts. Reserved and unsafe.\nReserved Literally reserved for system. You can see those letters in Linux env a lot. Safety of Url is not static when url containing reserved character. I think I\u0026rsquo;d better not use them. Unsafe Just do not use them. If I want to use one of them, I have to percent encoding and it\u0026rsquo;s on RFC. (plus) only use lower case in URL We had better use Lowercase when it comes to making urls.\nLowercase is more traditional way and considering UX, users don\u0026rsquo;t have to press shift. And don\u0026rsquo;t mix them too. (It called safe approach). With SEO(search engine optimize), google consider lowercase url and uppercase url are the same.\nFor example, http://love.pizza.com/HOME.html and http://lova.pizza.com/home.html are the same. Take a look at [lower/upper matters] for more information.\n","date":"2021-01-13","permalink":"https://leeleelee3264.github.io/post/2021-01-13-linux-host-name-convention/","tags":["Infra"],"title":"[Infra] (en) Name convention of host and URL"},{"content":"\nCheat sheet for Java Regex: Changing plain text, extracting substring.\nIndex\nChange plain text to Korean Phone Number format Extracting SubString Change plain text to Korean Phone Number format 1 2 3 4 5 6 7 8 public static void testNumber(String src) { Pattern SRC_PHONE_FORMAT = Pattern.compile(\u0026#34;(\\\\d{3})(\\\\d{4})(\\\\d{4})\u0026#34;); Pattern DEST_PHONE_FORMAT = Pattern.compile(\u0026#34;$1-$2-$3\u0026#34;); String dest = src.replaceFirst(SRC_PHONE_FORMAT.toString(), DEST_PHONE_FORMAT.toString()); System.out.println(dest); } 맨처음에는 패턴들에 통째로 replaceAll을 걸었는데 그럼 결과가 그냥 DEST_PHONE_FORMAT.toString()이랑 동일한 결과가 나온다. replaceFirst를 걸어서 ()()() 이렇게 끊어서 하나씩 적용을 시켜주면 된다. 원래 regex group으로 값들을 꺼내서 바꿀까 했는데 이 쪽이 조금 더 명시적인 느낌이 들었다.\nExtracting SubString Extracting before and after of specific character Let\u0026rsquo;s say I get string with static prefix and I only need string after the prefix.\nFor example, A:COME_IN. A is prefix and I only need COME_IN. I could guess prefix is only one character, however it wouldn\u0026rsquo;t be general to many case. So I will set wild care before and after of the prefix.\nExtracting before and after with :\n1 2 3 4 5 6 7 8 9 String src = \u0026#34;A:COME_IN\u0026#34;; Pattern pattern = Pattern.compile(\u0026#34;([^,]*):([^,]*)\u0026#34;); Matcher mater = pattern.matcher(src); if(mater.find()) { String before = mater.group(1); // \u0026#34;A\u0026#34; String after = mater.group(1); // \u0026#34;COME_IN\u0026#34; } Code detail\nUse find() to move forward. It works like iterator hasNext(). It keeps moving after checking a part of the src string. Finding makes matcher keep moving forward group(0) return whole chosen string if src matches pattern. group(1) is \u0026lsquo;A\u0026rsquo;, group(2) is \u0026lsquo;COME_IN\u0026rsquo; I usually don\u0026rsquo;t make matcher. I just go for pattern.matcher(src).matches because in that case I don\u0026rsquo;t have to return a string. It was just for checking existence and compatibility for format. But in this case, I have to return sub string.\nExtracting between special characters Think about string like this. I (Love) you and let\u0026rsquo;s say I want to extract Love. This will teach me how to do this.\nExtract Love between ()\n1 2 3 4 5 6 7 Pattern pattern = Pattern.compile(\u0026#34;\\\\((.*?)\\\\)\u0026#34;); Matcher matcher = pattern.matcher(src); if(matcher.find()) { System.out.println(matcher.group(0)); // (Love) System.out.println(matcher.group(1)); // Love } Code detail\n\\\\ is escape for ( and ). It is well known fact that some characters can cause confusion to computer, so it\u0026rsquo;s like safety lock for those kind of letters. Leave the link about group for later. I think I\u0026rsquo;ll confused by the concept again [what is group in java regex]\n","date":"2021-01-11","permalink":"https://leeleelee3264.github.io/post/2021-01-11-java-regex-cheat-sheet/","tags":["Cheat"],"title":"[Cheat Sheet] (en) Java regex cheat sheet"},{"content":"\nHandles how to load static or dynamic resource in Spring jar: ClassPathResource, ResourceLoader.\nIndex\nHow to load static/dynamic resource in Spring jar Fun fact How to load static/dynamic resource in Spring jar First case: Static resource Roughly 4 months age, I was assigned to make function with excel. It was like this. I had made a template excel in advance and my colleagues downloaded it and filled it. After that, I read it and inserted data to database.\nPoint of static resource I have to make and save file before deploying. (making jar file) Never got changed, because it\u0026rsquo;s a template! Can saved in jar static dir (inside of jar) Second case: Dynamic resource And this week, I was assigned new function also related with excel. I have to make a excel file which is analysis purpose and let it get downloaded. Sadly, data in the excel file get changed frequently. This function has to make excel every request.\nPoint of dynamic resource I have to make and save file on run time. Always freshly made! jar already closed, can saved in other dir (outside of jar) Both are about downloading excel file in server. But there is a huge difference.\nHow to load then? Use ClassPathResource and relative path to get file static resource in jar Use ResourceLoader and absolute path to get file dynamic resource outside of jar Take a look at [Classpath resource not found when running as jar].\nClassPathResource for static resource\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 private final MediaType excelType = MediaType.parseMediaType(\u0026#34;application/vnd.ms-excel\u0026#34;); String EXCEL_STATIC_PATH = \u0026#34;static/excel/\u0026#34; @GetMapping(\u0026#34;/{data}/template/download\u0026#34;) @ResponseBody public ResponseEntity\u0026lt;Resource\u0026gt; getTemplate( @PathVariable(name = \u0026#34;data\u0026#34;) String data ) { String requestData = Util.nvl(data); String fileName = \u0026#34;\u0026#34;; if (requestData.equals(\u0026#34;vts\u0026#34;)) { fileName = \u0026#34;vts_template.xlsx\u0026#34;; } try { ClassPathResource classPathResource = new ClassPathResource(EXCEL_STATIC_PATH + fileName); return ResponseEntity.ok() .header(HttpHeaders.CONTENT_DISPOSITION, \u0026#34;attachment;filename=\u0026#34; + classPathResource.getFilename()) .header(HttpHeaders.CONTENT_LENGTH, String.valueOf(classPathResource.contentLength())) .header(HttpHeaders.CONTENT_TYPE, String.valueOf(excelType)) .body(classPathResource); } catch (Exception e) { log.error(e.getMessage(), e); return ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR).build(); } } ResourceLoader for dynamic resource\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 @Autowired private ResourceLoader resourceLoader; @Value(\u0026#34;${property.excel.dir}\u0026#34;) private String EXCEL_GENERATED_DIR; @GetMapping(value = \u0026#34;/{type}/count/download\u0026#34;) @ResponseBody public ResponseEntity\u0026lt;Resource\u0026gt; testGetValueFromExcel( @PathVariable(name = \u0026#34;type\u0026#34;) String type ) { String ttsType = validDnxTts.get(type); String fileName = excelService.makeVtsMessageExcel(ttsType, type, EXCEL_GENERATED_DIR); Resource resultResource; try { // This name will be showed up to client String fullFile = new StringBuilder().append(EXCEL_SEVER_HOST).append(\u0026#34;_\u0026#34;).append(fileName).toString(); resultResource = resourceLoader.getResource(\u0026#34;file:\u0026#34; + EXCEL_GENERATED_DIR + fileName); return ResponseEntity.ok() .header(HttpHeaders.CONTENT_DISPOSITION, \u0026#34;attachment;filename=\u0026#34; + fullFile) .header(HttpHeaders.CONTENT_LENGTH, String.valueOf(resultResource.contentLength())) .header(HttpHeaders.CONTENT_TYPE, String.valueOf(excelType)) .body(resultResource); } catch (Exception e) { log.error(e.getMessage(), e); return ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR).build(); } } Fun fact File Path in Windows and Linux File storage path grammar is different, so had better write down both on properties file.\nWindows file path\n1 property.excel.dir=\\\\C:\\\\Temp\\\\Admin\\\\ Linux file path\n1 property.excel.dir=/opt/temp/ Accident with / I accidentally attached / in resourceLoader.getResource(\u0026ldquo;file:\u0026rdquo; + EXCEL_GENERATED_DIR + fileName);\nSo in Linux, it looks like file://opt/temp/. In this case, I saw java.net.UnknownHostException: socialweb-analytics.lcloud.com: Name or service not known exception lol. Server thinks it\u0026rsquo;s host.\n","date":"2021-01-08","permalink":"https://leeleelee3264.github.io/post/2021-01-08-spring-resource-load/","tags":["Backend"],"title":"[Backend] (en) Load static/dynamic resource Spring Jar"},{"content":"\nWrite shell script to deploy server and rotate log.\nIndex\nShell script to deploy server Shell script to rotate log (plus) /dev/null UPDATE VERSION OF SHELL SCRIPT WITH PASSWORD One month ago, I make this shell script with hard coded user password and I knew that was a bad idea. It is well known fact that leaving password somewhere or some files can be a huge problem with security later. So I decided to fix it.\nHow to request password in shell script\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 #!/bin/bash # This file is for deployment aiv site to latest version. echo enter the password for ${USER} and press enter IFS= read -rs PASSWD sudo -k if sudo -lS \u0026amp;\u0026gt; /dev/null \u0026lt;\u0026lt;EOF $PASSWD EOF then sudo systemctl reload shiny-server sleep .9 sudo systemctl status shiny-server | cat echo \u0026#34;Successfully updated aiv\u0026#34; else echo \u0026#34;The password is wrong. Please check again\u0026#34; fi In this way, I don\u0026rsquo;t have to hard code password. It will check the password and then do other thing. It deletes the password after that, so if I want to do one more, I have to enter the password again.\nShell script to deploy server Today, I handed over a project to a teammate I had taken over from a colleague who was about to quit the job. The one thing I was concerning is the fact that the teammate will take over whole part of the project. It means she is going to do deploy as well and she is quite new with Linux.\nI didn\u0026rsquo;t think it is a good idea to let her know all the command using for deployment. So I decided to making shell script containing the command! (I\u0026rsquo;ve wanted to learn about shell script too)\nLet\u0026rsquo;s write the script step-by-step The most important beginning of shell script! As far as I remember, all the shell script is working with bash file in bin directory. To execute shell script, do not forget to wirte #!/bin/bash line.\n1 #!/bin/bash Give the bash file you were writing execution roll. Bash file is for execution after all. This is almost everything I know about shell script. Now let\u0026rsquo;s take a look at the bash file I wrote today.\n1 2 touch test_bash.sh chmod +x test_bash.sh deploy shell script\n1 2 3 4 5 6 7 8 9 10 11 #!/bin/bash # add some comment to let people know what this bash file for. pw=\u0026#34;password_I_used_in_server\u0026#34; echo ${pw} | sudo -kS systemctl reload shiny-server sleep .5 echo ${pw} | sudo -kS systemctl status shiny-server | cat echo \u0026#34;Successfully updated aiv\u0026#34; deploy shell script detail\necho ${pw} (lint 7) I wanted to make bash file which passes writing password in the middle of execution. So I just give password as param. For this, I have to write pw in a file in advance. Bad for security. Need to think about more secure way. sudo -kS (line 7) S is for getting password from \u0026rsquo;echo\u0026rsquo; command. k is for reset timestamp. When the timestamp remains in sudo command, it might cause some error. sleep .5 (line 8) To put delay from reloading. systemctl status shiny-server | cat (line 10) I want to show the status of service. But Systemctl needs to be quit. So I just printed it. Shell Script to deploy spring server In previous example, I wrote shell script to deploy shiny-server. But I usually use shell script to deploy Spring jar server.\ndeploy shell script for Spring\n1 2 3 4 5 6 7 8 9 10 11 12 #!/bin/bash DATE=`date +\u0026#39;%Y%m%d\u0026#39;` echo $DATE ETC_JAVA_OPTS=-XX:+UseStringDeduplication # supposed to be one line. nohup java -Xms128m -Xmx128m -XX:NewRatio=1 -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -Xloggc:./gc.log -Dspring.profiles.active=prod $* -jar file_name.jar \u0026gt;\u0026gt; ./server.log \u0026amp; tail -F server.log It\u0026rsquo;s Java, so I passed GC, heap size, running profile etc.\nIt should be running in background. Unless, it will get stopped when I close a terminal which is running jar. So I used nohup command.\nThe file is not just about running jar file. It makes log file keep going. \u0026gt;\u0026gt; command means stdout will be remained in a file located right behind the command. Tail is just to make sure the jar file is successfully built.\nShell script to rotate log shell script to rotate log\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 #!/bin/bash DATE=`date +\u0026#39;%Y%m%d\u0026#39;` DATE2=`date +\u0026#39;%Y%m\u0026#39;` LOG_FILENAME=backup_server$DATE.log LOG_DIRNAME=\u0026#34;backup_server${DATE2}_log\u0026#34; if [ -e $LOG_FILENAME ] then echo \u0026#34;$LOG_FILENAME exist\u0026#34; else echo \u0026#34;cp server.log $LOG_FILENAME\u0026#34; cp server.log $LOG_FILENAME cp -f /dev/null server.log echo \u0026#34;now tail..\u0026#34; fi # moving log to dir file script if [ -e $LOG_DIRNAME ] then : else mkdir $LOG_DIRNAME fi mv $LOG_FILENAME $LOG_DIRNAME tail -F server.log This one is for managing log file. These two main process are like that.\n(1) make current log file to backup file and make new log file to be used soon\n(2) move freshly made backup log file to proper log file directory.\n/dev/null What is /dev/null in the bash command? It\u0026rsquo;s like official empty file of Linux. The file is always empty. It has to be.\nIn the bash, I didn\u0026rsquo;t delete server.log file, because I\u0026rsquo;ll keep using it after moving all contents in file to a backup file. In this purpose, /dev/null is best choice. Think like this.\nScenario detail\n/dev/null is always empty Copy content in /dev/null to server.log It means content(which is empty) is copied to server.log. Basically, server.log file should be empty too. You can also use \u0026lsquo;cat /dev/null \u0026gt; server.log\u0026rsquo; cat will print /dev/null content(which is empty) and \u0026lsquo;\u0026gt;\u0026rsquo; will pass the content to specific file(server.log). Using this command makes so easy to remove data and make file size to zero. I have a feeling it will be super useful managing linux server memory!\n","date":"2021-01-05","permalink":"https://leeleelee3264.github.io/post/2021-01-05-linux-writing-shell-script/","tags":["Infra"],"title":"[Infra] (en) Write Shell script to deploy server and rotate log"},{"content":"\nLinux command를 위한 Cheat Sheet: zcat, grep, jar, apt-cache, pgrep, route, ip, watch, wc, ss etc.\nIndex\nIf operation 2021 Cheat Sheet 2020 Cheat Sheet If operation IF operation 2021 Cheat Sheet 압축을 풀지 않고 압축 파일 확인하는 방법 1 2 3 4 5 zcat test2.log.gz # pagination with zcat # cat 커맨드 처럼 less를 뒤에 써주면 된다. zcat test2.log.gz | less 가끔 압축되어있는 gz 파일을 확인할 일이 생긴다. 압축된 파일을 보려면 압축을 풀고, 그 다음에 조회를 해야 한다고 생각했지만 zcat 커맨드를 사용하면 압축을 풀지 않고도 확인을 할 수 있다. 더 자세한 사용법은 [zcat 커맨드] 을 참고하면 된다.\n포스팅에는 zcat만 썼는데 찾아보니 zless도 있다고 한다.\n우분투 버전 확인 1 cat /etc/lsb-release grep with before/after num line 1 2 3 4 5 6 7 8 9 # after grep -A num \u0026#34;key_word\u0026#34; \u0026#34;file_name\u0026#34; # before grep -B num \u0026#34;key_word\u0026#34; \u0026#34;file_name\u0026#34; ex) grep -A 2 POST server.log grep -B 2 POST server.log grep을 쓰다 보면 딱 해당 사항의 문장들만 뽑아다 준다. 그래서 전후 상황을 알기가 어려운데 이때 -A와 -B 옵션으로 grep 앞/뒤에 있는 문장들 n 행까지 출력을 할 수 있다.\njar 파일 내부 보기 1 2 jar -tf \u0026#34;jar_name\u0026#34; ex) jar -tf test.jar jar 파일은 java + tar 로, 리눅스에서 사용하는 파일 묶음인 tar를 자바 버전으로 만들었다고 생각하면 된다.\n서버에 jar파일만 올리면 이 파일 안에 뭐가 들어있는지 알고 싶을 때가 있고 어떤 라이브러리의 어떤 버전을 쓰는지 알고 싶을 떄가 있다. 이때 해당 커맨드를 사용해주면 된다. 단, tar 파일이 압축파일은 아니다. 그냥 여러 파일들을 한데 묶는 역할을 한다고 생각하면 된다.\n설치된/설치할 apk 버전 확인하기 1 2 apt-cache policy \u0026#34;apk_name\u0026#34; ex) apt-cache policy nginx pgrep 1 2 pgrep -a \u0026#34;query\u0026#34; ex) pgrep -a java AKA process grep\nwith this, I don\u0026rsquo;t have to do ps -ef | grep java. pgrep is ps with grep function.\nGet info about command 1 2 type \u0026#34;query\u0026#34; ex) type ls Getting more info about command. It will let you know about command such as alias, shell embedded\nRelated with network route 1 route 머신의 라우팅 테이블을 보여준다. 어떤 네트워크들이 열려있는지, 그 네트워크들의 브로드케스팅과 넷마스크, 인터페이스를 볼 수 있다.\nip 1 ip a 제일 많이 사용하는 네트워크 정보 커맨드는 ifconfig인데 ifconfig는 net-tools를 설치해야 사용을 할 수 있다. ip a 를 사용하면 비슷한 정보들을 뽑아낼 수 있다. 오히려 ifconfig 보다 ip 커맨드가 더 기능이 많아보인다.\nMoving cursor in vi 1 2 3 4 5 go to end point of sentance : home key go to start point of sentance : end key go to the top of file : gg go to the bottom of file : ctrl + g 여태 vi 조작을 할 때 문장의 맨끝과 맨앞을 쉽게 가는 방법을 몰라서 방향키를 열심히 눌렀는데 home 키를 누르면 문장의 바로 앞으로, end 키를 누르면 문장의 끝으로 손쉽게 갈 수 있다.\nwatch 1 2 3 4 watch ss -tlp # 한 번에 여러개 보기 watch \u0026#34;ss -tlp;df\u0026#34; 규칙적으로 값을 갱신해서 화면에 보여준다. default 값은 2초인 거 같다. 한마디로 이 커맨드를 쓰면 서버 상태를 계속 모니터링 할 수 있는 거다. 아예 고정을 해놓고 실시간으로 업데이트를 해서 모여주니까 모니터링하기 편하다. 약간 top과 비슷하다고 할 수 있다.\ntree 1 2 3 4 tree tc-admin # 디렉터리들 지정해서 tree 만들기 tree tc-admin tc-admin-prod 이건 작업할 때 많이 쓴다기 보다는 블로그에 포스팅할 때 디렉터리 구조 보여주는데 더 많이 쓰는 것 같다. 원래의 용도는 한 디렉터리의 구조를 deep down 하게 들어가 tree 를 만들어 보여주는 것이다. 디렉터리를 한 눈에 파악하기 좋다.\n키워드 문장 제외 하고 검색하기 1 2 3 4 # 키워드가 있는 문장을 제외하고 보여준다. # -v 는 invert의 v 이다. 단어 그대로 결과 값을 뒤집는 것. grep -v \u0026#34;keyword\u0026#34; test.txt Useful command from Linux Pocket Guide 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # file line count wc -l file_name # find difference between two files diff file1 file2 # cat with number cat -n file_name # less with number less -N file_name # get dir info, not files info in the dir ls -d dir_name # try to read binary file with human eyes strings file less -N file_name is fascinating command indeed. I usually use vi to see a whole file, if I try to access the file from different terminal I face a conflict .swp problem. I can solve it with less. Technically, Less does not make the file open. It just prints all lines in the file using stream.\nNot like cat, less is a stream ,so I can go back and forth. Number + g event let you go to the number directly. 100g means go to 100 th lines in the file.\nsetting number in vim 1 2 3 4 5 6 7 8 9 : set number # remove number : set nonumber # set number to vim permanently vim ~/.vimrc # type set number in .vimrc file set number shutdown linux 1 shutdown -h now Run Java file in Linux I usually work with windows. But mostly, our server env is Linux. Because of this, I have to test in local(windows) and Linux both. Everything is quite same, however just like downloading/making file AKA involved with path makes things harder.\nToday(2021-01-08) I have to check project dir in both env, so I wrote a short java code to run in Linux as well.\nJava code to get directory 1 String currnetPath = System.getProperty(\u0026#34;user.dir\u0026#34;); // it will return a currnet working dir Run Java 1 2 3 4 5 6 7 # let\u0026#39;s say I made CurrnetPath.java which contains class CurrentPath. # compile javac CurrnetPath.java # after compiling, I\u0026#39;m able to find CurrnetPath.class file. It\u0026#39;s the result of compiling and I\u0026#39;ll run it. Basically, I run class file to execute java code. java CurrnetPath service related command 1 2 3 4 5 6 1. show installed service service --status-all 2. show running service systemctl --type=service Linux commands related to service are mostly start as systemctl. Service is kind of out-dated command, so it had better using systemctl\nubuntu 18 network connection 1 2 3 4 cd /etc/netplan sudo vi 01-network-manager-all.yaml ~ change setting ~ sudo netplan apply In Ubuntu 18 version, network connection setting is quite different with order versions.\nNow setting file is located in /etc/netplan. It means netplan is now the commander of netwokring. I had not known about applying command, so I just rebooted the os when I changed something. But you\u0026rsquo;d better just using netplan apply command.\nLinux command 2020 sftp connection 1 2 3 4 5 6 7 8 9 10 11 12 # 접속 sftp hostId@hostName # 파일 받아오기 get file #파일 올리기 put file #디렉터리 작업 (linux 에서 자주 보이는 옵션 -r은 recursion이다. 디렉터리 속으로 들어가서 재귀적으로 다 가져온다는 뜻인가봄 get -r file put -r file - command to go previous directory 1 cd - 내가 현재 디렉토리로 옮기기 직전에 있었던 디렉토리로 가는 커맨드.\n작업을 하다보면 cd를 써서 디렉토리를 옮기는 일이 정말 많다. 그런데 방금 있었던 디렉토리로 돌아가야 할 때 cd 경로를 다시 치는 것 보다 cd - 를 사용하면 손 쉽게 갈 수 있다.\n직전의 커맨드 불러오기 1 !! 바로 이전의 커맨드는 사실 ↑을 이용하는데 가끔 이전의 명령 불러오기가 안되는 경우가 있다고 한다. 그때 사용하면 좋은 커맨드. !를 잘 기억해두자. 과거의 커맨드를 불러들이는데 연관이 있는 커맨드다.\nhistory에 있는 n 번째 커맨드 불러오기 1 2 !number ex) !120 history 커맨드를 사용하면 내가 여태까지 사용한 커맨드들의 기록을 보여준다. 그때 만약 다시 실행하고 싶은 커맨드가 있다면 !12 (실행하고 싶은 커맨드 번호)를 쓰면된다.\n제일 마지막에 실행했던 특정 커맨드 블러오기 1 2 ! previous command ex) !cd 아까 !12와 비슷하다. 단, 여기서는 history를 볼 필요가 없다. !cd (내가 제일 마지막으로 실행했던 cd) 라는 뜻이다. 결국 !뒤에 써진 커맨드는 내가 제일 최근에 실행했던 커맨드를 다시 실행해달라는 뜻이다.\n프롬프트에 입력된 문자 모두 지우기 1 ctrl + u ctrl + u를 사용하면 여태 입력했던 문자가 싹 지워진다. 어떨 때 제일 좋냐하면 CUI 환경에서 비밀번호 치다가 오타 났을 때 ctrl+c 눌러서 명령 취소 안 하고 ctrl + u 눌러서 다시 입력할 때 좋다.\n전체 실행중인 프로세스 모니터링 1 2 top htop 가끔 리눅스 안에서 실행되는 프로세스들을 확인하고 싶을 때가 생긴다. 예를 들어 실행한 jar 파일이 잘 돌고 있는지, 지금 막 db 배치를 돌렸는데 일을 잘 하고 있는지 등등. 프로세스 확인에는 ps -ef를 제일 많이 사용하겠지만 mysql, java 등등 돌아가는게 한 눈에 보기 좋은 커맨드는 top 이다.\ntop에서 shift+ p는 cpu 많이 잡아먹는 순으로 보여주고 shift + m 은 메모리 많이 잡아먹는 순으로 보여준다.\n포트와 프로세스 연결 확인 1 netstat -ntlp 사실 이건 고수 커맨드는 아니고 내가 자꾸 깜빡깜빡해서 넣어놨다. 운영체제 안의 몇 번 포트들이 어느 프로세스와 연결되어 열려있는지를 볼 수 있다.\n리눅스 스크린 조작 커맨드 1: screen 1 screen 대부분의 터미널은 bash 프로세스와 연결이 되어있다. 그래서 터미널을 닫아버리면 내가 실행하고 있던 프로세스가 같이 끝나버린다. 그래서 jar 실행 등 리눅스 자체가 멈추지 않는 한 계속 실행되기 바라는 작업들은 리눅스가 죽기 전 까지 죽지 않는 root 프로세스에 연결한다. nohop 커맨드가 바로 그런 역할을 한다.\n그런데 이 방법 말고도 터미널이 죽어도 작업이 죽지 않는 방법이 있다. 바로 screen 커맨드다. 사실 screen 커맨드는 하나의 터미널에 여러 tab을 만들 때 사용하면 좋다. 여기저기서 작업을 할 수 있어서 편리하다. attached/detached 개념을 사용해서 터미널에 붙어있는지 아닌지를 알 수 있다.\n이렇게 스크린을 만들어 두면 작업 도중에 회사에서 집으로 옮겼을 경우 detached된 screen을 attached해서 사용하면 회사에서 하던 작업을 그대로 할 수 있다.\nscreen 조작 1 2 3 4 5 6 7 8 screen -ls: 현재 만들어져있는 screen의 list를 보여줌 screen -S name: name이란 이름의 screen을 만들어서 실행시켜줌 ctrl + a + d: 현재 작업하고 있던 screen에서 나오기 (screen이 detached 상태가 된다) screen -r name: detached된 name에 다시 접속 ctrl + a + c: 해당 스크린 안에서 새로운 tab(bash)를 열어줌 ctrl + a + n: 다음 tab으로 넘어가기 ctrl + a + p: 이전 tab으로 넘어가기 screen -X -S name quit : name이라는 이름의 screen을 없애버림 그리고 screen의 환경설정을 .screenrc에서 하는데 이때 화면을 보다 보기 쉽게 만들어 줄 수 있다. 나의 추천은 [screen 세팅] 처럼 세팅하는 것이다. 간결해서 보기 편하다. 적용하면 밑에 그림처럼 screen창이 만들어진다.\n파일 끝 1 tail tail -f는 파일이 달라지는 걸 모른는데(포인터가 달라지는 둥) -F는 파일의 변화를 안다. 내용이 바뀐다는 것보다는 파일의 포인터가 달라진다는 얘기를 하는 것 같음.\n파일 시작 1 head tail 과 비슷한 커맨드인데, tail이 파일의 끝을 조작한다면 head는 파일의 시작을 조작한다.\n파일 만들기 1 touch 그냥 시간 업데이트 한다고 생각했는데 컴파일러와 상관이 있다. 컴파일러는 안의 내용이 바뀌지 않아도 시간이 갱신되면 다시 컴파일을 해준다. 그래서 컴파일을 다시 하고 싶은데 번거롭게 뭘 따로 하고 싶지 않으면 touch를 써서 쉽게 시간을 바꿔주면 컴파일이 된다.\n그리고 파일을 만들때도 vi로 열어서 저장을 하기 보다는 touch로 먼저 파일을 만들어두고 작업을 하는 경우가 많다.\n리눅스 스크린 조작 커맨드 2: tmux 1 tmux 다른 분이 알려주신 커멘드인데 정말 너무너무 잘 쓰고 있다. 예전에 이거 없이 어떻게 살았나 싶을 정도.. screen 커맨드와 비슷한데 훨씬 더 좋다. session으로 이전 작업을 기억할 뿐 아니라 화면을 나눌 수 있다.\ntmux 조작 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 scroll 하기 : ctrl + b + pageUp 새로운 session 열기: tmux new -s session_name session 끝내기: tmux kill-session -t session_name session 목록 확인: tmux ls session 종료하기: (tmux session에 들어가있는 상태에서) exit -\u0026gt; 양파처럼 윈도우가 하나씩 꺼진다. 마지막으로 exit을 하면 세션이 종료된다. session deattach 하기: ctrl+b, d session reattach 하기: tmux attach -t session\\_name session window 새로 만들기: ctrl+b, c 화면 가로로 나누기: ctrl+b, \u0026#34; 화면 세로로 나누기: ctrl+b, % 나눠진 화면 모양 바꾸기: ctrl+b, space 나눠진 화면 비율 바꾸기: ctrl+b 누른 상태로 방향키로 조정 화면 사이 이동하기: ctrl+b, 방향키 Nginx conf 파일 문법 체크 1 nginx -t 갑자기 리눅스 커맨드 얘기 하다가 nginx 얘기가 나왔는데 자주 깜빡해서 써두기로 했다. nignx의 conf 파일을 바꾸고 겁도 없이 바로 systemctl reload nginx 를 하고는 했는데 그 전에 conf 파일을 바꿨으면 검사를 먼저 해줘야 한다.\nsudo nginx -t 를 하면 잘 썼을 경우 ok가 나오는데 그때 systemctl reload nginx로 반영을 하자.\n네트워크 커맨드: ss ss에 대한 포스트를 읽다보니 ss가 new netstat 라는 말을 봤는데 그런것 같다. 열린 포트를 보려고 netstat을 해야하는데 정말 안 외워지는 커맨드 같다. ss가 조금더 사용이 쉽다고 한다.\n1 ss ss 커맨드 상세 1 2 3 4 5 6 7 8 9 ss: 연결된 포트가 다 나온다 ss -t : 연결된 tcp 포트 ss -u : 연결된 udp 포트 ss -x : 연결된 유닉스 포트 (시스템적으로 연결된거라서 결과가 정말 많이 뜬다) ss -tl : 연결된 tcp 포트 중에서 LISTEN 상태인것 ss -tlp : 연결된 tcp 중 LISTEN 상태를 돌리고 있는 프로세스까지 보여준다 ss state listening : 연결된 port중 LISTEN 상태인것 (ss -l 과 동일한테 줄바꿈이 안 일어난다) ss -n : 연결된 포트를 보는데 n: numeric 옵션이라서 포트 번호를 정확하게 보여준다. ss dst 특정 ip : 연결된 특정 ip에 대한 정보 (ip는 peer address의 ip이다.) ","date":"2020-12-12","permalink":"https://leeleelee3264.github.io/post/2020-12-12-linux-cheat-sheet/","tags":["Cheat"],"title":"[Cheat Sheet] Linux command cheat sheet"},{"content":"\nHandles MySQL View and UNION/JOIN\nIndex\nIntro View UNION/JOIN Intro Today, I faced my big mistake with new updated function which is made by me as well. The situation is something like this. We have a message service firing with multiple condition such as accomplish of today\u0026rsquo;s step count or tag reminder of today\u0026rsquo;s goal.\nTable Design When we designed tables, we separate to act_message and goal_message. (Obviously, it\u0026rsquo;s design error. I was supposed to create one table for these two). And there is mp3 file table music_list because the message will be playing designated mp3 file when it gets fired.\nIn act_message and goal_message, voice_id is recorded and it stands for the id of music in music_list.\nTables\nact_message goal_message (structure is the same with act_message) music_list Tables\u0026rsquo; Columns\n1 2 3 4 5 6 7 8 9 10 11 12 13 1. act_message, goal_message table | id | voice_id | message_type | is_active | |----|----------|--------------|-----------| | 1 | 100 | V | N | | 2 | 43 | V | Y | 2. music_list | id | title | url | |-----|--------------|--------------------| | 43 | Oh happy day | https://something1 | | 100 | 43 | https://something2 | Troubleshooting So, the new updated function was that delete mp3 file in music_list.\nWhat I forgot was that messages will not send when they cannot find recorded voice_id in music_list. Finally, mp3 file was removed clearly and messages which were supposed to be sent just stay calm.\nI should have checked the mp3 usage before removing. For this, I thought about how to gather voice id in all the separated message tables.\nAs you can predict, I decided to go with UNION query. But the query statement won\u0026rsquo;t be short due to combine many message tables. At that moment my project manager recommended me to use View!\nView I had never used VIEW command before. I did some research and found out VIEW is more like result of reserved select statement which is not a real table but fake one.\nAs far as I remember, VIEW table is literally for view. It cannot work with delete, update and insert. For short, it cannot manipulate real data in database.\nView\nView is basically reserved select statement. View table will be made when the view table called in query. So you don\u0026rsquo;t have to worry about updating data in View table. It will be updated every single time you call the table. There is no performance benefit of using view. It\u0026rsquo;s the same with select query. The good thing is that you don\u0026rsquo;t have to execute complicated select query. It\u0026rsquo;s already made with View. UNION and JOIN Now let\u0026rsquo;s talk about union a bit more. As you can see, I will use UNION command in the former query to collect all the voice id in tables. Collecting data in one place\u0026hellip; Familiar for you?\nYes! It sounds like JOIN. I used join many times, mostly when I have to combine two tables with one same value. For example, I have a table for user\u0026rsquo;s birthday and a table for user\u0026rsquo;s name and they both have user_id in the data. Then I can connect them with user_id and will get user\u0026rsquo;s name and birthday at once.\nJOIN VS UNION I want to think about the difference between UNION and JOIN. It\u0026rsquo;s quite simple. UNION combine data to column when JOIN combine data to row. What I mean is, UNION will extend top to bottom and JOIN will extend left to right.\nIf you want to have additional data in other table, go for JOIN. If you want to combine data in other table as same value, go for UNION. [Picture 1] Join process [Picture 2] Union process There is an excellent explanation of [What is the difference between JOIN and UNION?]\nFinal Query with View and Union Here is my final query! I made v_voice_usage which contains only one column \u0026lsquo;voice_id\u0026rsquo;. I can use v_voice_usage as a normal table when it comes to using select command. And as I said, the view query will build table when I\u0026rsquo;m looking for v_vocie_usage.\nQuery\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 CREATE VIEW v_voice_usage AS SELECT voice_id FROM act_message WHERE message_type = \u0026#39;V\u0026#39; AND voice_id \u0026gt; 0 UNION ALL SELECT voice_id FROM goal_message WHERE message_type = \u0026#39;V\u0026#39; AND voice_id \u0026gt; 0 // Let\u0026#39;s say if I have two voice id \u0026#39;voice_id\u0026#39; and \u0026#39;voice_id2\u0026#39;, I can use this way too. SELECT voice_id FROM act_message WHERE message_type = \u0026#39;V\u0026#39; AND voice_id \u0026gt; 0 UNION ALL SELECT voice_id2 AS voice_id FROM act_message WHERE message_type = \u0026#39;V\u0026#39; AND voice_id2 \u0026gt; 0 Query Result\n1 2 3 4 5 6 | voice_id | |----------| | 1 | | 43 | | 100 | | 5223 | UNION ALL or UNION? If I write UNION query, I wouldn\u0026rsquo;t get duplicated value. UNION command will distinct the same values. It\u0026rsquo;s pretty awesome function. However, it comes with price just like DISTINCT command. As you know DISTINCT is not cheap. It will make your program become quite slowly.\nIf I don\u0026rsquo;t care about duplicated values or you can handle it in server side, then I would go with UNION ALL. It doesn\u0026rsquo;t contain distinct process. Less work, faster query result.\nWhere to put WHERE for performance The worst case use WHERE on the result of UNION. all the distinct work and meaningless combine Slightly better case use WHERE before UNION. at least query will sort the data which is suitable with condition Better case use WHERE before UNION ALL less work + less wok Put WHERE\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 // 1. the worst CREATE v_voice_usage AS SELECT voice_id FROM act_message UNION SELECT voice_id FROM goal_message SELECT * FROM v_voice_usage WHERE voice_id \u0026gt; 0 // 2. better case SELECT voice_id FROM act_message WHERE message_type = \u0026#39;V\u0026#39; AND voice_id \u0026gt; 0 UNION SELECT voice_id FROM goal_message WHERE message_type = \u0026#39;V\u0026#39; AND voice_id \u0026gt; 0 // 3. Better case SELECT voice_id FROM act_message WHERE message_type = \u0026#39;V\u0026#39; AND voice_id \u0026gt; 0 UNION ALL SELECT voice_id FROM goal_message WHERE message_type = \u0026#39;V\u0026#39; AND voice_id \u0026gt; 0 ","date":"2020-10-28","permalink":"https://leeleelee3264.github.io/post/2020-10-28-sql-view-union-join/","tags":["Infra"],"title":"[Infra] (en) MySQL View and UNION/JOIN"},{"content":"\nHandles dividing layout in thymeleaf.\nIndex\nTrouble with not dividend html layout Divide layout Outro Trouble with not dividend html layout How do I feel about not divide html layout. Suffocating with 2000 line!\nAt first, I didn\u0026rsquo;t know the importance about splitting html and making layout. But when I saw 2000 line html files with so many duplicated code (mostly nav, header, side-bar) I felt like I should change my mind. In my opinion, the good things about such work are \u0026hellip;\nPros in dividend html layout\nI don\u0026rsquo;t have to rewrite every header, footer, side-bar (AKA general frame of Web HTML) each time. (no more duplication!) I can control general tags and web clouded script src and stylesheet in link tags quite easily Feel like more formatting with HTML files because they have certain blocks extends from default layout. Divide layout Library to Divide layout in Thymeleaf Currently, I\u0026rsquo;m using thymeleaf (basic template provided from spring boot). So I\u0026rsquo;m going to talk about what I did to make thymeleaf layout. To be honest, I don\u0026rsquo;t have many choice in terms of layout in thymeleaf.\nI was able to find one available user-based thymeleaf library. I\u0026rsquo;m still quite worrying about not using official library, but actually it\u0026rsquo;s very cool library. You can find the github page of the [ultraq/thymeleaf-layout-dialect] over here.\nAnd I just found out the fact that the library contains error with Java 11. In my company, we still use JDK 1.8, so I just went for it.\nLet\u0026rsquo;s implement! In this project, I have to separate page with three languages, Korean, English and Spanish. At that time, I didn\u0026rsquo;t think deeply and making a mistake of making too many html file. It\u0026rsquo;s pain in my ass nowadays.\nAnyway, the point is I don\u0026rsquo;t want to write same codes in each and every files and I found out the code for navigation bar and js css resource tag are everywhere! So I collected them and made to one common file.\nGoal of Implementation\nWrite a code in default layout if you want to apply to everywhere. Slice the page and make to fragment. The best option is footer, navigation bar and something unchangeable and contains everywhere. Keep slicing in defaultLayout page. Make room for body-part, head-part and script-part and more! It\u0026rsquo;s gonna be your lovely template. Finally, it\u0026rsquo;s time for action! Make real html aka part of your web and extends defaultLayout. I\u0026rsquo;m gonna leave a link about [스프링기반 프로젝트,리팩토링,디자인패턴 정보 생성 best practice]. You can figure out how to use the library.\ndefaultLayout.html (main template) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34; xmlns:th=\u0026#34;http://www.thymeleaf.org\u0026#34; xmlns:layout=\u0026#34;http://www.ultraq.net.nz/thymeleaf/layout\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;title layout:title-pattern=\u0026#34;$CONTENT_TITLE\u0026#34;\u0026gt;\u0026lt;/title\u0026gt; \u0026lt;link rel=\u0026#34;icon\u0026#34; type=\u0026#34;image/png\u0026#34; th:href=\u0026#34;@{/project/static/favicon.png}\u0026#34;\u0026gt; \u0026lt;script src=\u0026#34;https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\u0026#34; integrity=\u0026#34;sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\u0026#34; crossorigin=\u0026#34;anonymous\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#39;https://cdnjs.cloudflare.com/ajax/libs/jqueryui/1.12.1/jquery-ui.min.js\u0026#39;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;link th:href=\u0026#34;@{https://fonts.googleapis.com/css?family=Source+Sans+Pro\u0026amp;display=swap}\u0026#34; rel=\u0026#34;stylesheet\u0026#34;\u0026gt; \u0026lt;th:block layout:fragment=\u0026#34;customPreScript\u0026#34;\u0026gt; \u0026lt;/th:block\u0026gt; \u0026lt;th:block layout:fragment=\u0026#34;customStyle\u0026#34;\u0026gt; \u0026lt;/th:block\u0026gt; \u0026lt;style\u0026gt; #n_logout { cursor: pointer; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;th:block th:if=\u0026#34;${session.login.lang == \u0026#39;en\u0026#39;}\u0026#34;\u0026gt; \u0026lt;div th:replace=\u0026#34;fragments/portal/nav-side-en :: nav-side-en\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;/th:block\u0026gt; \u0026lt;th:block th:if=\u0026#34;${session.login.lang == \u0026#39;ko\u0026#39;}\u0026#34;\u0026gt; \u0026lt;div th:replace=\u0026#34;fragments/portal/nav-side-ko :: nav-side-ko\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;/th:block\u0026gt; \u0026lt;th:block th:if=\u0026#34;${session.login.lang == \u0026#39;es\u0026#39;}\u0026#34;\u0026gt; \u0026lt;div th:replace=\u0026#34;fragments/portal/nav-side-es :: nav-side-es\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;/th:block\u0026gt; \u0026lt;th:block layout:fragment=\u0026#34;content\u0026#34;\u0026gt; \u0026lt;/th:block\u0026gt; \u0026lt;script src=\u0026#34;/project/static/js/Util2.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script th:src=\u0026#34;@{/project/static/js/b/sideBar.js}\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script th:src=\u0026#34;@{/project/static/js/dnx_ajax.js}\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;th:block layout:fragment=\u0026#34;customScript\u0026#34;\u0026gt; \u0026lt;/th:block\u0026gt; \u0026lt;script\u0026gt; $(\u0026#39;#n_logout\u0026#39;).click(function () { alert(\u0026#39;로그아웃 하시겠습니까?\u0026#39;); let post_param = { request_url:\u0026#39;/project/b/logout\u0026#39;, next_url: (function (result) { if (result === 0) { window.location.href = \u0026#39;/project/b/login\u0026#39; } else { alert(\u0026#34;ERROR!! SERVER ERROR 발생! 개발자에게 알려주세요.\u0026#34;) } }) } ajax_post(post_param) }); \u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; nav-side-en.html (fragment) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34; xmlns:th=\u0026#34;http://www.thymeleaf.org\u0026#34; \u0026gt; \u0026lt;th:block th:fragment=\u0026#34;nav-side-en\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;nav-side-menu\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;brand\u0026#34;\u0026gt;\u0026lt;img src=\u0026#34;/tcadmin/static/img/w_logo.png\u0026#34; width=\u0026#34;154px\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;menu-list\u0026#34;\u0026gt; \u0026lt;ul id=\u0026#34;menu-content\u0026#34; class=\u0026#34;menu-content collapse out\u0026#34;\u0026gt; \u0026lt;li class=\u0026#34;collapsed\u0026#34;\u0026gt; \u0026lt;a\u0026gt; \u0026lt;div id=\u0026#34;num\u0026#34; class=\u0026#34;side-home\u0026#34;\u0026gt;Home\u0026lt;/div\u0026gt; \u0026lt;/a\u0026gt; \u0026lt;/li\u0026gt; \u0026lt;li class=\u0026#34;collapsed\u0026#34;\u0026gt; \u0026lt;a href=\u0026#34;/tcadmin/b/user/list\u0026#34;\u0026gt; \u0026lt;div id=\u0026#34;num\u0026#34; class=\u0026#34;side-home\u0026#34;\u0026gt;User\u0026lt;/div\u0026gt; \u0026lt;/a\u0026gt; \u0026lt;/li\u0026gt; \u0026lt;li class=\u0026#34;collapsed\u0026#34;\u0026gt; \u0026lt;a href=\u0026#34;/tcadmin/b/user/report\u0026#34;\u0026gt; \u0026lt;div id=\u0026#34;num\u0026#34; class=\u0026#34;side-home\u0026#34;\u0026gt;Report\u0026lt;/div\u0026gt; \u0026lt;/a\u0026gt; \u0026lt;/li\u0026gt; \u0026lt;li class=\u0026#34;collapsed\u0026#34;\u0026gt; \u0026lt;a href=\u0026#34;/tcadmin/tts/login\u0026#34;\u0026gt; \u0026lt;div id=\u0026#34;num\u0026#34; class=\u0026#34;side-home\u0026#34;\u0026gt;Admin\u0026lt;/div\u0026gt; \u0026lt;/a\u0026gt; \u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;nav\u0026#34;\u0026gt; \u0026lt;input type=\u0026#34;checkbox\u0026#34; id=\u0026#34;nav-check\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;nav-header\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;nav-title\u0026#34;\u0026gt; \u0026lt;span class=\u0026#34;home-menu\u0026#34;\u0026gt; \u0026lt;a href=\u0026#34;#\u0026#34; id=\u0026#34;home-menu-text\u0026#34;\u0026gt;\u0026lt;/a\u0026gt; \u0026lt;/span\u0026gt; \u0026lt;th:block layout:fragment=\u0026#34;customNav\u0026#34;\u0026gt; \u0026lt;/th:block\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;nav-links\u0026#34;\u0026gt; \u0026lt;a href=\u0026#34;#\u0026#34; style=\u0026#34;font-weight:bold;\u0026#34;\u0026gt;\u0026lt;span th:text=\u0026#34;${session.login.login_id}\u0026#34;\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;/a\u0026gt; \u0026lt;a id=\u0026#34;n_logout\u0026#34;\u0026gt;Log out\u0026lt;/a\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;script\u0026gt; (function f() { let pageTitle = document.title document.getElementById(\u0026#39;home-menu-text\u0026#39;).textContent= pageTitle let collClasses = document.getElementsByClassName(\u0026#34;side-home\u0026#34;) for(let i = 0; i \u0026lt; collClasses.length; i++) { let coll = collClasses.item(i) if(coll.textContent === pageTitle) { coll.closest(\u0026#39;li\u0026#39;).classList.add(\u0026#39;active\u0026#39;) } } })() \u0026lt;/script\u0026gt; \u0026lt;/th:block\u0026gt; \u0026lt;/html\u0026gt; en_list.html (real usage) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34; xmlns:th=\u0026#34;http://www.thymeleaf.org\u0026#34; xmlns:layout=\u0026#34;http://www.ultraq.net.nz/web/thymeleaf/layout\u0026#34; layout:decorate=\u0026#34;~{fragments/portal/defaultLayout}\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;User\u0026lt;/title\u0026gt; \u0026lt;th:block layout:fragment=\u0026#34;customStyle\u0026#34;\u0026gt; \u0026lt;link rel=\u0026#34;stylesheet\u0026#34; th:href=\u0026#34;@{/tcadmin/static/css1/user.css}\u0026#34;/\u0026gt; \u0026lt;link rel=\u0026#34;stylesheet\u0026#34; th:href=\u0026#34;@{/tcadmin/static/rMateChartH5/Assets/Css/rMateChartH5.css}\u0026#34;/\u0026gt; \u0026lt;/th:block\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;th:block layout:fragment=\u0026#34;content\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;card\u0026#34;\u0026gt; \u0026lt;p\u0026gt;This page contains real usage of layout divison\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/th:block\u0026gt; \u0026lt;th:block layout:fragment=\u0026#34;customScript\u0026#34;\u0026gt; \u0026lt;script\u0026gt; console.log(\u0026#34;Hello, you\u0026#34;) \u0026lt;/script\u0026gt; \u0026lt;/th:block\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Outro Importance of Dividing I\u0026rsquo;m writing this post to remind myself of importance of dividing html layout that I had not known about before. After writing and writing, now I slowly think about how important to find uniformity and extract that part as a single function to reduce code and usability.\nIn the future, when I have to create new function or manage the code, it will be way better in this way, because I don\u0026rsquo;t have to look around all 2000 line code. It\u0026rsquo;s in the one file. I\u0026rsquo;ll re-think before coding to build better program to manage.\n","date":"2020-10-25","permalink":"https://leeleelee3264.github.io/post/2020-10-25-front-thymeleaf-layout/","tags":["Frontend"],"title":"[Frontend] (en) How to divide Layout in Thymeleaf"},{"content":"\nImplement drag and drop to upload file.\nIndex\nIntro Implement Drag and Drop Intro Upload file with more action! Few days ago, I had to make a function that is about taking excel file from client side and reading it to save to our database. I usually go as simple as possible in terms of HTML/CSS, but this time I did some research and work!\nInstead of using basic file chose button, dragging a file and dropping in to a box. I was very happy when finishing these process because I thought it\u0026rsquo;s quite user friendly interface.\n[Picture 1] Drag and Drop Implement Drag and Drop HTML \u0026lt;div class=\u0026quot;file-upload\u0026quot;\u0026gt; \u0026lt;button class=\u0026quot;file-upload-btn\u0026quot; type=\u0026quot;button\u0026quot; onclick=\u0026quot;$('.file-upload-input').trigger( 'click' )\u0026quot;\u0026gt;Add Excel\u0026lt;/button\u0026gt; \u0026lt;div class=\u0026quot;image-upload-wrap\u0026quot;\u0026gt; \u0026lt;form id=\u0026quot;vts_form\u0026quot; enctype=\u0026quot;multipart/form-data\u0026quot; method=\u0026quot;post\u0026quot;\u0026gt; \u0026lt;input class=\u0026quot;file-upload-input\u0026quot; name=\u0026quot;input-file\u0026quot; type='file' onchange=\u0026quot;readURL(this);\u0026quot; accept=\u0026quot;application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\u0026quot; /\u0026gt; \u0026lt;/form\u0026gt; \u0026lt;div class=\u0026quot;drag-text\u0026quot;\u0026gt; \u0026lt;h3\u0026gt;Drag your excel or click\u0026lt;strong\u0026gt; ADD EXCEL\u0026lt;/strong\u0026gt;\u0026lt;/h3\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026quot;file-upload-content\u0026quot;\u0026gt; \u0026lt;img class=\u0026quot;file-upload-image\u0026quot; src=\u0026quot;/assets/img/demo/excelIcon.png\u0026quot; alt=\u0026quot;excel icon\u0026quot; /\u0026gt; \u0026lt;div class=\u0026quot;image-title-wrap\u0026quot;\u0026gt; \u0026lt;button type=\u0026quot;button\u0026quot; onclick=\u0026quot;removeUpload()\u0026quot; class=\u0026quot;remove-image\u0026quot;\u0026gt;Remove \u0026lt;span class=\u0026quot;image-title\u0026quot;\u0026gt;Uploaded Excel\u0026lt;/span\u0026gt;\u0026lt;/button\u0026gt; \u0026lt;button type=\u0026quot;button\u0026quot; class=\u0026quot;upload-image\u0026quot;\u0026gt;Upload \u0026lt;span class=\u0026quot;image-title\u0026quot;\u0026gt;Uploaded Excel\u0026lt;/span\u0026gt;\u0026lt;/button\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; MIME Type In file input form, I had to set accept attribute first to let server know what kind of Multipartfile will be uploaded. It called MIME type. I found that MIME type of microsoft file are quite complicated and long. And surprised that there are so many same excel file with hundred extension such as xlsx, xls, cvs! Maybe next time, I need to consider the most general file extension for excel too. [MIME type for input tag]\nTake file extensions as many as possible And as we all know, our clients make unexpected exception. For example, I made a formatted excel template to write data easily. (giving them select box in cell). But my colleague didn\u0026rsquo;t have excel software and try to open the template in google sheet.\nSadly, select box wasn\u0026rsquo;t working. Furthermore, it didn\u0026rsquo;t let us input data manually. It just gave us many alert of \u0026lsquo;It\u0026rsquo;s not allowed data in the cell\u0026rsquo;. The point is, like i said I have to think about more general type of client situation.\nExtension MIME Type\n.doc application/msword .dot application/msword .docx application/vnd.openxmlformats-officedocument.wordprocessingml.document .dotx application/vnd.openxmlformats-officedocument.wordprocessingml.template .docm application/vnd.ms-word.document.macroEnabled.12 .dotm application/vnd.ms-word.template.macroEnabled.12 .xls application/vnd.ms-excel .xlt application/vnd.ms-excel .xla application/vnd.ms-excel .xlsx application/vnd.openxmlformats-officedocument.spreadsheetml.sheet .xltx application/vnd.openxmlformats-officedocument.spreadsheetml.template .xlsm application/vnd.ms-excel.sheet.macroEnabled.12 .xltm application/vnd.ms-excel.template.macroEnabled.12 .xlam application/vnd.ms-excel.addin.macroEnabled.12 .xlsb application/vnd.ms-excel.sheet.binary.macroEnabled.12 .ppt application/vnd.ms-powerpoint .pot application/vnd.ms-powerpoint .pps application/vnd.ms-powerpoint .ppa application/vnd.ms-powerpoint .pptx application/vnd.openxmlformats-officedocument.presentationml.presentation .potx application/vnd.openxmlformats-officedocument.presentationml.template .ppsx application/vnd.openxmlformats-officedocument.presentationml.slideshow .ppam application/vnd.ms-powerpoint.addin.macroEnabled.12 .pptm application/vnd.ms-powerpoint.presentation.macroEnabled.12 .potm application/vnd.ms-powerpoint.template.macroEnabled.12 .ppsm application/vnd.ms-powerpoint.slideshow.macroEnabled.12 .mdb application/vnd.ms-access Javascript function readURL(input) { if (input.files \u0026amp;\u0026amp; input.files[0]) { let reader = new FileReader(); reader.onload = function (e) { document.querySelector('.image-upload-wrap').style.display = 'none'; document.querySelector('.file-upload-content').style.display = 'block'; let imgTitles = document.querySelectorAll('.image-title'); for(let title of imgTitles) { title.innerHTML = input.files[0].name; } }; reader.readAsDataURL(input.files[0]); } else { removeUpload(); } } function removeUpload() { document.querySelector('.file-upload-input').value = \u0026quot;\u0026quot;; document.querySelector('.file-upload-content').style.display = 'none'; document.querySelector('.image-upload-wrap').style.display = 'block'; } document.addEventListener('DOMContentLoaded', function () { let imageElement = document.querySelector('.image-upload-wrap'); let addedClass = 'image-dropping'; imageElement.addEventListener('dragover', function () { imageElement.classList.add(addedClass) }); imageElement.addEventListener('dragleave', function () { imageElement.classList.remove(addedClass) }); }) Vanilla javascript Recently I\u0026rsquo;ve been trying to use vanilla javascript only and no jQuery. I was actually surprised that plain javascript cover almost everything. And I struggled a lot with getElementByClassName. It returns collection type, quite different with getElementById. I found out $(\u0026rsquo;.className\u0026rsquo;) is the same with document.querySelector(\u0026rsquo;.className\u0026rsquo;). [youmightnotneedjquery] is very helpful.\nMinor troubleshooting When file is uploaded, excel icon will show up. But there was a problem. After removing an uploaded file, I tried to uploading the same file with previous one but the excel icon didn\u0026rsquo;t show up because they are the same one. So I had to add removing img in removeUpload().\nCSS *, *:before, *:after { box-sizing: border-box; } .file-upload { background-color: #ffffff; width: 100%; margin: 0 auto; padding: 20px; } .file-upload-btn { width: 100%; margin: 0; color: #fff; background: dodgerblue; border: none; padding: 10px; border-radius: 4px; border-bottom: 4px solid cornflowerblue; transition: all .2s ease; outline: none; text-transform: uppercase; font-weight: 700; } .file-upload-btn:hover { background: dodgerblue; color: #ffffff; transition: all .2s ease; cursor: pointer; } .file-upload-btn:active { border: 0; transition: all .2s ease; } .file-upload-content { display: none; text-align: center; } .file-upload-input { position: absolute; margin: 0; padding: 0; width: 100%; height: 100%; outline: none; opacity: 0; cursor: pointer; } .image-upload-wrap { margin-top: 20px; border: 4px dashed #1FB264; position: relative; } .image-dropping, .image-upload-wrap:hover { background-color: gainsboro; border: 4px dashed #ffffff; } .image-title-wrap { padding: 0 15px 15px 15px; color: #222; } .drag-text { text-align: center; } .drag-text h3 { font-weight: 100; text-transform: uppercase; color: #15824B; padding: 60px 0; } .file-upload-image { max-height: 200px; max-width: 200px; margin: auto; padding: 20px; } .remove-image { width: 200px; margin: 0; color: #fff; background: #cd4535; border: none; padding: 10px; border-radius: 4px; border-bottom: 4px solid #b02818; transition: all .2s ease; outline: none; text-transform: uppercase; font-weight: 700; } .remove-image:hover { background: grey; color: #ffffff; transition: all .2s ease; cursor: pointer; } .remove-image:active { border: 0; transition: all .2s ease; } .upload-image { width: 200px; margin: 0; color: #fff; background: lightseagreen; border: none; padding: 10px; border-radius: 4px; border-bottom: 4px solid forestgreen; transition: all .2s ease; outline: none; text-transform: uppercase; font-weight: 700; } .upload-image:hover { background: grey; color: #ffffff; transition: all .2s ease; cursor: pointer; } .upload-image:active { border: 0; transition: all .2s ease; } ","date":"2020-10-08","permalink":"https://leeleelee3264.github.io/post/2020-10-08-front-dropdown-file/","tags":["Frontend"],"title":"[Frontend] (en) How to implement drag and drop?"},{"content":"\nCheat sheet for Java stream: computIfPresent, computeIfAbsent, disjoint, joining.\nIndex\nMap Stream Map computeIfPresent - update value 1 2 3 4 5 Map\u0026lt;String, Integer\u0026gt; test = new HashMap\u0026lt;\u0026gt;(){{ test.put(\u0026#34;ME\u0026#34;, 25); test.put(\u0026#34;Mom\u0026#34;, 67); }}; test.computeIfPresent(\u0026#34;Me\u0026#34;, (k, v) -\u0026gt; v + 1); computeIfAbsent - put k, v when the key is not there 1 test.computeIfAbsent(\u0026#34;Sister\u0026#34;, k -\u0026gt; 25); Once, I wanted to use computeIfPresent and computeIfAbsent at once. My intend was putting (key, value) in specific map when ther is no key. And later, when the map meet the same key again, then update value.\nBut I missed the point. Thoese two method cannot be used at the same time. So I just gave a go with old fashioned way.\nOld way with computeIfPresent\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 // in this case, I had to extract keys to merge the list value. // in the meantime, I updated value as well. List\u0026lt;SampleInfoVO\u0026gt; infoById = sampleDao.getValueFromDb(param); Map\u0026lt;Integer, String\u0026gt; combineInfo = new HashMap\u0026lt;\u0026gt;(); for (SampleInfoVO single : infoById) { if(parsedStt.get(single.getUploadVoiceId()) == null) { parsedStt.put(single.getUploadVoiceId(), single.getTranscript()); continue; } // maybe I should have used string builder for string operating parsedStt.computeIfPresent(single.getUploadVoiceId(), (k, v) -\u0026gt; v + single.getTranscript()); } ComputeIfAbsent vs putIfAbsent In Map built-in method, ComputeIfAbsent and PutIfAbsent are very much similar. Main purpose is put key and value when the key isn\u0026rsquo;t there. But I found that computeIfAbsent is better for safety and performance.\nPutIfAbsent details\nputIfAbsent always create value object even if key exist in map. putIfAbsent also put null in value. putIfAbsent\nmake (\u0026ldquo;key\u0026rdquo;, new ArrayList) at first and go check the key computeIfAbsent\ngo check the key first if there is no key, then make (\u0026ldquo;key\u0026rdquo;, new ArrayList). No waste. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 public class MapTest { static Map\u0026lt;Integer, String\u0026gt; testMap; static { testMap = new HashMap\u0026lt;\u0026gt;(); testMap.put(1, \u0026#34;Pizza\u0026#34;); testMap.put(2, \u0026#34;IceCream\u0026#34;); } static void printMap(Map\u0026lt;?, ?\u0026gt; param) { param.entrySet().forEach(System.out::println); } public static void main(String [] args) { testMap.computeIfAbsent(3, k-\u0026gt; \u0026#34;Coffee\u0026#34;); testMap.putIfAbsent(4, null); testMap.computeIfAbsent(5, k -\u0026gt; null); printMap(testMap); // result 1=Pizza 2=IceCream 3=Coffee 4=null } } In this result, there is no 5 in testMap. Because computeIfAbsent ignore an attempt to put null in map. But 4 is indeed in the map. putIfAbsent doesn\u0026rsquo;t care much. Perfect answer is here!\ninit and insert at once with computeIfAbsent I just thought init and insert at once without if was not possible. Just write computeIfAbsent first to make init value container (in this case, List) and append the rest command that you need to save the value in the container.\nIt will execute computeIfAbsent when the container doesn\u0026rsquo;t have a key after that, it will execute the rest code. If the container has code, it will just ignore the first command line.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 public class StreamTest { private List\u0026lt;PojoSample\u0026gt; sampleListContainer; { sampleListContainer = new ArrayList\u0026lt;\u0026gt;(); PojoSample one = new PojoSample(1, \u0026#34;Jamie\u0026#34;, \u0026#34;Korea\u0026#34;); PojoSample four = new PojoSample(1, \u0026#34;Jamie\u0026#34;, \u0026#34;Canada\u0026#34;); PojoSample two = new PojoSample(2, \u0026#34;Travis\u0026#34;, \u0026#34;Canada\u0026#34;); PojoSample three = new PojoSample(3, \u0026#34;Naomi\u0026#34;, \u0026#34;Japan\u0026#34;); sampleListContainer.add(one); sampleListContainer.add(two); sampleListContainer.add(three); sampleListContainer.add(four); } @Test public void testComputeIfAbsent() { Map\u0026lt;Integer, List\u0026lt;PojoSample\u0026gt;\u0026gt; pojoById = new HashMap\u0026lt;\u0026gt;(); for(PojoSample single: sampleListContainer) { pojoById.computeIfAbsent(single.getId(), k -\u0026gt; new ArrayList\u0026lt;\u0026gt;()).add(single); } } } Stream Collections.disjoint AKA checking if one list contains an element in the other list Sometimes, we have to check if the given list has element in the other list. I usually liked to use for-loop to dig in. But as we know. The simpler, the better.\nCollections.disjoint help me up with this. In the document, they say it will return true if the two list doesn\u0026rsquo;t have a common. When I take a look at the method, it consists with (1) for loop (2) contains from collection. It means the performance is the same with for loop.\nIn an answer of StackOverFlow, if I need to search a large list, then I had better use HashSet. More info is here!\nCollections.disjoint and HashSet\n1 2 3 4 5 6 List\u0026lt;Integer\u0026gt; rs1 = Arrays.asList(1,2,3,4,4); List\u0026lt;Integer\u0026gt; rs2 = Arrays.asList(5,6,7,8); System.out.println(Collections.disjoint(rs1, rs2)); Set\u0026lt;Integer\u0026gt; rs1Set = new HashSet\u0026lt;\u0026gt;(rs1); System.out.println(Collections.disjoint(rs1Set, rs2)); If it is possible, think about use the first list to set in Collections.disjoint. It will show better performance. It is in the official document!\nWant less memory -\u0026gt; two list Want more speed -\u0026gt; one set, one list disjoint Implementation\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 public static boolean disjoint(Collection\u0026lt;?\u0026gt; c1, Collection\u0026lt;?\u0026gt; c2) { // The collection to be used for contains(). Preference is given to // the collection who\u0026#39;s contains() has lower O() complexity. Collection\u0026lt;?\u0026gt; contains = c2; // The collection to be iterated. If the collections\u0026#39; contains() impl // are of different O() complexity, the collection with slower // contains() will be used for iteration. For collections who\u0026#39;s // contains() are of the same complexity then best performance is // achieved by iterating the smaller collection. Collection\u0026lt;?\u0026gt; iterate = c1; // Performance optimization cases. The heuristics: // 1. Generally iterate over c1. // 2. If c1 is a Set then iterate over c2. // 3. If either collection is empty then result is always true. // 4. Iterate over the smaller Collection. if (c1 instanceof Set) { // Use c1 for contains as a Set\u0026#39;s contains() is expected to perform // better than O(N/2) iterate = c2; contains = c1; } else if (!(c2 instanceof Set)) { // Both are mere Collections. Iterate over smaller collection. // Example: If c1 contains 3 elements and c2 contains 50 elements and // assuming contains() requires ceiling(N/2) comparisons then // checking for all c1 elements in c2 would require 75 comparisons // (3 * ceiling(50/2)) vs. checking all c2 elements in c1 requiring // 100 comparisons (50 * ceiling(3/2)). int c1size = c1.size(); int c2size = c2.size(); if (c1size == 0 || c2size == 0) { // At least one collection is empty. Nothing will match. return true; } if (c1size \u0026gt; c2size) { iterate = c2; contains = c1; } } for (Object e : iterate) { if (contains.contains(e)) { // Found a common element. Collections are not disjoint. return false; } } // No common elements were found. return true; } String Joining Collectors.joining is very similar with String.join. Both methods containing string values with inputted character. And Sometimes I have to change int value to string doing something like Collectors.joining method. First map will extract value from the list and second map convert extracted int value to string.\nExtract value from List and joining 1 2 3 4 5 6 7 8 9 public class StreamTest { private List\u0026lt;PojoSample\u0026gt; sampleListContainer; @Test public void testJoiningAndConverting() { String nameJoin = sampleListContainer.stream().map(PojoSample::getName).collect(Collectors.joining(\u0026#34;/\u0026#34;)); } } How to convert int to String in List 1 2 3 4 5 6 7 8 9 public class StreamTest { private List\u0026lt;PojoSample\u0026gt; sampleListContainer; @Test public void testJoiningAndConverting() { String intConvertJoin = sampleListCOntainer.stream().map(PojoSample::getId).map(String::valueOf).collect(Collectors.joining(\u0026#34;/\u0026#34;)); } } ","date":"2020-10-07","permalink":"https://leeleelee3264.github.io/post/2020-10-07-java-stream-cheat/","tags":["Cheat"],"title":"[Cheat Sheet] (en) Java stream cheat sheet"},{"content":"\nCheat sheet for Git: log, diff, branch, switch, checkout, restore, commit, reset etc.\nIndex\nGit Cheat Sheet Git Cheat Sheet make annotated tag 1 git tag -a v1.0.0 -m \u0026#34;leave message here for tag\u0026#34; git log 1 2 3 4 5 6 7 8 # show log with one line git log --oneline # show log with graph tree git log --graph # show log with both git log --oneline --graph compare file between two branches 1 2 3 git diff branch1..branch2 -- \u0026#34;file_name\u0026#34; ex) diff 2021_0405_user..master -- build.gradle change branch name 1 git branch -m \u0026#34;branch_name\u0026#34; making branch with specific commit 1 2 3 git branch branch_name commit_version ex) git branch 2021_0422_add a9f5fc39c52dd362f99eb58fc1a011a6a12f9b1a switching branch hack Going back to previous branch with one second.\n1 git switch - Show branches 1 2 3 4 5 6 # local git branch # remote git branch -r # all git branch -a Check merged branch in Local 1 2 3 4 git branch --merged # check no merged branch in Local git branch --no-merged Check merged branch in Remote 1 git branch -r --merged Delete local branch 1 2 3 4 5 6 7 8 9 git branch -d branch_name # delete non merged branch git branch -D branch_name # delete merged branches (no specific command, have to use pipe # egrep is grep with regex expression # egpre -v find reverse result with regex. it will return without string contains master, tpi, gov git branch --merged | egrep -v \u0026#34;(^\\*|master|tpi|gov)\u0026#34; | xargs git branch -d Delete remote branch 1 2 3 (1) git push repo_name --delete branch_name (2) git branch -d branch_name git push orign branch_name 원격 저장소 branch 가져오기 1 2 3 4 # 1. 일단 pull 하고 local 과 remote에 있는 branch 확인하기 git branch -a # 2. remote에 있다고 표기된 branch 가져오기 git checkout -t origin/0921_excel git switch 로 브랜치 작업하기 1 2 3 4 5 6 # 1. 브랜치 변경하기 git switch master # 2. 브랜치 새로 만들어서 변경하기 git switch -c master # 3. 특정 버전 commit 에서 브랜치 새로 만들어서 변경하기 git switch -c master commit-number restore 로 unstaged 파일 작업 되돌리기 1 2 3 4 5 # 브랜치에서 작업했던 파일 작업 전으로 날려버리기 (add 하기 전) git restore src/path/sample.java # 브랜치에서 작업했던 파일 작업 전으로 날려버리기 (add 한 후) git resotre --staged src/path/sample.java 현재 브랜치에 없는 파일 가져오기 1 2 3 4 5 # commit 을 안 해서 없는 (새로 생성한) 파일은 그냥 cherry-pick으로 가져오자. # 이 커맨드 써서 가져오려면 꼬이기만 한다. # 얘는 현재의 브랜치에서는 삭제되어 없는 파일을 과거 브랜치에서 가져오는 용으로 git restore --source branchName src/file/you/delete/before.java commit 취소하기 1 2 3 4 5 #1. commit 취소하고 add 파일까지 unstaged 하기 git reset --mixed HEAD^ #2. commit 취소하나 add 파일은 여전히 staged 하기 git reset --softed HEAD^ commit message 변경하기 1 2 #1. 제일 최근에 한 commit의 message 변경 git commit --amend 제일 최근에 한 commit에 새로운 파일 추가하기 1 2 git add src/file/you/want/to/add.java git commit --amend commit 날짜 바꾸기 aka 인공 잔디 심기 1 2 3 #1. 제일 최근에 한 commit의 날짜 변경 # 마지막에 +0900 은 KST 시간대 설정 git commit --amend --no-edit --date \u0026#34;Fri Oct 23 11:11:11 2020 +0900\u0026#34; working directory를 remote repo의 마지막 commit으로 되돌리기 1 2 # 내가 이전까지 작업했던 파일들이 몽땅 날아가니 주의! git reset --hard HEAD^ add 취소하기 1 git reset HEAD 파일경로 remote push 취소하기 1 2 3 4 5 6 7 8 9 10 # remote에 올라간 push를 취소하고, local에서 수정을 한 다음 다시 push를 한다 # 취소한 commit이 remote에서도, local에서도 이전의 모습으로 돌아간다. # 여러명과 작업할 때 버젼 문제가 있으니 조심해서 쓰자. (for local) git reset HEAD^ # 커밋 취소 git reset 커밋아이디 # 내가 원하는 시점의 commit으로 이동 git commit -m \u0026#34;Commit to unpush\u0026#34; (for remote) git push origin 브랜치이름 -f merge 취소 1 2 3 4 #1. local에서 진행한 merge 취소하기 git reset --merge ORIG_HEAD #2. 더 쉬운 방법 (Conflict 발생했을 때 쓴다) git merge --abort 특정 브랜치의 commit 가져오기: git cherry-pick 🍒 1 2 3 4 git cherry-pick 커밋아이디 # 가끔 cherry-pick으로 commit 가져오다가 branch끼리 conflict 가 난다. 그때는 cherry-pick을 취소하자 git cherry-pick --abort get removed file back 1 2 3 4 5 # 1.When you didn\u0026#39;t commit # Freshly made file will be tracked after adding, # but in terms of removing, nothing will remain. I cannot even add the change. I can only commit the removing. git checkout HEAD file_you_removed reset(revert) file to specific commit version Let\u0026rsquo;s say you wrote something on A file but you were supposed to write the thing on B file.\nSo you just copy the content in A to B. Now B is fine. How about A? You should get the previous content back to A. This command will be lifesaver.\n1 2 3 4 5 # 1. Want to revert the file to specific commit version git checkout commit_number -- file_you_should_revert # 2. Want to revert the file to before the specific commit version git checkout commit_number~1 -- file_you_should_revert The second way works like this. If you change a file a lot in the latest commit and before that, the file was modified age ago. Then you can just revert the file before the latest commit.\n브랜치에서 작업했던 파일 작업 전으로 날려버리기 (add 하기 전) 1 git checkout src/path/sample.java git 기본 편집기 vim 으로 설정하기 1 git config --global core.editor \u0026#34;vim\u0026#34; git 계정 등록 1 2 git config --global user.name \u0026#34;absinthe4902\u0026#34; git config --global user.email \u0026#34;absinthe4902@naver.com\u0026#34; CRLF 개행문자 설정 1 2 # 맨날 LF Warning이 떠서 쓰기는 하는데 사실 먹히는지 모르겠다. git config --global core.autocrlf true working directory 에서 local 계정 만들기 git hub에서 내 commit으로 허용이 되려면 git hub에 등록된 이메일을 사용해야 한다. 개인용 컴퓨터는 상관이 없는데 회사에는 git에 회사 이메일이 등록되어있어 가끔 이메일을 바꿔줘야 하는데 매번 바꿀 필요 없이 commit하려는 디렉토리에만 local 계정을 만들어주면 된다.\n1 2 3 4 5 6 git config --local user.name \u0026#34;local_absinthe4902\u0026#34; git config --local user.email \u0026#34;local_absinthe4902@naver.com\u0026#34; # working directory 에 적용된 config 확인하기 (이 이름으로 push 된다고 생각하면 된다) # id name check git config user.name 연결된 remote url 변경하기 1 2 3 4 # 1. check name of remote branch git remote -v # 2. change url of remote branch git remote set-url origin https://github.com/leeleelee3264/leeleelee3264.github.io.git .gitginore 가 안 먹힐 때 AKA tracking 하지 말아야 할 파일을 트래킹 할 때 git cached 삭제하기 1 2 3 git rm -r --cached . git add . git commit -m \u0026#34;RESOLVED: .gitignore is not working\u0026#34; Error resolve: refusing to merge unrelated histories I saw this error when I had tried to change commit with \u0026ndash;amend but didn\u0026rsquo;t finish properly. People say it will show up when trying to merge two different projects with no history about each others.\n1 git pull origin branch_name --allow-unrelated-histories git init cancel 1 2 3 4 # sometimes I make wrong directory to git repo. Then I have to cancel it. # It\u0026#39;s all about .git directory. When I init directory, I\u0026#39;ll get git repo and it works like that. # Just remove the file and it will become normal directory rm -r .git git version update 1 2 3 4 5 # 1.version check git --version # 2. git version update git update-git-for-windows ","date":"2020-09-24","permalink":"https://leeleelee3264.github.io/post/2020-09-24-git-cheat-sheet/","tags":["Cheat"],"title":"[Cheat Sheet] (en) Git cheat sheet"},{"content":"\nHandles forked repo to standalone repo.\nIndex\nContribution not showing up on forked repo! Make forked repo to standalone repo Reference Contribution not showing up on forked repo! This started with one day one commit It\u0026rsquo;s been a graceful four days since starting one day one commit project. But something weird happen with yesterday commit data. I certainly updated a repo for github.io. I could see the record of it in the repo.\nHowever, the commit wasn\u0026rsquo;t recording in the contribution table in overview page. This contribution table is everything about one day one commit, so I have to dig in to see what\u0026rsquo;s going wrong.\nAnswer from Github: How to Contribute forked repo [github support -Why are my contributions not showing up on my profile?]\nIt doesn\u0026rsquo;t matter how to make contribution for active repository. Forking, coding, adding, committing and pulling with request\u0026hellip; The master owner will check on that later. Or you can write issue or asking the owner join you as a collaborator.\nBut I\u0026rsquo;m not going to contribute origin repo! But these are for actual solution of team repo. (or open source) I\u0026rsquo;m in different situation. I just copied this beautiful theme repo to use as my github.io template. I\u0026rsquo;m not going to make real contribution for the repo. It\u0026rsquo;s read-only archived.\nMake your forked repo to standalone repo AKA make the forked repo to your own repository.\nThis is the easiest way to fix contribution not showing problem. Thinking this as swap operation between repos!\nStep-by-Step\nClone the repo you want to use as github.io theme. (Do not fork) Create new repo in Github Set remote url in your local repo (you can skip this if the url already indicate your own github.io repo) to the new repo. Git push Now you have standalone repo! My problem was forking the repo at the very first time. Keep in mind cloning github.io repo to use! And normal purpose of repo is okay to fork for sure.\nReference [GitHub: make fork an “own project”] [github support] ","date":"2020-09-19","permalink":"https://leeleelee3264.github.io/post/2020-09-19-forked-repo-commit/","tags":["General"],"title":"[General] (en) How to change forked repo to standalone repo"},{"content":"\nHandles SQL Exception in Spring boot: Checked exception, unchecked exception,\nIndex\nException in Java Handle SQL Exception Reference Exception in Java A week ago, I felt maybe I need to adjust exception of sql. At that time, I just used try-catch with Exception and print stacktrace for backend, passed some certain value to let front know something went wrong with request. I\u0026rsquo;ve never went deep down to Exception and how to handle, so let\u0026rsquo;s first check what kind of Exception java has.\nException hierarchy [Picture 1] Java Exception Java use Throwable type when something goes wrong. The highest type Throwable then separate to three pieces. Error, Checked Exception and Unchecked Exception.\nError is just error. We cannot actually do something with error. It means, we don\u0026rsquo;t have to (and can\u0026rsquo;t) handle error. Then what about unchecked exception and checked exception?\nChecked Exception If I want to run program, I have to compile code first. Compiling is like translating. Translating human friendly language such as Java, C, Python to computer friendly language which contains 0 and 1.\nIf I make a mistake like opening not exist file, using not existing class or method then compiler will tell me. What should I do next? Without any choice, I must fix it. If not, I won\u0026rsquo;t be able to run the program I made. Straight forwardly, checked exception will be checked by compiler at the very first time.\nUnchecked Exception On the other hand, compiler can\u0026rsquo;t catch unchecked exception which will happen in the future. And I also can\u0026rsquo;t catch them either. I can only predict and try to prevent code from exception with Exception handling.\nThese are happend when program is actually running. We have certain rules about programming. Using legal argument, not accessing out of arrays, casting variable with proper casting method, not using null as value etc. However, these kind of exception sometimes happens due to client passing wrong value, server using mistaken code ect. We shell see them after running program.\nAnd here is the point! Can you see SQLException under Exception tree? SQLException is one of checked exception. And it\u0026rsquo;s today\u0026rsquo;s topic. Now let\u0026rsquo;s dig in to SQLException.\nHandle SQL Exception Why SQLException is checked exception? What I understand is that checked exception always pop up at compiler time. But in my ordinary code, exception related to sql pop up run time and compiler time both.\nChecked Exception in SQL When I miss or don\u0026rsquo;t configure setting about sql, compiler give me lots of error and stop right there. It were mostly about connection with spring and sql.\nRuntime Exception in SQL On the other hand, If I made bad sql grammar or try to use invalid value in sql sentence, then I got runtime exception. I didn\u0026rsquo;t get value I\u0026rsquo;d expected, but program was still running.\nI did some research and found out there are SQLException and DataAccessException in spring. SQLException is checked exception and DataAccessException implements RuntimeException.\nChange SQLException to DataAccessException Now it\u0026rsquo;s a trend to change checked exception to unchecked exception in standard spec and open source framework.\nMost of the time, we can\u0026rsquo;t react to checked exception right away. Because Java stepped into server area, we have to go through couple of step to fix exceptions. (coding, building, uploading, running\u0026hellip;)\nPeople changed mind. If we can\u0026rsquo;t fix exceptions fast, then at least we can make exceptions not too critical. Let\u0026rsquo;s make checked exception to unchecked exception! It\u0026rsquo;s bad hobby but sometimes we just ignore runtime exception and got ugly exception notification. I\u0026rsquo;ll summarize it.\nGoal of Exception Handler Turn checked exception to unchecked exception when caller (programmer) cannot fix, recover exception. Then what can I do with exception with sql? connection failure, bad sql grammar\u0026hellip; obviously not many things. So let\u0026rsquo;s just wrap it to unchecked exception to prevent stop running program. We still can use try-catch with runtime exception if we want. It even reduces writing throws and try-catch as well. Reason of wrapping SQLException to DataAccessException Only DAO has SQLException We should not pass sqlexception all the way through with bunch of throws. Wrap SqlException to DataAccessException With DataAccessException, we can chose using exception handling when we really need. It called abstraction of exception. make low level to high level. With high level exception, we can get more detail about the occurred exception. Implement Exception Handler When I wrote this code, it looked decent. But I feel like I have to change a couple of things to improve. It\u0026rsquo;s prototype anyway.\nNow the code\u0026rsquo;s flow is very simple. Meeting exception, logging exception. I might need to handle individual case of exception later. At that moment, I\u0026rsquo;ll use this code quite useful. And for good measure, I separate exception from mybatis and general concept of sql exception. Just in case.\nImplementation\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 /** * Created By Seungmin lee * User: *** * Date: 2020-09-15 * Description: 각종 Exception 을 logging 하고 처리할 목적으로 만든 Util */ @Slf4j public class ExceptionUtil { /** * mybatis 와 mysql에서 발생하는 error 들의 상세 logging 을 위한 메소드. * 추후에 로깅뿐 아니라 예외처리 로직도 넣어둘 예정. * @param ex MVC 단에서 던지는 예외 */ public static void sqlException(Exception ex) { if (Objects.isNull(ex.getCause()) || Objects.isNull(ex.getMessage())) return; // Mybatis 라이브러리 내부에 어떤 오류둘이 있는지 몰라 일단 한 번에 처리 if(ex instanceof MyBatisSystemException) { log.error(\u0026#34;**** Mybatis inner exception :{} ****\u0026#34;, ((MyBatisSystemException) ex).getRootCause().getMessage()); return; } // TODO: DataAccessException 과 SQLException 은 같은 수준인데 Spring에서는 DataAcessException 이 더 제너럴해서 나중에 바꿀 수 있다. SQLException higherEx; if (ex instanceof DataAccessException) { higherEx = (SQLException) ((DataAccessException) ex).getRootCause(); log.error(\u0026#34;**** DataAccessException :{} // : {} ****\u0026#34;, higherEx.getErrorCode(), higherEx.getMessage()); } if (ex instanceof BadSqlGrammarException) { higherEx = (SQLException) ((BadSqlGrammarException) ex).getRootCause(); log.error(\u0026#34;**** BadSqlGrammarException :{} // : {} ****\u0026#34;, higherEx.getErrorCode(), higherEx.getMessage()); } else if(ex instanceof InvalidResultSetAccessException) { higherEx = (SQLException) ((InvalidResultSetAccessException) ex).getRootCause(); log.error(\u0026#34;**** InvalidResultSetAccessException :{} // : {} ****\u0026#34;, higherEx.getErrorCode(), higherEx.getMessage()); } else if(ex instanceof DuplicateKeyException) { higherEx = (SQLException) ((DuplicateKeyException) ex).getRootCause(); log.error(\u0026#34;**** DuplicateKeyException :{} // : {} ****\u0026#34;, higherEx.getErrorCode(), higherEx.getMessage()); } else if(ex instanceof DataIntegrityViolationException) { higherEx = (SQLException) ((DataIntegrityViolationException) ex).getRootCause(); log.error(\u0026#34;**** DataIntegrityViolationException :{} // : {} ****\u0026#34;, higherEx.getErrorCode(), higherEx.getMessage()); } else if(ex instanceof DataAccessResourceFailureException) { higherEx = (SQLException) ((DataAccessResourceFailureException) ex).getRootCause(); log.error(\u0026#34;**** DataAccessResourceFailureException :{} // : {} ****\u0026#34;, higherEx.getErrorCode(), higherEx.getMessage()); } else if(ex instanceof CannotAcquireLockException) { higherEx = (SQLException) ((CannotAcquireLockException) ex).getRootCause(); log.error(\u0026#34;**** CannotAcquireLockException :{} // : {} ****\u0026#34;, higherEx.getErrorCode(), higherEx.getMessage()); } else if (ex instanceof DeadlockLoserDataAccessException) { higherEx = (SQLException) ((DeadlockLoserDataAccessException) ex).getRootCause(); log.error(\u0026#34;**** DeadlockLoserDataAccessException :{} // : {} ****\u0026#34;, higherEx.getErrorCode(), higherEx.getMessage()); } else if (ex instanceof CannotSerializeTransactionException) { higherEx = (SQLException) ((CannotSerializeTransactionException) ex).getRootCause(); log.error(\u0026#34;**** CannotSerializeTransactionException :{} // : {} ****\u0026#34;, higherEx.getErrorCode(), higherEx.getMessage()); } else { log.error(\u0026#34;**** Exception :{} ****\u0026#34;, ex.getMessage()); } } } TODO List\nStop using SQLException. Use DataAccessException instead. Do not use DataAccessException at the top of if statement. If is the lowest exception. Do I really need to print detailed logging? I could just print stack trace. It even has more info. Reference [RuntimeException and higher exception] [SQLException to DataAccessException] [Runtime and Compile time] [Spring SQLException Handling] ","date":"2020-09-18","permalink":"https://leeleelee3264.github.io/post/2020-09-18-java-checkuncheck-exception/","tags":["Backend"],"title":"[Backend] (en) How to deal with SQL exception in Spring boot"}]